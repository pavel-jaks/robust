{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import MnistData, Clipper\n",
    "from models import ModelManager, ModelType\n",
    "from adversarials import ClassificationAdversarials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "linfty_norm_radius = 50 / 255\n",
    "lone_norm_radius = 100\n",
    "ltwo_norm_radius = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelManager.get_trained(ModelType.MnistCnnB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MnistData(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CW functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_for_linfty(model, benign_image, label, c_lambda, max_norm):\n",
    "    step_size = 1e-2\n",
    "    adv = torch.zeros(benign_image.shape)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "    norm_of_diff = lambda x, y: torch.max(torch.abs(x - y))\n",
    "    adv = adv.unsqueeze(0)\n",
    "    benign_image = benign_image.unsqueeze(0)\n",
    "    for _ in range(100):\n",
    "        adv.requires_grad = True\n",
    "        if adv.grad is not None:\n",
    "            adv.grad.zero_()\n",
    "        loss = norm_of_diff(adv, benign_image) \\\n",
    "            - c_lambda * loss_fn(model(adv), torch.Tensor([label]).type(torch.long))\n",
    "        loss.backward()\n",
    "        new_adv = Clipper.clip(\n",
    "            benign_image,\n",
    "            (adv - step_size * adv.grad.apply_(lambda x: 1 if x >= 0 else -1)),\n",
    "            max_norm\n",
    "        )\n",
    "        adv = new_adv\n",
    "    if torch.argmax(model(adv), dim=1)[0] != label or c_lambda > 10:\n",
    "        return adv.squeeze(0)\n",
    "    return None\n",
    "\n",
    "def cw_linfty(model: nn.Module, benign_examples: torch.Tensor, labels: torch.Tensor, max_norm) -> torch.Tensor:\n",
    "    advs = []\n",
    "    for i in range(len(benign_examples)):\n",
    "        print(f'--- {i} ---')\n",
    "        benign_example, label = benign_examples[i], labels[i]\n",
    "        adv = None\n",
    "        c_lambda = 1e-2\n",
    "        while adv is None:\n",
    "            adv = solve_for_linfty(model, benign_example, label, c_lambda, max_norm)\n",
    "            c_lambda *= 1.1\n",
    "        advs.append(adv)\n",
    "    return torch.Tensor([adv.tolist() for adv in advs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lone_norm(examples: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.sum(torch.abs(examples))\n",
    "\n",
    "def solve_for_lone(model, benign_image, label, c_lambda, norm):\n",
    "    step_size = 1e-2\n",
    "    adv = torch.zeros(benign_image.shape)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "    adv = adv.unsqueeze(0)\n",
    "    benign_image = benign_image.unsqueeze(0)\n",
    "    for _ in range(100):\n",
    "        adv.requires_grad = True\n",
    "        if adv.grad is not None:\n",
    "            adv.grad.zero_()\n",
    "        loss = lone_norm(adv - benign_image) \\\n",
    "            - c_lambda * loss_fn(model(adv), torch.Tensor([label]).type(torch.long))\n",
    "        loss.backward()\n",
    "        new_adv = Clipper.clip_with_custom_norm(\n",
    "            benign_image,\n",
    "            (adv - step_size * adv.grad.apply_(lambda x: 1 if x >= 0 else -1)),\n",
    "            lone_norm,\n",
    "            norm\n",
    "        )\n",
    "        adv = new_adv\n",
    "    if torch.argmax(model(adv), dim=1)[0] != label or c_lambda > 10:\n",
    "        return adv.squeeze(0)\n",
    "    return None\n",
    "\n",
    "def cw_lone(model: nn.Module, benign_examples: torch.Tensor, labels: torch.Tensor, norm) -> torch.Tensor:\n",
    "    advs = []\n",
    "    for i in range(len(benign_examples)):\n",
    "        print(f'--- {i} ---')\n",
    "        benign_example, label = benign_examples[i], labels[i]\n",
    "        adv = None\n",
    "        c_lambda = 1e-2\n",
    "        while adv is None:\n",
    "            adv = solve_for_lone(model, benign_example, label, c_lambda, norm)\n",
    "            c_lambda *= 1.1\n",
    "        advs.append(adv)\n",
    "    return torch.Tensor([adv.tolist() for adv in advs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ltwo_norm(examples: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.sum(torch.abs(examples) ** 2) ** (1 / 2)\n",
    "\n",
    "def solve_for_ltwo(model, benign_image, label, c_lambda, norm):\n",
    "    step_size = 1e-2\n",
    "    adv = torch.zeros(benign_image.shape)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "    adv = adv.unsqueeze(0)\n",
    "    benign_image = benign_image.unsqueeze(0)\n",
    "    for _ in range(100):\n",
    "        adv.requires_grad = True\n",
    "        if adv.grad is not None:\n",
    "            adv.grad.zero_()\n",
    "        loss = ltwo_norm(adv - benign_image) \\\n",
    "            - c_lambda * loss_fn(model(adv), torch.Tensor([label]).type(torch.long))\n",
    "        loss.backward()\n",
    "        new_adv = Clipper.clip_with_custom_norm(\n",
    "            benign_image,\n",
    "            (adv - step_size * adv.grad.apply_(lambda x: 1 if x >= 0 else -1)),\n",
    "            ltwo_norm,\n",
    "            norm\n",
    "        )\n",
    "        adv = new_adv\n",
    "    if torch.argmax(model(adv), dim=1)[0] != label or c_lambda > 10:\n",
    "        return adv.squeeze(0)\n",
    "    return None\n",
    "\n",
    "def cw_ltwo(model: nn.Module, benign_examples: torch.Tensor, labels: torch.Tensor, norm) -> torch.Tensor:\n",
    "    advs = []\n",
    "    for i in range(len(benign_examples)):\n",
    "        print(f'--- {i} ---')\n",
    "        benign_example, label = benign_examples[i], labels[i]\n",
    "        adv = None\n",
    "        c_lambda = 1e-2\n",
    "        while adv is None:\n",
    "            adv = solve_for_ltwo(model, benign_example, label, c_lambda, norm)\n",
    "            c_lambda *= 1.1\n",
    "        advs.append(adv)\n",
    "    return torch.Tensor([adv.tolist() for adv in advs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_examples, labels = data.choose_first_well_classified(batch_size, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0 ---\n",
      "--- 1 ---\n",
      "--- 2 ---\n",
      "--- 3 ---\n",
      "--- 4 ---\n",
      "--- 5 ---\n",
      "--- 6 ---\n",
      "--- 7 ---\n",
      "--- 8 ---\n",
      "--- 9 ---\n"
     ]
    }
   ],
   "source": [
    "cw_linfty_examples = cw_linfty(model, benign_examples, labels, linfty_norm_radius)\n",
    "cw_lone_examples = cw_lone(model, benign_examples, labels, lone_norm_radius)\n",
    "cw_ltwo_examples = cw_ltwo(model, benign_examples, labels, ltwo_norm_radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cw_linfty: 8\n",
      "cw_lone: 4\n",
      "cw_ltwo: 10\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save'em all\n",
    "for i in range(batch_size):\n",
    "    example = np.array(benign_examples[i].detach()).reshape(28, 28)\n",
    "    plt.imshow(example, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f\"AEXAMPLES\\\\CW_NORMS\\\\benign_{i}.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "    \n",
    "    example = np.array(cw_linfty_examples[i].detach()).reshape(28, 28)\n",
    "    plt.imshow(example, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f\"AEXAMPLES\\\\CW_NORMS\\\\cw_linfty_{i}.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "    \n",
    "    example = np.array(cw_lone_examples[i].detach()).reshape(28, 28)\n",
    "    plt.imshow(example, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f\"AEXAMPLES\\\\CW_NORMS\\\\cw_lone_{i}.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "    \n",
    "    example = np.array(cw_ltwo_examples[i].detach()).reshape(28, 28)\n",
    "    plt.imshow(example, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f\"AEXAMPLES\\\\CW_NORMS\\\\cw_ltwo_{i}.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "\n",
    "cw_linfty_adversarials = ClassificationAdversarials.get_adversarials(model, benign_examples, labels, cw_linfty_examples)\n",
    "print(f'cw_linfty: {len(cw_linfty_adversarials)}')\n",
    "\n",
    "cw_lone_adversarials = ClassificationAdversarials.get_adversarials(model, benign_examples, labels, cw_lone_examples)\n",
    "print(f'cw_lone: {len(cw_lone_adversarials)}')\n",
    "\n",
    "cw_ltwo_adversarials = ClassificationAdversarials.get_adversarials(model, benign_examples, labels, cw_ltwo_examples)\n",
    "print(f'cw_ltwo: {len(cw_ltwo_adversarials)}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9d03d93fdbeeb361cc45deae4888a34596acc9ca3c93366af240ad46910d2d2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
