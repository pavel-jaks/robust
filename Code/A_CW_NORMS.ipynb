{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.linalg as linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import MnistData, Clipper\n",
    "from models import ModelManager, ModelType\n",
    "from adversarials import ClassificationAdversarials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linfty_norm_radius = 50 / 255\n",
    "lone_norm_radius = 28 * 28 * 50 / 255\n",
    "ltwo_norm_radius = 28 * 50 / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelManager.get_trained(ModelType.MnistCnnB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MnistData(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CW functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_for_linfty(model, benign_image, label, c_lambda, max_norm):\n",
    "    step_size = 1e-2\n",
    "    adv = torch.zeros(benign_image.shape)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "    norm_of_diff = lambda x, y: torch.max(torch.abs(x - y))\n",
    "    adv = adv.unsqueeze(0)\n",
    "    benign_image = benign_image.unsqueeze(0)\n",
    "    for _ in range(100):\n",
    "        adv.requires_grad = True\n",
    "        if adv.grad is not None:\n",
    "            adv.grad.zero_()\n",
    "        loss = norm_of_diff(adv, benign_image) \\\n",
    "            - c_lambda * loss_fn(model(adv), torch.Tensor([label]).type(torch.long))\n",
    "        loss.backward()\n",
    "        new_adv = Clipper.clip(\n",
    "            benign_image,\n",
    "            (adv - step_size * adv.grad.apply_(lambda x: 1 if x >= 0 else -1)),\n",
    "            max_norm\n",
    "        )\n",
    "        adv = new_adv\n",
    "    if torch.argmax(model(adv), dim=1)[0] != label or c_lambda > 10:\n",
    "        return adv.squeeze(0)\n",
    "    return None\n",
    "\n",
    "def cw_linfty(model: nn.Module, benign_examples: torch.Tensor, labels: torch.Tensor, max_norm) -> torch.Tensor:\n",
    "    advs = []\n",
    "    for i in range(len(benign_examples)):\n",
    "        print(f'--- {i} ---')\n",
    "        benign_example, label = benign_examples[i], labels[i]\n",
    "        adv = None\n",
    "        c_lambda = 1e-2\n",
    "        while adv is None:\n",
    "            adv = solve_for_linfty(model, benign_example, label, c_lambda, max_norm)\n",
    "            c_lambda *= 1.1\n",
    "        advs.append(adv)\n",
    "    return torch.Tensor([adv.tolist() for adv in advs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lone_norm(examples: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.sum(torch.abs(examples))\n",
    "\n",
    "def solve_for_lone(model, benign_image, label, c_lambda, norm):\n",
    "    step_size = 1e-2\n",
    "    adv = torch.zeros(benign_image.shape)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "    adv = adv.unsqueeze(0)\n",
    "    benign_image = benign_image.unsqueeze(0)\n",
    "    for _ in range(100):\n",
    "        adv.requires_grad = True\n",
    "        if adv.grad is not None:\n",
    "            adv.grad.zero_()\n",
    "        loss = lone_norm(adv - benign_image) \\\n",
    "            - c_lambda * loss_fn(model(adv), torch.Tensor([label]).type(torch.long))\n",
    "        loss.backward()\n",
    "        new_adv = Clipper.clip_with_custom_norm(\n",
    "            benign_image,\n",
    "            (adv - step_size * adv.grad.apply_(lambda x: 1 if x >= 0 else -1)),\n",
    "            lone_norm,\n",
    "            norm\n",
    "        )\n",
    "        adv = new_adv\n",
    "    if torch.argmax(model(adv), dim=1)[0] != label or c_lambda > 10:\n",
    "        return adv.squeeze(0)\n",
    "    return None\n",
    "\n",
    "def cw_lone(model: nn.Module, benign_examples: torch.Tensor, labels: torch.Tensor, norm) -> torch.Tensor:\n",
    "    advs = []\n",
    "    for i in range(len(benign_examples)):\n",
    "        print(f'--- {i} ---')\n",
    "        benign_example, label = benign_examples[i], labels[i]\n",
    "        adv = None\n",
    "        c_lambda = 1e-2\n",
    "        while adv is None:\n",
    "            adv = solve_for_lone(model, benign_example, label, c_lambda, norm)\n",
    "            c_lambda *= 1.1\n",
    "        advs.append(adv)\n",
    "    return torch.Tensor([adv.tolist() for adv in advs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ltwo_norm(examples: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.sum(torch.abs(examples) ** 2) ** (1 / 2)\n",
    "\n",
    "def solve_for_ltwo(model, benign_image, label, c_lambda, norm):\n",
    "    step_size = 1e-2\n",
    "    adv = torch.zeros(benign_image.shape)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "    adv = adv.unsqueeze(0)\n",
    "    benign_image = benign_image.unsqueeze(0)\n",
    "    for _ in range(100):\n",
    "        adv.requires_grad = True\n",
    "        if adv.grad is not None:\n",
    "            adv.grad.zero_()\n",
    "        loss = ltwo_norm(adv - benign_image) \\\n",
    "            - c_lambda * loss_fn(model(adv), torch.Tensor([label]).type(torch.long))\n",
    "        loss.backward()\n",
    "        new_adv = Clipper.clip_with_custom_norm(\n",
    "            benign_image,\n",
    "            (adv - step_size * adv.grad.apply_(lambda x: 1 if x >= 0 else -1)),\n",
    "            ltwo_norm,\n",
    "            norm\n",
    "        )\n",
    "        adv = new_adv\n",
    "    if torch.argmax(model(adv), dim=1)[0] != label or c_lambda > 10:\n",
    "        return adv.squeeze(0)\n",
    "    return None\n",
    "\n",
    "def cw_ltwo(model: nn.Module, benign_examples: torch.Tensor, labels: torch.Tensor, norm) -> torch.Tensor:\n",
    "    advs = []\n",
    "    for i in range(len(benign_examples)):\n",
    "        print(f'--- {i} ---')\n",
    "        benign_example, label = benign_examples[i], labels[i]\n",
    "        adv = None\n",
    "        c_lambda = 1e-2\n",
    "        while adv is None:\n",
    "            adv = solve_for_ltwo(model, benign_example, label, c_lambda, norm)\n",
    "            c_lambda *= 1.1\n",
    "        advs.append(adv)\n",
    "    return torch.Tensor([adv.tolist() for adv in advs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_examples, labels = data.choose_first_well_classified(batch_size, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_linfty_examples = cw_linfty(model, benign_examples, labels, linfty_norm_radius)\n",
    "cw_lone_examples = cw_lone(model, benign_examples, labels, lone_norm_radius)\n",
    "cw_ltwo_examples = cw_ltwo(model, benign_examples, labels, ltwo_norm_radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save'em all\n",
    "for i in range(batch_size):\n",
    "    example = np.array(benign_examples[i].detach()).reshape(28, 28)\n",
    "    plt.imshow(example, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f\"AEXAMPLES\\\\CW_NORMS\\\\benign_{i}.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "    \n",
    "    example = np.array(cw_linfty_examples[i].detach()).reshape(28, 28)\n",
    "    plt.imshow(example, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f\"AEXAMPLES\\\\CW_NORMS\\\\cw_linfty_{i}.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "    \n",
    "    example = np.array(cw_lone_examples[i].detach()).reshape(28, 28)\n",
    "    plt.imshow(example, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f\"AEXAMPLES\\\\CW_NORMS\\\\cw_lone_{i}.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "    \n",
    "    example = np.array(cw_ltwo_examples[i].detach()).reshape(28, 28)\n",
    "    plt.imshow(example, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f\"AEXAMPLES\\\\CW_NORMS\\\\cw_ltwo_{i}.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "\n",
    "cw_linfty_adversarials = ClassificationAdversarials.get_adversarials(model, benign_examples, labels, cw_linfty_examples)\n",
    "print(f'cw_linfty: {len(cw_linfty_adversarials)}')\n",
    "\n",
    "cw_lone_adversarials = ClassificationAdversarials.get_adversarials(model, benign_examples, labels, cw_lone_examples)\n",
    "print(f'cw_lone: {len(cw_lone_adversarials)}')\n",
    "\n",
    "cw_ltwo_adversarials = ClassificationAdversarials.get_adversarials(model, benign_examples, labels, cw_ltwo_examples)\n",
    "print(f'cw_ltwo: {len(cw_ltwo_adversarials)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ad Hoc experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ltwo_norm_mnist(examples: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.sum(torch.abs(examples) ** 2, dim=2).sum(dim=2).sum(dim=1) ** (1 / 2)\n",
    "\n",
    "def phi(model, benign_examples, delta, labels, ltwo_norm_radius, c_lambda):\n",
    "    loss_fn = nn.L1Loss(reduction='sum')\n",
    "    delta.requires_grad = True\n",
    "    if delta.grad is not None:\n",
    "        delta.grad.zero_()\n",
    "    c_lambda.requires_grad = True\n",
    "    if c_lambda.grad is not None:\n",
    "        c_lambda.grad.zero_()\n",
    "    dim = 1\n",
    "    for i in range(1, len(benign_examples.shape)):\n",
    "        dim *= benign_examples.shape[i]\n",
    "    one_hot = torch.Tensor([[1 if label == j else 0 for j in range(10)] for label in labels])\n",
    "    loss = loss_fn(model(benign_examples + delta),one_hot)\n",
    "    loss.backward(retain_graph=True)\n",
    "    phi_value = (- 2 * delta.reshape(len(benign_examples), dim) * c_lambda.reshape(len(benign_examples), 1) \\\n",
    "        - delta.grad.reshape(len(benign_examples), dim)).reshape(len(benign_examples), dim)\n",
    "    phi_n_1 = - (delta ** 2).sum(dim=2).sum(dim=2).sum(dim=1) + ltwo_norm_radius\n",
    "    return phi_value, phi_n_1\n",
    "\n",
    "def cw_newton_ltwo(model, benign_examples, labels, ltwo_norm_radius):\n",
    "    benign_examples.requires_grad = True\n",
    "    dim = 1\n",
    "    for i in range(1, len(benign_examples.shape)):\n",
    "        dim *= benign_examples.shape[i]\n",
    "    delta = torch.rand(benign_examples.shape) - 0.5\n",
    "    norms_inverse = (1 / ltwo_norm_mnist(delta)).reshape(len(benign_examples),1)\n",
    "    delta = (norms_inverse * delta.reshape(len(benign_examples), dim)).reshape(len(benign_examples), 1, 28, 28).detach()\n",
    "    c_lambda = torch.ones(len(benign_examples), 1)\n",
    "    c_lambda.requires_grad = True\n",
    "    while True:\n",
    "        d_phi_value = torch.zeros(len(benign_examples), dim + 1, dim + 1)\n",
    "        phi_value, phi_n_1 = phi(model, benign_examples, delta, labels, ltwo_norm_radius, c_lambda)\n",
    "        for i in range(dim):\n",
    "            if delta.grad is not None:\n",
    "                delta.grad.zero_()\n",
    "            if c_lambda.grad is not None:\n",
    "                c_lambda.grad.zero_()\n",
    "            phi_value_i = phi_value[:, i]\n",
    "            phi_value_i = phi_value_i.sum()\n",
    "            phi_value_i.backward(retain_graph=True)\n",
    "            d_phi_value[:, i, 0: dim] = delta.grad.detach().reshape(len(benign_examples), dim)\n",
    "            d_phi_value[:, i, dim] = c_lambda.grad.detach().reshape(len(benign_examples))\n",
    "        if delta.grad is not None:\n",
    "            delta.grad.zero_()\n",
    "        if c_lambda.grad is not None:\n",
    "            c_lambda.grad.zero_()\n",
    "        phi_n_1.sum().backward()\n",
    "        d_phi_value[:, dim, 0: dim] = delta.grad.detach().reshape(len(benign_examples), dim)\n",
    "        # d_phi_value_inverse = torch.inverse(d_phi_value).reshape(len(benign_examples), dim+1, dim+1)\n",
    "        whole_phi = torch.zeros(len(benign_examples), dim + 1, 1)\n",
    "        whole_phi[:, 0: dim, 0] = phi_value\n",
    "        whole_phi[:, dim, 0] = phi_n_1\n",
    "        whole_delta = torch.zeros(len(benign_examples), dim + 1)\n",
    "        whole_delta[:, 0:dim] = delta.reshape(len(benign_examples), dim)\n",
    "        whole_delta[:, dim] = c_lambda.reshape(len(benign_examples))\n",
    "        zbuchlo_to = False\n",
    "        try:\n",
    "            product = linalg.solve( d_phi_value, whole_phi)\n",
    "        except RuntimeError:\n",
    "            product = whole_phi\n",
    "            zbuchlo_to = True\n",
    "        new_whole_delta = whole_delta - product.reshape(len(benign_examples), dim+1)\n",
    "        if nn.MSELoss(reduction='sum')(whole_delta, new_whole_delta) <= len(benign_examples) * 1e-7 or zbuchlo_to:\n",
    "            delta, c_lambda = new_whole_delta[:, 0: dim].reshape(len(benign_examples), 1, 28, 28), new_whole_delta[:, dim].reshape(len(benign_examples),1)\n",
    "            break\n",
    "        else:\n",
    "            delta, c_lambda = new_whole_delta[:, 0: dim].reshape(len(benign_examples), 1, 28, 28).detach(), new_whole_delta[:, dim].detach()\n",
    "    return benign_examples + delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_newton = cw_newton_ltwo(model, benign_examples, labels, ltwo_norm_radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(batch_size):\n",
    "    MnistData.display(cw_newton[i], scale=True)\n",
    "advs = ClassificationAdversarials.get_adversarials(model, benign_examples, labels, cw_newton)\n",
    "len(advs)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9d03d93fdbeeb361cc45deae4888a34596acc9ca3c93366af240ad46910d2d2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
