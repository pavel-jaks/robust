{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import Coach\n",
    "from utils import MnistData\n",
    "from models import ModelManager, ModelType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MnistData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple, adagrad, rmsprop, adam = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- (0) ---\n",
      "Training started at 1646075967.5054333\n",
      "Epoch 0: 69.15373992919922: 0.1028\n",
      "Epoch 5000: 51.699440002441406: 0.7527\n",
      "Training finished at 1646075983.0046306; lasted 15.499197244644165 seconds.\n",
      "75.27000000000001 % success on test data\n",
      "--- (1) ---\n",
      "Training started at 1646075983.7705455\n",
      "Epoch 0: 69.10469055175781: 0.0982\n",
      "Epoch 5000: 50.3803825378418: 0.8067\n",
      "Training finished at 1646075998.5445123; lasted 14.773966789245605 seconds.\n",
      "80.67 % success on test data\n",
      "--- (2) ---\n",
      "Training started at 1646075999.3016746\n",
      "Epoch 0: 68.90754699707031: 0.0892\n",
      "Epoch 5000: 50.926544189453125: 0.8317\n",
      "Training finished at 1646076014.6737518; lasted 15.372077226638794 seconds.\n",
      "83.17 % success on test data\n",
      "--- (3) ---\n",
      "Training started at 1646076015.4202065\n",
      "Epoch 0: 69.0920639038086: 0.1032\n",
      "Epoch 5000: 48.185089111328125: 0.9009\n",
      "Training finished at 1646076029.9130168; lasted 14.492810249328613 seconds.\n",
      "90.09 % success on test data\n",
      "--- (4) ---\n",
      "Training started at 1646076030.6746583\n",
      "Epoch 0: 68.94232177734375: 0.0892\n",
      "Epoch 5000: 51.827735900878906: 0.7035\n",
      "Training finished at 1646076045.4567766; lasted 14.782118320465088 seconds.\n",
      "70.35 % success on test data\n",
      "--- (5) ---\n",
      "Training started at 1646076046.230938\n",
      "Epoch 0: 69.12140655517578: 0.1032\n",
      "Epoch 5000: 48.773685455322266: 0.8297\n",
      "Training finished at 1646076061.85187; lasted 15.62093210220337 seconds.\n",
      "82.97 % success on test data\n",
      "--- (6) ---\n",
      "Training started at 1646076062.6661725\n",
      "Epoch 0: 68.974609375: 0.0974\n",
      "Epoch 5000: 53.862693786621094: 0.8235\n",
      "Training finished at 1646076077.547464; lasted 14.881291389465332 seconds.\n",
      "82.35 % success on test data\n",
      "--- (7) ---\n",
      "Training started at 1646076078.3399246\n",
      "Epoch 0: 69.06343078613281: 0.0983\n",
      "Epoch 5000: 50.805145263671875: 0.8094\n",
      "Training finished at 1646076093.3327634; lasted 14.992838859558105 seconds.\n",
      "80.94 % success on test data\n",
      "--- (8) ---\n",
      "Training started at 1646076094.111051\n",
      "Epoch 0: 69.1795883178711: 0.1103\n",
      "Epoch 5000: 46.976810455322266: 0.78\n",
      "Training finished at 1646076109.0183578; lasted 14.907306671142578 seconds.\n",
      "78.0 % success on test data\n",
      "--- (9) ---\n",
      "Training started at 1646076109.8112838\n",
      "Epoch 0: 69.22696685791016: 0.1132\n",
      "Epoch 5000: 45.8740234375: 0.8238\n",
      "Training finished at 1646076124.4855173; lasted 14.674233436584473 seconds.\n",
      "82.38 % success on test data\n",
      "--- (10) ---\n",
      "Training started at 1646076125.3181236\n",
      "Epoch 0: 69.12982940673828: 0.1021\n",
      "Epoch 5000: 54.06377410888672: 0.7611\n",
      "Training finished at 1646076139.7955482; lasted 14.477424621582031 seconds.\n",
      "76.11 % success on test data\n",
      "--- (11) ---\n",
      "Training started at 1646076140.5464349\n",
      "Epoch 0: 68.98442840576172: 0.101\n",
      "Epoch 5000: 45.79730987548828: 0.8982\n",
      "Training finished at 1646076154.9182367; lasted 14.371801853179932 seconds.\n",
      "89.82 % success on test data\n",
      "--- (12) ---\n",
      "Training started at 1646076155.7028291\n",
      "Epoch 0: 69.1893081665039: 0.1009\n",
      "Epoch 5000: 48.31568145751953: 0.8199\n",
      "Training finished at 1646076171.6871314; lasted 15.984302282333374 seconds.\n",
      "81.99 % success on test data\n",
      "--- (13) ---\n",
      "Training started at 1646076172.466307\n",
      "Epoch 0: 69.06187438964844: 0.0869\n",
      "Epoch 5000: 52.26738739013672: 0.735\n",
      "Training finished at 1646076188.1657836; lasted 15.699476718902588 seconds.\n",
      "73.5 % success on test data\n",
      "--- (14) ---\n",
      "Training started at 1646076188.9212933\n",
      "Epoch 0: 69.05337524414062: 0.0892\n",
      "Epoch 5000: 49.826072692871094: 0.817\n",
      "Training finished at 1646076203.8474076; lasted 14.926114320755005 seconds.\n",
      "81.69999999999999 % success on test data\n",
      "--- (15) ---\n",
      "Training started at 1646076204.6428196\n",
      "Epoch 0: 69.02318572998047: 0.0892\n",
      "Epoch 5000: 48.245452880859375: 0.8283\n",
      "Training finished at 1646076220.0039923; lasted 15.361172676086426 seconds.\n",
      "82.83 % success on test data\n",
      "--- (16) ---\n",
      "Training started at 1646076220.75431\n",
      "Epoch 0: 69.05287170410156: 0.0982\n",
      "Epoch 5000: 47.83522033691406: 0.8238\n",
      "Training finished at 1646076235.1694932; lasted 14.415183305740356 seconds.\n",
      "82.38 % success on test data\n",
      "--- (17) ---\n",
      "Training started at 1646076235.9656901\n",
      "Epoch 0: 69.04755401611328: 0.1046\n",
      "Epoch 5000: 48.834442138671875: 0.8261\n",
      "Training finished at 1646076250.5464091; lasted 14.580718994140625 seconds.\n",
      "82.61 % success on test data\n",
      "--- (18) ---\n",
      "Training started at 1646076251.3260856\n",
      "Epoch 0: 69.24890899658203: 0.1024\n",
      "Epoch 5000: 45.40913009643555: 0.8778\n",
      "Training finished at 1646076265.8993766; lasted 14.573291063308716 seconds.\n",
      "87.78 % success on test data\n",
      "--- (19) ---\n",
      "Training started at 1646076266.6615481\n",
      "Epoch 0: 69.1960678100586: 0.0892\n",
      "Epoch 5000: 46.05748748779297: 0.9053\n",
      "Training finished at 1646076281.0642009; lasted 14.402652740478516 seconds.\n",
      "90.53 % success on test data\n",
      "--- (20) ---\n",
      "Training started at 1646076281.8281891\n",
      "Epoch 0: 68.9679946899414: 0.1135\n",
      "Epoch 5000: 50.0283317565918: 0.7572\n",
      "Training finished at 1646076295.9985151; lasted 14.170325994491577 seconds.\n",
      "75.72 % success on test data\n",
      "--- (21) ---\n",
      "Training started at 1646076296.7736142\n",
      "Epoch 0: 69.05220031738281: 0.101\n",
      "Epoch 5000: 49.84776306152344: 0.7534\n",
      "Training finished at 1646076311.1271708; lasted 14.353556632995605 seconds.\n",
      "75.33999999999999 % success on test data\n",
      "--- (22) ---\n",
      "Training started at 1646076311.8985252\n",
      "Epoch 0: 69.15771484375: 0.101\n",
      "Epoch 5000: 52.391998291015625: 0.6799\n",
      "Training finished at 1646076326.3463838; lasted 14.447858572006226 seconds.\n",
      "67.99 % success on test data\n",
      "--- (23) ---\n",
      "Training started at 1646076327.1570513\n",
      "Epoch 0: 69.08265686035156: 0.101\n",
      "Epoch 5000: 46.91554641723633: 0.8299\n",
      "Training finished at 1646076341.503667; lasted 14.3466157913208 seconds.\n",
      "82.99 % success on test data\n",
      "--- (24) ---\n",
      "Training started at 1646076342.2645853\n",
      "Epoch 0: 69.09642791748047: 0.1372\n",
      "Epoch 5000: 51.774009704589844: 0.7878\n",
      "Training finished at 1646076356.5539618; lasted 14.289376497268677 seconds.\n",
      "78.78 % success on test data\n",
      "--- (25) ---\n",
      "Training started at 1646076357.2961166\n",
      "Epoch 0: 69.13094329833984: 0.0892\n",
      "Epoch 5000: 50.56671142578125: 0.8469\n",
      "Training finished at 1646076371.7163315; lasted 14.420214891433716 seconds.\n",
      "84.69 % success on test data\n",
      "--- (26) ---\n",
      "Training started at 1646076372.4670615\n",
      "Epoch 0: 69.26368713378906: 0.0952\n",
      "Epoch 5000: 46.93778991699219: 0.8871\n",
      "Training finished at 1646076386.7912047; lasted 14.324143171310425 seconds.\n",
      "88.71 % success on test data\n",
      "--- (27) ---\n",
      "Training started at 1646076387.5529723\n",
      "Epoch 0: 69.00162506103516: 0.1135\n",
      "Epoch 5000: 49.86111068725586: 0.8886\n",
      "Training finished at 1646076401.8159602; lasted 14.262987852096558 seconds.\n",
      "88.86 % success on test data\n",
      "--- (28) ---\n",
      "Training started at 1646076402.6034524\n",
      "Epoch 0: 69.22313690185547: 0.1028\n",
      "Epoch 5000: 45.83531188964844: 0.8655\n",
      "Training finished at 1646076417.119927; lasted 14.516474485397339 seconds.\n",
      "86.55000000000001 % success on test data\n",
      "--- (29) ---\n",
      "Training started at 1646076417.8907094\n",
      "Epoch 0: 69.0755615234375: 0.1009\n",
      "Epoch 5000: 48.8321418762207: 0.7514\n",
      "Training finished at 1646076433.289861; lasted 15.39915156364441 seconds.\n",
      "75.14 % success on test data\n",
      "--- (30) ---\n",
      "Training started at 1646076434.1149936\n",
      "Epoch 0: 68.9828109741211: 0.0799\n",
      "Epoch 5000: 48.838932037353516: 0.7853\n",
      "Training finished at 1646076448.668707; lasted 14.553713321685791 seconds.\n",
      "78.53 % success on test data\n",
      "--- (31) ---\n",
      "Training started at 1646076449.4702308\n",
      "Epoch 0: 69.08097839355469: 0.1036\n",
      "Epoch 5000: 48.82390594482422: 0.8379\n",
      "Training finished at 1646076464.0545; lasted 14.584269285202026 seconds.\n",
      "83.78999999999999 % success on test data\n",
      "--- (32) ---\n",
      "Training started at 1646076464.816469\n",
      "Epoch 0: 68.969970703125: 0.1341\n",
      "Epoch 5000: 47.95842742919922: 0.809\n",
      "Training finished at 1646076479.1667655; lasted 14.35029649734497 seconds.\n",
      "80.9 % success on test data\n",
      "--- (33) ---\n",
      "Training started at 1646076479.939747\n",
      "Epoch 0: 69.02798461914062: 0.1028\n",
      "Epoch 5000: 47.822998046875: 0.7513\n",
      "Training finished at 1646076494.5004528; lasted 14.560705661773682 seconds.\n",
      "75.13 % success on test data\n",
      "--- (34) ---\n",
      "Training started at 1646076495.2552419\n",
      "Epoch 0: 68.8893051147461: 0.0958\n",
      "Epoch 5000: 47.89772033691406: 0.8234\n",
      "Training finished at 1646076509.725469; lasted 14.470227241516113 seconds.\n",
      "82.34 % success on test data\n",
      "--- (35) ---\n",
      "Training started at 1646076510.4934134\n",
      "Epoch 0: 69.05634307861328: 0.135\n",
      "Epoch 5000: 48.28071212768555: 0.8346\n",
      "Training finished at 1646076524.9173424; lasted 14.42392897605896 seconds.\n",
      "83.46000000000001 % success on test data\n",
      "--- (36) ---\n",
      "Training started at 1646076525.716654\n",
      "Epoch 0: 69.22238159179688: 0.101\n",
      "Epoch 5000: 47.80406951904297: 0.9095\n",
      "Training finished at 1646076539.9352233; lasted 14.218569278717041 seconds.\n",
      "90.95 % success on test data\n",
      "--- (37) ---\n",
      "Training started at 1646076540.7310202\n",
      "Epoch 0: 69.10074615478516: 0.1009\n",
      "Epoch 5000: 49.846866607666016: 0.8406\n",
      "Training finished at 1646076555.3625765; lasted 14.631556272506714 seconds.\n",
      "84.06 % success on test data\n",
      "--- (38) ---\n",
      "Training started at 1646076556.1749878\n",
      "Epoch 0: 69.04528045654297: 0.0982\n",
      "Epoch 5000: 45.38331604003906: 0.9082\n",
      "Training finished at 1646076570.5109947; lasted 14.336006879806519 seconds.\n",
      "90.82000000000001 % success on test data\n",
      "--- (39) ---\n",
      "Training started at 1646076571.267422\n",
      "Epoch 0: 68.98401641845703: 0.0958\n",
      "Epoch 5000: 45.83719253540039: 0.907\n",
      "Training finished at 1646076585.7820802; lasted 14.514658212661743 seconds.\n",
      "90.7 % success on test data\n",
      "--- (40) ---\n",
      "Training started at 1646076586.5480304\n",
      "Epoch 0: 69.14967346191406: 0.0974\n",
      "Epoch 5000: 51.779537200927734: 0.767\n",
      "Training finished at 1646076601.072261; lasted 14.524230718612671 seconds.\n",
      "76.7 % success on test data\n",
      "--- (41) ---\n",
      "Training started at 1646076601.8135688\n",
      "Epoch 0: 69.08029174804688: 0.1032\n",
      "Epoch 5000: 50.87656784057617: 0.8272\n",
      "Training finished at 1646076616.0636497; lasted 14.250080823898315 seconds.\n",
      "82.72 % success on test data\n",
      "--- (42) ---\n",
      "Training started at 1646076616.811202\n",
      "Epoch 0: 69.16693115234375: 0.1028\n",
      "Epoch 5000: 48.843994140625: 0.7537\n",
      "Training finished at 1646076631.365986; lasted 14.554784059524536 seconds.\n",
      "75.37 % success on test data\n",
      "--- (43) ---\n",
      "Training started at 1646076632.1461637\n",
      "Epoch 0: 68.92964935302734: 0.1135\n",
      "Epoch 5000: 48.835994720458984: 0.7609\n",
      "Training finished at 1646076646.3177564; lasted 14.171592712402344 seconds.\n",
      "76.09 % success on test data\n",
      "--- (44) ---\n",
      "Training started at 1646076647.142357\n",
      "Epoch 0: 69.00165557861328: 0.0981\n",
      "Epoch 5000: 48.84983444213867: 0.8262\n",
      "Training finished at 1646076661.4841974; lasted 14.341840267181396 seconds.\n",
      "82.62 % success on test data\n",
      "--- (45) ---\n",
      "Training started at 1646076662.220545\n",
      "Epoch 0: 68.83041381835938: 0.0958\n",
      "Epoch 5000: 51.805747985839844: 0.7593\n",
      "Training finished at 1646076676.8345797; lasted 14.614034652709961 seconds.\n",
      "75.92999999999999 % success on test data\n",
      "--- (46) ---\n",
      "Training started at 1646076677.632747\n",
      "Epoch 0: 68.9853744506836: 0.0974\n",
      "Epoch 5000: 43.95489501953125: 0.9025\n",
      "Training finished at 1646076692.1541815; lasted 14.521434545516968 seconds.\n",
      "90.25 % success on test data\n",
      "--- (47) ---\n",
      "Training started at 1646076692.9392383\n",
      "Epoch 0: 69.12409973144531: 0.1138\n",
      "Epoch 5000: 45.79328155517578: 0.9125\n",
      "Training finished at 1646076707.216213; lasted 14.27697467803955 seconds.\n",
      "91.25 % success on test data\n",
      "--- (48) ---\n",
      "Training started at 1646076707.9819477\n",
      "Epoch 0: 69.13164520263672: 0.0958\n",
      "Epoch 5000: 51.803714752197266: 0.7128\n",
      "Training finished at 1646076722.7084162; lasted 14.726468563079834 seconds.\n",
      "71.28 % success on test data\n",
      "--- (49) ---\n",
      "Training started at 1646076723.5056179\n",
      "Epoch 0: 69.01415252685547: 0.0958\n",
      "Epoch 5000: 51.86943054199219: 0.6725\n",
      "Training finished at 1646076737.922202; lasted 14.416584253311157 seconds.\n",
      "67.25 % success on test data\n",
      "--- (50) ---\n",
      "Training started at 1646076738.6912704\n",
      "Epoch 0: 68.92491912841797: 0.1032\n",
      "Epoch 5000: 45.000587463378906: 0.8306\n",
      "Training finished at 1646076752.9617708; lasted 14.270500421524048 seconds.\n",
      "83.06 % success on test data\n",
      "--- (51) ---\n",
      "Training started at 1646076753.7250085\n",
      "Epoch 0: 69.1098403930664: 0.101\n",
      "Epoch 5000: 49.83293914794922: 0.7456\n",
      "Training finished at 1646076768.096963; lasted 14.371954441070557 seconds.\n",
      "74.56 % success on test data\n",
      "--- (52) ---\n",
      "Training started at 1646076768.8343966\n",
      "Epoch 0: 69.16410064697266: 0.1009\n",
      "Epoch 5000: 50.75611114501953: 0.7473\n",
      "Training finished at 1646076783.994544; lasted 15.160147428512573 seconds.\n",
      "74.72999999999999 % success on test data\n",
      "--- (53) ---\n",
      "Training started at 1646076784.739182\n",
      "Epoch 0: 69.01481628417969: 0.1223\n",
      "Epoch 5000: 47.78160858154297: 0.8353\n",
      "Training finished at 1646076799.7393217; lasted 15.000139713287354 seconds.\n",
      "83.53 % success on test data\n",
      "--- (54) ---\n",
      "Training started at 1646076800.485247\n",
      "Epoch 0: 69.12613677978516: 0.1009\n",
      "Epoch 5000: 48.8321418762207: 0.8367\n",
      "Training finished at 1646076815.2240772; lasted 14.738830327987671 seconds.\n",
      "83.67 % success on test data\n",
      "--- (55) ---\n",
      "Training started at 1646076815.9863698\n",
      "Epoch 0: 69.04766082763672: 0.0957\n",
      "Epoch 5000: 49.848167419433594: 0.756\n",
      "Training finished at 1646076830.6184416; lasted 14.632071733474731 seconds.\n",
      "75.6 % success on test data\n",
      "--- (56) ---\n",
      "Training started at 1646076831.413514\n",
      "Epoch 0: 69.17066955566406: 0.0905\n",
      "Epoch 5000: 46.035972595214844: 0.8337\n",
      "Training finished at 1646076845.7348356; lasted 14.321321725845337 seconds.\n",
      "83.37 % success on test data\n",
      "--- (57) ---\n",
      "Training started at 1646076846.4678402\n",
      "Epoch 0: 69.17887878417969: 0.0892\n",
      "Epoch 5000: 51.63280487060547: 0.738\n",
      "Training finished at 1646076860.8590899; lasted 14.391249656677246 seconds.\n",
      "73.8 % success on test data\n",
      "--- (58) ---\n",
      "Training started at 1646076861.612177\n",
      "Epoch 0: 69.04938507080078: 0.0883\n",
      "Epoch 5000: 48.93522262573242: 0.7443\n",
      "Training finished at 1646076876.2906744; lasted 14.678497552871704 seconds.\n",
      "74.42999999999999 % success on test data\n",
      "--- (59) ---\n",
      "Training started at 1646076877.1104405\n",
      "Epoch 0: 69.07249450683594: 0.1032\n",
      "Epoch 5000: 49.41750717163086: 0.8393\n",
      "Training finished at 1646076891.4849966; lasted 14.374556064605713 seconds.\n",
      "83.93 % success on test data\n",
      "--- (60) ---\n",
      "Training started at 1646076892.258719\n",
      "Epoch 0: 69.03327941894531: 0.098\n",
      "Epoch 5000: 53.14324951171875: 0.6759\n",
      "Training finished at 1646076906.5745819; lasted 14.315862894058228 seconds.\n",
      "67.58999999999999 % success on test data\n",
      "--- (61) ---\n",
      "Training started at 1646076907.3140795\n",
      "Epoch 0: 69.14741516113281: 0.1009\n",
      "Epoch 5000: 48.835601806640625: 0.9034\n",
      "Training finished at 1646076922.0191088; lasted 14.705029249191284 seconds.\n",
      "90.34 % success on test data\n",
      "--- (62) ---\n",
      "Training started at 1646076922.8123674\n",
      "Epoch 0: 69.0659408569336: 0.1144\n",
      "Epoch 5000: 49.07426071166992: 0.8204\n",
      "Training finished at 1646076937.0751154; lasted 14.262748003005981 seconds.\n",
      "82.04 % success on test data\n",
      "--- (63) ---\n",
      "Training started at 1646076937.8959203\n",
      "Epoch 0: 69.17536926269531: 0.065\n",
      "Epoch 5000: 51.83403015136719: 0.7527\n",
      "Training finished at 1646076952.4628081; lasted 14.566887855529785 seconds.\n",
      "75.27000000000001 % success on test data\n",
      "--- (64) ---\n",
      "Training started at 1646076953.242983\n",
      "Epoch 0: 69.03556060791016: 0.1028\n",
      "Epoch 5000: 47.81973648071289: 0.7518\n",
      "Training finished at 1646076967.8003185; lasted 14.557335376739502 seconds.\n",
      "75.18 % success on test data\n",
      "--- (65) ---\n",
      "Training started at 1646076968.577623\n",
      "Epoch 0: 69.13102722167969: 0.1031\n",
      "Epoch 5000: 49.79863357543945: 0.823\n",
      "Training finished at 1646076983.0248394; lasted 14.447216510772705 seconds.\n",
      "82.3 % success on test data\n",
      "--- (66) ---\n",
      "Training started at 1646076983.8184376\n",
      "Epoch 0: 69.04947662353516: 0.1201\n",
      "Epoch 5000: 46.79251480102539: 0.8241\n",
      "Training finished at 1646076998.1179817; lasted 14.299544095993042 seconds.\n",
      "82.41000000000001 % success on test data\n",
      "--- (67) ---\n",
      "Training started at 1646076998.886791\n",
      "Epoch 0: 69.0999984741211: 0.098\n",
      "Epoch 5000: 45.37580108642578: 0.8791\n",
      "Training finished at 1646077013.1299007; lasted 14.243109703063965 seconds.\n",
      "87.91 % success on test data\n",
      "--- (68) ---\n",
      "Training started at 1646077013.891059\n",
      "Epoch 0: 69.01564025878906: 0.0999\n",
      "Epoch 5000: 51.76431655883789: 0.7345\n",
      "Training finished at 1646077028.2198179; lasted 14.328758955001831 seconds.\n",
      "73.45 % success on test data\n",
      "--- (69) ---\n",
      "Training started at 1646077029.0005825\n",
      "Epoch 0: 69.25970458984375: 0.101\n",
      "Epoch 5000: 47.3928337097168: 0.8829\n",
      "Training finished at 1646077043.433047; lasted 14.432464599609375 seconds.\n",
      "88.29 % success on test data\n",
      "--- (70) ---\n",
      "Training started at 1646077044.1982071\n",
      "Epoch 0: 69.04507446289062: 0.1047\n",
      "Epoch 5000: 52.69929122924805: 0.7573\n",
      "Training finished at 1646077058.567172; lasted 14.368964910507202 seconds.\n",
      "75.73 % success on test data\n",
      "--- (71) ---\n",
      "Training started at 1646077059.321323\n",
      "Epoch 0: 69.0479736328125: 0.1135\n",
      "Epoch 5000: 48.040252685546875: 0.7207\n",
      "Training finished at 1646077073.7114635; lasted 14.390140533447266 seconds.\n",
      "72.07000000000001 % success on test data\n",
      "--- (72) ---\n",
      "Training started at 1646077074.4962811\n",
      "Epoch 0: 68.92634582519531: 0.0892\n",
      "Epoch 5000: 50.290977478027344: 0.8312\n",
      "Training finished at 1646077089.1058216; lasted 14.609540462493896 seconds.\n",
      "83.12 % success on test data\n",
      "--- (73) ---\n",
      "Training started at 1646077089.85164\n",
      "Epoch 0: 69.0441665649414: 0.098\n",
      "Epoch 5000: 50.110225677490234: 0.7915\n",
      "Training finished at 1646077104.0115802; lasted 14.159940242767334 seconds.\n",
      "79.14999999999999 % success on test data\n",
      "--- (74) ---\n",
      "Training started at 1646077104.7786987\n",
      "Epoch 0: 68.88925170898438: 0.1135\n",
      "Epoch 5000: 46.66321563720703: 0.8313\n",
      "Training finished at 1646077119.0680182; lasted 14.289319515228271 seconds.\n",
      "83.13000000000001 % success on test data\n",
      "--- (75) ---\n",
      "Training started at 1646077119.8331907\n",
      "Epoch 0: 69.07234954833984: 0.0958\n",
      "Epoch 5000: 51.7530632019043: 0.8323\n",
      "Training finished at 1646077134.453551; lasted 14.620360374450684 seconds.\n",
      "83.23 % success on test data\n",
      "--- (76) ---\n",
      "Training started at 1646077135.2223186\n",
      "Epoch 0: 69.06785583496094: 0.1009\n",
      "Epoch 5000: 48.78498077392578: 0.899\n",
      "Training finished at 1646077149.8657362; lasted 14.643417596817017 seconds.\n",
      "89.9 % success on test data\n",
      "--- (77) ---\n",
      "Training started at 1646077150.6366267\n",
      "Epoch 0: 69.02093505859375: 0.101\n",
      "Epoch 5000: 51.83635711669922: 0.7469\n",
      "Training finished at 1646077165.0498092; lasted 14.413182497024536 seconds.\n",
      "74.69 % success on test data\n",
      "--- (78) ---\n",
      "Training started at 1646077165.837982\n",
      "Epoch 0: 69.11090087890625: 0.0974\n",
      "Epoch 5000: 47.806884765625: 0.7534\n",
      "Training finished at 1646077180.5532756; lasted 14.715293645858765 seconds.\n",
      "75.33999999999999 % success on test data\n",
      "--- (79) ---\n",
      "Training started at 1646077181.3362918\n",
      "Epoch 0: 69.17037200927734: 0.098\n",
      "Epoch 5000: 49.853912353515625: 0.82\n",
      "Training finished at 1646077195.5945642; lasted 14.258272409439087 seconds.\n",
      "82.0 % success on test data\n",
      "--- (80) ---\n",
      "Training started at 1646077196.3385763\n",
      "Epoch 0: 69.253662109375: 0.1002\n",
      "Epoch 5000: 58.745880126953125: 0.7454\n",
      "Training finished at 1646077210.7322826; lasted 14.393706321716309 seconds.\n",
      "74.53999999999999 % success on test data\n",
      "--- (81) ---\n",
      "Training started at 1646077211.5359132\n",
      "Epoch 0: 69.12969970703125: 0.0747\n",
      "Epoch 5000: 53.78022766113281: 0.6839\n",
      "Training finished at 1646077225.810343; lasted 14.27442979812622 seconds.\n",
      "68.39 % success on test data\n",
      "--- (82) ---\n",
      "Training started at 1646077226.5835469\n",
      "Epoch 0: 69.08097839355469: 0.1258\n",
      "Epoch 5000: 50.93760299682617: 0.8916\n",
      "Training finished at 1646077240.7585454; lasted 14.17499852180481 seconds.\n",
      "89.16 % success on test data\n",
      "--- (83) ---\n",
      "Training started at 1646077241.525503\n",
      "Epoch 0: 69.03267669677734: 0.1135\n",
      "Epoch 5000: 44.832984924316406: 0.8305\n",
      "Training finished at 1646077255.794058; lasted 14.268555164337158 seconds.\n",
      "83.05 % success on test data\n",
      "--- (84) ---\n",
      "Training started at 1646077256.536767\n",
      "Epoch 0: 69.13729858398438: 0.101\n",
      "Epoch 5000: 46.10636901855469: 0.8192\n",
      "Training finished at 1646077270.9179816; lasted 14.381214618682861 seconds.\n",
      "81.92 % success on test data\n",
      "--- (85) ---\n",
      "Training started at 1646077271.6941562\n",
      "Epoch 0: 69.38103485107422: 0.101\n",
      "Epoch 5000: 49.81573486328125: 0.8186\n",
      "Training finished at 1646077286.139134; lasted 14.444977760314941 seconds.\n",
      "81.86 % success on test data\n",
      "--- (86) ---\n",
      "Training started at 1646077286.8904421\n",
      "Epoch 0: 69.14118194580078: 0.1009\n",
      "Epoch 5000: 47.219566345214844: 0.8397\n",
      "Training finished at 1646077301.5481255; lasted 14.657683372497559 seconds.\n",
      "83.97 % success on test data\n",
      "--- (87) ---\n",
      "Training started at 1646077302.3029637\n",
      "Epoch 0: 69.12175750732422: 0.1135\n",
      "Epoch 5000: 45.813209533691406: 0.8202\n",
      "Training finished at 1646077316.6151245; lasted 14.312160730361938 seconds.\n",
      "82.02000000000001 % success on test data\n",
      "--- (88) ---\n",
      "Training started at 1646077317.3649762\n",
      "Epoch 0: 69.06072235107422: 0.098\n",
      "Epoch 5000: 51.841583251953125: 0.7585\n",
      "Training finished at 1646077331.6977148; lasted 14.332738637924194 seconds.\n",
      "75.85 % success on test data\n",
      "--- (89) ---\n",
      "Training started at 1646077332.4745545\n",
      "Epoch 0: 69.04761505126953: 0.0982\n",
      "Epoch 5000: 48.82050323486328: 0.8119\n",
      "Training finished at 1646077346.925471; lasted 14.450916528701782 seconds.\n",
      "81.19 % success on test data\n",
      "--- (90) ---\n",
      "Training started at 1646077347.6716218\n",
      "Epoch 0: 69.05184936523438: 0.0934\n",
      "Epoch 5000: 50.788082122802734: 0.8314\n",
      "Training finished at 1646077362.2539933; lasted 14.582371473312378 seconds.\n",
      "83.14 % success on test data\n",
      "--- (91) ---\n",
      "Training started at 1646077363.0359955\n",
      "Epoch 0: 69.12166595458984: 0.1009\n",
      "Epoch 5000: 47.82637023925781: 0.8341\n",
      "Training finished at 1646077377.823942; lasted 14.787946462631226 seconds.\n",
      "83.41 % success on test data\n",
      "--- (92) ---\n",
      "Training started at 1646077378.622273\n",
      "Epoch 0: 69.07513427734375: 0.098\n",
      "Epoch 5000: 47.87059783935547: 0.8384\n",
      "Training finished at 1646077392.9009192; lasted 14.278646230697632 seconds.\n",
      "83.84 % success on test data\n",
      "--- (93) ---\n",
      "Training started at 1646077393.6884766\n",
      "Epoch 0: 69.0698013305664: 0.1135\n",
      "Epoch 5000: 50.82095718383789: 0.7475\n",
      "Training finished at 1646077407.9928944; lasted 14.304417848587036 seconds.\n",
      "74.75 % success on test data\n",
      "--- (94) ---\n",
      "Training started at 1646077408.888814\n",
      "Epoch 0: 69.08061218261719: 0.0982\n",
      "Epoch 5000: 46.455780029296875: 0.891\n",
      "Training finished at 1646077423.2643173; lasted 14.375503301620483 seconds.\n",
      "89.1 % success on test data\n",
      "--- (95) ---\n",
      "Training started at 1646077424.0304897\n",
      "Epoch 0: 69.10594940185547: 0.0982\n",
      "Epoch 5000: 51.219017028808594: 0.8056\n",
      "Training finished at 1646077438.7081585; lasted 14.677668809890747 seconds.\n",
      "80.56 % success on test data\n",
      "--- (96) ---\n",
      "Training started at 1646077439.4753172\n",
      "Epoch 0: 69.21792602539062: 0.1135\n",
      "Epoch 5000: 47.530479431152344: 0.8471\n",
      "Training finished at 1646077453.7745445; lasted 14.299227237701416 seconds.\n",
      "84.71 % success on test data\n",
      "--- (97) ---\n",
      "Training started at 1646077454.5375588\n",
      "Epoch 0: 69.10102081298828: 0.098\n",
      "Epoch 5000: 47.83442687988281: 0.831\n",
      "Training finished at 1646077468.8358319; lasted 14.298273086547852 seconds.\n",
      "83.1 % success on test data\n",
      "--- (98) ---\n",
      "Training started at 1646077469.5937622\n",
      "Epoch 0: 68.96118927001953: 0.1009\n",
      "Epoch 5000: 47.33107376098633: 0.9111\n",
      "Training finished at 1646077484.6252465; lasted 15.031484365463257 seconds.\n",
      "91.11 % success on test data\n",
      "--- (99) ---\n",
      "Training started at 1646077485.3984203\n",
      "Epoch 0: 69.07347106933594: 0.101\n",
      "Epoch 5000: 47.79016876220703: 0.838\n",
      "Training finished at 1646077499.8616226; lasted 14.463202238082886 seconds.\n",
      "83.8 % success on test data\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(f'--- ({i}) ---')\n",
    "    model = ModelManager.get_untrained(ModelType.MnistMLPNevil)\n",
    "    Coach.train(\n",
    "        model,\n",
    "        data,\n",
    "        nn.CrossEntropyLoss(reduction='sum'),\n",
    "        optim.SGD(model.parameters(), lr=1e-2),\n",
    "        30,\n",
    "        5001,\n",
    "        5000\n",
    "    )\n",
    "    simple.append(Coach.measure_performance(model, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "adagrad = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- (0) ---\n",
      "Training started at 1646082793.5258584\n",
      "Epoch 0: 68.95343780517578: 0.0892\n",
      "Epoch 5000: 51.93141555786133: 0.7826\n",
      "Training finished at 1646082808.6109035; lasted 15.085045099258423 seconds.\n",
      "78.25999999999999 % success on test data\n",
      "--- (1) ---\n",
      "Training started at 1646082809.4161901\n",
      "Epoch 0: 69.30687713623047: 0.1009\n",
      "Epoch 5000: 51.42715072631836: 0.7228\n",
      "Training finished at 1646082824.9044; lasted 15.488209962844849 seconds.\n",
      "72.28 % success on test data\n",
      "--- (2) ---\n",
      "Training started at 1646082825.7683408\n",
      "Epoch 0: 69.20330810546875: 0.0892\n",
      "Epoch 5000: 50.70942306518555: 0.7253\n",
      "Training finished at 1646082841.0360653; lasted 15.267724514007568 seconds.\n",
      "72.53 % success on test data\n",
      "--- (3) ---\n",
      "Training started at 1646082841.8006341\n",
      "Epoch 0: 69.26296997070312: 0.0974\n",
      "Epoch 5000: 46.150146484375: 0.8034\n",
      "Training finished at 1646082857.4610188; lasted 15.66038465499878 seconds.\n",
      "80.34 % success on test data\n",
      "--- (4) ---\n",
      "Training started at 1646082858.2579787\n",
      "Epoch 0: 69.2431640625: 0.1047\n",
      "Epoch 5000: 47.78483581542969: 0.8256\n",
      "Training finished at 1646082873.4023218; lasted 15.144343137741089 seconds.\n",
      "82.56 % success on test data\n",
      "--- (5) ---\n",
      "Training started at 1646082874.1767788\n",
      "Epoch 0: 69.14705657958984: 0.1025\n",
      "Epoch 5000: 50.19486618041992: 0.736\n",
      "Training finished at 1646082889.109482; lasted 14.932703256607056 seconds.\n",
      "73.6 % success on test data\n",
      "--- (6) ---\n",
      "Training started at 1646082889.9075413\n",
      "Epoch 0: 69.23839569091797: 0.0892\n",
      "Epoch 5000: 49.056243896484375: 0.7731\n",
      "Training finished at 1646082904.9768767; lasted 15.069335460662842 seconds.\n",
      "77.31 % success on test data\n",
      "--- (7) ---\n",
      "Training started at 1646082905.775823\n",
      "Epoch 0: 69.05225372314453: 0.0958\n",
      "Epoch 5000: 50.401668548583984: 0.8165\n",
      "Training finished at 1646082920.8431373; lasted 15.067314147949219 seconds.\n",
      "81.65 % success on test data\n",
      "--- (8) ---\n",
      "Training started at 1646082921.6070328\n",
      "Epoch 0: 69.07521057128906: 0.0953\n",
      "Epoch 5000: 51.399044036865234: 0.7934\n",
      "Training finished at 1646082936.8064094; lasted 15.199376583099365 seconds.\n",
      "79.34 % success on test data\n",
      "--- (9) ---\n",
      "Training started at 1646082937.5553832\n",
      "Epoch 0: 69.154296875: 0.1009\n",
      "Epoch 5000: 49.022735595703125: 0.7307\n",
      "Training finished at 1646082952.7569911; lasted 15.201607942581177 seconds.\n",
      "73.07000000000001 % success on test data\n",
      "--- (10) ---\n",
      "Training started at 1646082953.5605237\n",
      "Epoch 0: 68.9986343383789: 0.1028\n",
      "Epoch 5000: 49.3277587890625: 0.8758\n",
      "Training finished at 1646082968.6019852; lasted 15.04146146774292 seconds.\n",
      "87.58 % success on test data\n",
      "--- (11) ---\n",
      "Training started at 1646082969.3991385\n",
      "Epoch 0: 69.12934875488281: 0.0951\n",
      "Epoch 5000: 51.6966552734375: 0.784\n",
      "Training finished at 1646082984.5217817; lasted 15.122643232345581 seconds.\n",
      "78.4 % success on test data\n",
      "--- (12) ---\n",
      "Training started at 1646082985.336277\n",
      "Epoch 0: 68.99383544921875: 0.0892\n",
      "Epoch 5000: 52.90647888183594: 0.8322\n",
      "Training finished at 1646083000.420993; lasted 15.084716081619263 seconds.\n",
      "83.22 % success on test data\n",
      "--- (13) ---\n",
      "Training started at 1646083001.2054977\n",
      "Epoch 0: 69.11578369140625: 0.1031\n",
      "Epoch 5000: 46.86174774169922: 0.7961\n",
      "Training finished at 1646083016.1698072; lasted 14.964309453964233 seconds.\n",
      "79.61 % success on test data\n",
      "--- (14) ---\n",
      "Training started at 1646083016.9199607\n",
      "Epoch 0: 69.20697784423828: 0.1185\n",
      "Epoch 5000: 47.65952682495117: 0.8569\n",
      "Training finished at 1646083031.7381; lasted 14.81813931465149 seconds.\n",
      "85.69 % success on test data\n",
      "--- (15) ---\n",
      "Training started at 1646083032.5027182\n",
      "Epoch 0: 69.10334777832031: 0.1065\n",
      "Epoch 5000: 52.008636474609375: 0.7793\n",
      "Training finished at 1646083047.437412; lasted 14.934693813323975 seconds.\n",
      "77.92999999999999 % success on test data\n",
      "--- (16) ---\n",
      "Training started at 1646083048.2033665\n",
      "Epoch 0: 69.08169555664062: 0.0958\n",
      "Epoch 5000: 49.00659942626953: 0.7395\n",
      "Training finished at 1646083063.241683; lasted 15.038316488265991 seconds.\n",
      "73.95 % success on test data\n",
      "--- (17) ---\n",
      "Training started at 1646083064.026635\n",
      "Epoch 0: 69.186767578125: 0.1192\n",
      "Epoch 5000: 53.10676956176758: 0.7524\n",
      "Training finished at 1646083079.2159498; lasted 15.189314842224121 seconds.\n",
      "75.24 % success on test data\n",
      "--- (18) ---\n",
      "Training started at 1646083079.9958725\n",
      "Epoch 0: 68.92279052734375: 0.0957\n",
      "Epoch 5000: 51.03606033325195: 0.7477\n",
      "Training finished at 1646083096.2231658; lasted 16.227293252944946 seconds.\n",
      "74.77000000000001 % success on test data\n",
      "--- (19) ---\n",
      "Training started at 1646083097.0164452\n",
      "Epoch 0: 69.29305267333984: 0.0985\n",
      "Epoch 5000: 50.077613830566406: 0.7922\n",
      "Training finished at 1646083112.8584213; lasted 15.841976165771484 seconds.\n",
      "79.22 % success on test data\n",
      "--- (20) ---\n",
      "Training started at 1646083113.6463778\n",
      "Epoch 0: 69.05296325683594: 0.105\n",
      "Epoch 5000: 48.20172119140625: 0.8566\n",
      "Training finished at 1646083128.859725; lasted 15.21334719657898 seconds.\n",
      "85.66 % success on test data\n",
      "--- (21) ---\n",
      "Training started at 1646083129.6546714\n",
      "Epoch 0: 69.1268539428711: 0.0892\n",
      "Epoch 5000: 53.821197509765625: 0.7013\n",
      "Training finished at 1646083145.1580272; lasted 15.503355741500854 seconds.\n",
      "70.13000000000001 % success on test data\n",
      "--- (22) ---\n",
      "Training started at 1646083145.9628747\n",
      "Epoch 0: 69.082275390625: 0.0982\n",
      "Epoch 5000: 49.31658172607422: 0.8546\n",
      "Training finished at 1646083165.067052; lasted 19.104177236557007 seconds.\n",
      "85.46000000000001 % success on test data\n",
      "--- (23) ---\n",
      "Training started at 1646083165.8689854\n",
      "Epoch 0: 69.20600128173828: 0.1032\n",
      "Epoch 5000: 47.90831756591797: 0.8814\n",
      "Training finished at 1646083183.512474; lasted 17.64348864555359 seconds.\n",
      "88.14 % success on test data\n",
      "--- (24) ---\n",
      "Training started at 1646083184.2881706\n",
      "Epoch 0: 69.21098327636719: 0.1096\n",
      "Epoch 5000: 50.43832778930664: 0.732\n",
      "Training finished at 1646083199.2345262; lasted 14.94635558128357 seconds.\n",
      "73.2 % success on test data\n",
      "--- (25) ---\n",
      "Training started at 1646083199.9609518\n",
      "Epoch 0: 69.08172607421875: 0.0815\n",
      "Epoch 5000: 48.66008377075195: 0.79\n",
      "Training finished at 1646083214.7448015; lasted 14.783849716186523 seconds.\n",
      "79.0 % success on test data\n",
      "--- (26) ---\n",
      "Training started at 1646083215.5610428\n",
      "Epoch 0: 69.10269927978516: 0.1073\n",
      "Epoch 5000: 46.60770797729492: 0.8689\n",
      "Training finished at 1646083230.5030925; lasted 14.942049741744995 seconds.\n",
      "86.89 % success on test data\n",
      "--- (27) ---\n",
      "Training started at 1646083231.2950532\n",
      "Epoch 0: 69.11579895019531: 0.0974\n",
      "Epoch 5000: 50.72867202758789: 0.812\n",
      "Training finished at 1646083246.4973354; lasted 15.202282190322876 seconds.\n",
      "81.2 % success on test data\n",
      "--- (28) ---\n",
      "Training started at 1646083247.2602472\n",
      "Epoch 0: 69.06231689453125: 0.0982\n",
      "Epoch 5000: 49.75379943847656: 0.8314\n",
      "Training finished at 1646083262.315642; lasted 15.055394887924194 seconds.\n",
      "83.14 % success on test data\n",
      "--- (29) ---\n",
      "Training started at 1646083263.1630075\n",
      "Epoch 0: 69.1774673461914: 0.0982\n",
      "Epoch 5000: 47.0725212097168: 0.8655\n",
      "Training finished at 1646083278.1975768; lasted 15.034569263458252 seconds.\n",
      "86.55000000000001 % success on test data\n",
      "--- (30) ---\n",
      "Training started at 1646083278.9645689\n",
      "Epoch 0: 69.07142639160156: 0.1105\n",
      "Epoch 5000: 46.27225875854492: 0.8636\n",
      "Training finished at 1646083293.945867; lasted 14.981298208236694 seconds.\n",
      "86.36 % success on test data\n",
      "--- (31) ---\n",
      "Training started at 1646083294.7188382\n",
      "Epoch 0: 69.15380096435547: 0.1009\n",
      "Epoch 5000: 53.90685272216797: 0.7275\n",
      "Training finished at 1646083309.9480827; lasted 15.229244470596313 seconds.\n",
      "72.75 % success on test data\n",
      "--- (32) ---\n",
      "Training started at 1646083310.7375982\n",
      "Epoch 0: 69.35630798339844: 0.1009\n",
      "Epoch 5000: 48.08975601196289: 0.8163\n",
      "Training finished at 1646083325.922873; lasted 15.185274839401245 seconds.\n",
      "81.63 % success on test data\n",
      "--- (33) ---\n",
      "Training started at 1646083326.7226546\n",
      "Epoch 0: 69.02015686035156: 0.0892\n",
      "Epoch 5000: 51.51095962524414: 0.7451\n",
      "Training finished at 1646083341.705536; lasted 14.982881307601929 seconds.\n",
      "74.51 % success on test data\n",
      "--- (34) ---\n",
      "Training started at 1646083342.5068014\n",
      "Epoch 0: 69.13690185546875: 0.1021\n",
      "Epoch 5000: 47.741294860839844: 0.7894\n",
      "Training finished at 1646083357.5033526; lasted 14.996551275253296 seconds.\n",
      "78.94 % success on test data\n",
      "--- (35) ---\n",
      "Training started at 1646083358.3051267\n",
      "Epoch 0: 69.16374969482422: 0.0908\n",
      "Epoch 5000: 49.87601089477539: 0.7996\n",
      "Training finished at 1646083373.3522418; lasted 15.047115087509155 seconds.\n",
      "79.96 % success on test data\n",
      "--- (36) ---\n",
      "Training started at 1646083374.0972478\n",
      "Epoch 0: 69.0533447265625: 0.2032\n",
      "Epoch 5000: 52.494606018066406: 0.8718\n",
      "Training finished at 1646083389.1846213; lasted 15.087373495101929 seconds.\n",
      "87.18 % success on test data\n",
      "--- (37) ---\n",
      "Training started at 1646083389.9676917\n",
      "Epoch 0: 69.20955657958984: 0.1046\n",
      "Epoch 5000: 50.27919387817383: 0.8146\n",
      "Training finished at 1646083405.1628423; lasted 15.19515061378479 seconds.\n",
      "81.46 % success on test data\n",
      "--- (38) ---\n",
      "Training started at 1646083405.9308276\n",
      "Epoch 0: 69.00462341308594: 0.1135\n",
      "Epoch 5000: 50.82362747192383: 0.8081\n",
      "Training finished at 1646083420.635139; lasted 14.70431137084961 seconds.\n",
      "80.81 % success on test data\n",
      "--- (39) ---\n",
      "Training started at 1646083421.3854077\n",
      "Epoch 0: 69.07090759277344: 0.1745\n",
      "Epoch 5000: 53.84438705444336: 0.8011\n",
      "Training finished at 1646083436.4254305; lasted 15.040022850036621 seconds.\n",
      "80.11 % success on test data\n",
      "--- (40) ---\n",
      "Training started at 1646083437.207375\n",
      "Epoch 0: 68.97186279296875: 0.098\n",
      "Epoch 5000: 52.34304428100586: 0.8253\n",
      "Training finished at 1646083452.051678; lasted 14.844302892684937 seconds.\n",
      "82.53 % success on test data\n",
      "--- (41) ---\n",
      "Training started at 1646083452.8216581\n",
      "Epoch 0: 69.02529907226562: 0.1112\n",
      "Epoch 5000: 47.910736083984375: 0.8606\n",
      "Training finished at 1646083468.0109127; lasted 15.189254522323608 seconds.\n",
      "86.06 % success on test data\n",
      "--- (42) ---\n",
      "Training started at 1646083468.7849314\n",
      "Epoch 0: 69.19795989990234: 0.0982\n",
      "Epoch 5000: 52.910152435302734: 0.7268\n",
      "Training finished at 1646083483.7482362; lasted 14.9633047580719 seconds.\n",
      "72.68 % success on test data\n",
      "--- (43) ---\n",
      "Training started at 1646083484.5154555\n",
      "Epoch 0: 69.12142944335938: 0.1293\n",
      "Epoch 5000: 50.00849151611328: 0.8281\n",
      "Training finished at 1646083499.4055765; lasted 14.89012098312378 seconds.\n",
      "82.80999999999999 % success on test data\n",
      "--- (44) ---\n",
      "Training started at 1646083500.1743617\n",
      "Epoch 0: 69.09955596923828: 0.1028\n",
      "Epoch 5000: 45.51476287841797: 0.8873\n",
      "Training finished at 1646083515.246868; lasted 15.072506189346313 seconds.\n",
      "88.73 % success on test data\n",
      "--- (45) ---\n",
      "Training started at 1646083516.0168433\n",
      "Epoch 0: 68.89163970947266: 0.0985\n",
      "Epoch 5000: 50.47049331665039: 0.8789\n",
      "Training finished at 1646083530.8134384; lasted 14.796595096588135 seconds.\n",
      "87.89 % success on test data\n",
      "--- (46) ---\n",
      "Training started at 1646083531.5634801\n",
      "Epoch 0: 68.94117736816406: 0.1009\n",
      "Epoch 5000: 52.10218048095703: 0.8072\n",
      "Training finished at 1646083546.7586184; lasted 15.195138216018677 seconds.\n",
      "80.72 % success on test data\n",
      "--- (47) ---\n",
      "Training started at 1646083547.5493684\n",
      "Epoch 0: 69.14689636230469: 0.1028\n",
      "Epoch 5000: 48.116397857666016: 0.744\n",
      "Training finished at 1646083562.6866663; lasted 15.137297868728638 seconds.\n",
      "74.4 % success on test data\n",
      "--- (48) ---\n",
      "Training started at 1646083563.4121299\n",
      "Epoch 0: 69.06086730957031: 0.1001\n",
      "Epoch 5000: 55.84258270263672: 0.7324\n",
      "Training finished at 1646083578.306955; lasted 14.894825220108032 seconds.\n",
      "73.24000000000001 % success on test data\n",
      "--- (49) ---\n",
      "Training started at 1646083579.036285\n",
      "Epoch 0: 68.93116760253906: 0.1028\n",
      "Epoch 5000: 47.82143020629883: 0.8215\n",
      "Training finished at 1646083594.0812373; lasted 15.044952392578125 seconds.\n",
      "82.15 % success on test data\n",
      "--- (50) ---\n",
      "Training started at 1646083594.8302236\n",
      "Epoch 0: 69.16376495361328: 0.0892\n",
      "Epoch 5000: 51.16596984863281: 0.8726\n",
      "Training finished at 1646083609.9160147; lasted 15.085791110992432 seconds.\n",
      "87.26 % success on test data\n",
      "--- (51) ---\n",
      "Training started at 1646083610.6914036\n",
      "Epoch 0: 69.17781066894531: 0.1016\n",
      "Epoch 5000: 48.9542236328125: 0.7392\n",
      "Training finished at 1646083625.6957788; lasted 15.004375219345093 seconds.\n",
      "73.92 % success on test data\n",
      "--- (52) ---\n",
      "Training started at 1646083626.4592838\n",
      "Epoch 0: 69.03751373291016: 0.1515\n",
      "Epoch 5000: 50.80706787109375: 0.8006\n",
      "Training finished at 1646083641.2727032; lasted 14.813419342041016 seconds.\n",
      "80.06 % success on test data\n",
      "--- (53) ---\n",
      "Training started at 1646083642.05765\n",
      "Epoch 0: 69.11387634277344: 0.0982\n",
      "Epoch 5000: 54.32334899902344: 0.7368\n",
      "Training finished at 1646083657.09405; lasted 15.036399841308594 seconds.\n",
      "73.68 % success on test data\n",
      "--- (54) ---\n",
      "Training started at 1646083657.8429966\n",
      "Epoch 0: 69.1614761352539: 0.1028\n",
      "Epoch 5000: 53.60263442993164: 0.7454\n",
      "Training finished at 1646083672.8873348; lasted 15.04433822631836 seconds.\n",
      "74.53999999999999 % success on test data\n",
      "--- (55) ---\n",
      "Training started at 1646083673.6304543\n",
      "Epoch 0: 68.90047454833984: 0.101\n",
      "Epoch 5000: 48.18864822387695: 0.8233\n",
      "Training finished at 1646083688.5675976; lasted 14.937143325805664 seconds.\n",
      "82.33 % success on test data\n",
      "--- (56) ---\n",
      "Training started at 1646083689.3155913\n",
      "Epoch 0: 68.99214172363281: 0.1347\n",
      "Epoch 5000: 46.837890625: 0.8571\n",
      "Training finished at 1646083704.2669768; lasted 14.951385498046875 seconds.\n",
      "85.71 % success on test data\n",
      "--- (57) ---\n",
      "Training started at 1646083705.0308425\n",
      "Epoch 0: 69.03666687011719: 0.1032\n",
      "Epoch 5000: 49.161476135253906: 0.8155\n",
      "Training finished at 1646083719.921689; lasted 14.890846490859985 seconds.\n",
      "81.55 % success on test data\n",
      "--- (58) ---\n",
      "Training started at 1646083720.6716187\n",
      "Epoch 0: 68.9210205078125: 0.0958\n",
      "Epoch 5000: 51.940738677978516: 0.8151\n",
      "Training finished at 1646083735.7974646; lasted 15.125845909118652 seconds.\n",
      "81.51 % success on test data\n",
      "--- (59) ---\n",
      "Training started at 1646083736.544475\n",
      "Epoch 0: 69.19517517089844: 0.0892\n",
      "Epoch 5000: 48.005828857421875: 0.8782\n",
      "Training finished at 1646083751.621664; lasted 15.077188968658447 seconds.\n",
      "87.82 % success on test data\n",
      "--- (60) ---\n",
      "Training started at 1646083752.4157002\n",
      "Epoch 0: 69.17745971679688: 0.0958\n",
      "Epoch 5000: 47.66445541381836: 0.7969\n",
      "Training finished at 1646083767.5292456; lasted 15.113545417785645 seconds.\n",
      "79.69000000000001 % success on test data\n",
      "--- (61) ---\n",
      "Training started at 1646083768.3193178\n",
      "Epoch 0: 69.22946166992188: 0.101\n",
      "Epoch 5000: 51.2994270324707: 0.7096\n",
      "Training finished at 1646083783.3728793; lasted 15.053561449050903 seconds.\n",
      "70.96000000000001 % success on test data\n",
      "--- (62) ---\n",
      "Training started at 1646083784.1792643\n",
      "Epoch 0: 69.07569122314453: 0.1464\n",
      "Epoch 5000: 51.20830154418945: 0.8069\n",
      "Training finished at 1646083799.245643; lasted 15.066378593444824 seconds.\n",
      "80.69 % success on test data\n",
      "--- (63) ---\n",
      "Training started at 1646083800.0015311\n",
      "Epoch 0: 69.17778015136719: 0.1135\n",
      "Epoch 5000: 53.67066955566406: 0.7368\n",
      "Training finished at 1646083814.910893; lasted 14.909361839294434 seconds.\n",
      "73.68 % success on test data\n",
      "--- (64) ---\n",
      "Training started at 1646083815.7173994\n",
      "Epoch 0: 68.9202880859375: 0.0895\n",
      "Epoch 5000: 50.73793029785156: 0.8055\n",
      "Training finished at 1646083830.7824485; lasted 15.065049171447754 seconds.\n",
      "80.55 % success on test data\n",
      "--- (65) ---\n",
      "Training started at 1646083831.564169\n",
      "Epoch 0: 69.0519790649414: 0.098\n",
      "Epoch 5000: 49.392311096191406: 0.8153\n",
      "Training finished at 1646083846.4161901; lasted 14.852021217346191 seconds.\n",
      "81.53 % success on test data\n",
      "--- (66) ---\n",
      "Training started at 1646083847.2063887\n",
      "Epoch 0: 69.02915954589844: 0.1009\n",
      "Epoch 5000: 55.26658630371094: 0.725\n",
      "Training finished at 1646083862.4987464; lasted 15.292357683181763 seconds.\n",
      "72.5 % success on test data\n",
      "--- (67) ---\n",
      "Training started at 1646083863.2653677\n",
      "Epoch 0: 68.98719787597656: 0.0892\n",
      "Epoch 5000: 47.67585754394531: 0.8107\n",
      "Training finished at 1646083878.3520036; lasted 15.086635828018188 seconds.\n",
      "81.07 % success on test data\n",
      "--- (68) ---\n",
      "Training started at 1646083879.1029265\n",
      "Epoch 0: 68.9246826171875: 0.1182\n",
      "Epoch 5000: 50.442161560058594: 0.7185\n",
      "Training finished at 1646083894.4067879; lasted 15.303861379623413 seconds.\n",
      "71.85000000000001 % success on test data\n",
      "--- (69) ---\n",
      "Training started at 1646083895.185397\n",
      "Epoch 0: 69.06864929199219: 0.1068\n",
      "Epoch 5000: 47.447166442871094: 0.8794\n",
      "Training finished at 1646083910.0448709; lasted 14.859473943710327 seconds.\n",
      "87.94 % success on test data\n",
      "--- (70) ---\n",
      "Training started at 1646083910.8065357\n",
      "Epoch 0: 68.71068572998047: 0.191\n",
      "Epoch 5000: 49.802703857421875: 0.8048\n",
      "Training finished at 1646083925.7972949; lasted 14.990759134292603 seconds.\n",
      "80.47999999999999 % success on test data\n",
      "--- (71) ---\n",
      "Training started at 1646083926.5622737\n",
      "Epoch 0: 69.1443099975586: 0.1011\n",
      "Epoch 5000: 48.604312896728516: 0.8015\n",
      "Training finished at 1646083941.6592991; lasted 15.097025394439697 seconds.\n",
      "80.15 % success on test data\n",
      "--- (72) ---\n",
      "Training started at 1646083942.40707\n",
      "Epoch 0: 69.20691680908203: 0.098\n",
      "Epoch 5000: 51.792476654052734: 0.7962\n",
      "Training finished at 1646083957.229371; lasted 14.822301149368286 seconds.\n",
      "79.62 % success on test data\n",
      "--- (73) ---\n",
      "Training started at 1646083958.016352\n",
      "Epoch 0: 69.10150146484375: 0.0974\n",
      "Epoch 5000: 47.64554977416992: 0.8615\n",
      "Training finished at 1646083973.2856655; lasted 15.26931357383728 seconds.\n",
      "86.15 % success on test data\n",
      "--- (74) ---\n",
      "Training started at 1646083974.0714958\n",
      "Epoch 0: 69.15164184570312: 0.0892\n",
      "Epoch 5000: 49.54656982421875: 0.8038\n",
      "Training finished at 1646083990.427478; lasted 16.355982303619385 seconds.\n",
      "80.38 % success on test data\n",
      "--- (75) ---\n",
      "Training started at 1646083991.2158651\n",
      "Epoch 0: 68.84817504882812: 0.1008\n",
      "Epoch 5000: 52.71144104003906: 0.8093\n",
      "Training finished at 1646084007.2439551; lasted 16.028090000152588 seconds.\n",
      "80.93 % success on test data\n",
      "--- (76) ---\n",
      "Training started at 1646084008.0239158\n",
      "Epoch 0: 69.05257415771484: 0.1009\n",
      "Epoch 5000: 53.377479553222656: 0.6565\n",
      "Training finished at 1646084023.274336; lasted 15.250420331954956 seconds.\n",
      "65.64999999999999 % success on test data\n",
      "--- (77) ---\n",
      "Training started at 1646084024.1090133\n",
      "Epoch 0: 69.17330932617188: 0.1051\n",
      "Epoch 5000: 46.407711029052734: 0.8176\n",
      "Training finished at 1646084039.059356; lasted 14.950342655181885 seconds.\n",
      "81.76 % success on test data\n",
      "--- (78) ---\n",
      "Training started at 1646084039.804527\n",
      "Epoch 0: 69.14637756347656: 0.1272\n",
      "Epoch 5000: 52.209808349609375: 0.7569\n",
      "Training finished at 1646084054.961604; lasted 15.157077074050903 seconds.\n",
      "75.69 % success on test data\n",
      "--- (79) ---\n",
      "Training started at 1646084055.7945695\n",
      "Epoch 0: 69.17603302001953: 0.1637\n",
      "Epoch 5000: 51.882259368896484: 0.7961\n",
      "Training finished at 1646084070.9049215; lasted 15.110352039337158 seconds.\n",
      "79.61 % success on test data\n",
      "--- (80) ---\n",
      "Training started at 1646084071.6518767\n",
      "Epoch 0: 69.11897277832031: 0.1132\n",
      "Epoch 5000: 47.48688888549805: 0.8184\n",
      "Training finished at 1646084086.8716457; lasted 15.21976900100708 seconds.\n",
      "81.84 % success on test data\n",
      "--- (81) ---\n",
      "Training started at 1646084087.649345\n",
      "Epoch 0: 69.06352996826172: 0.0982\n",
      "Epoch 5000: 47.609619140625: 0.8265\n",
      "Training finished at 1646084102.644465; lasted 14.99512004852295 seconds.\n",
      "82.65 % success on test data\n",
      "--- (82) ---\n",
      "Training started at 1646084103.4115374\n",
      "Epoch 0: 68.96318054199219: 0.1135\n",
      "Epoch 5000: 49.55620574951172: 0.8659\n",
      "Training finished at 1646084118.4199972; lasted 15.00845980644226 seconds.\n",
      "86.59 % success on test data\n",
      "--- (83) ---\n",
      "Training started at 1646084119.2138743\n",
      "Epoch 0: 69.17747497558594: 0.1457\n",
      "Epoch 5000: 52.77236557006836: 0.8104\n",
      "Training finished at 1646084134.1610603; lasted 14.94718599319458 seconds.\n",
      "81.04 % success on test data\n",
      "--- (84) ---\n",
      "Training started at 1646084134.9244165\n",
      "Epoch 0: 69.1980209350586: 0.1584\n",
      "Epoch 5000: 50.45988845825195: 0.7671\n",
      "Training finished at 1646084150.028267; lasted 15.103850364685059 seconds.\n",
      "76.71 % success on test data\n",
      "--- (85) ---\n",
      "Training started at 1646084150.8132498\n",
      "Epoch 0: 69.18272399902344: 0.1134\n",
      "Epoch 5000: 53.010318756103516: 0.7473\n",
      "Training finished at 1646084165.7210066; lasted 14.907756805419922 seconds.\n",
      "74.72999999999999 % success on test data\n",
      "--- (86) ---\n",
      "Training started at 1646084166.453531\n",
      "Epoch 0: 69.05083465576172: 0.0892\n",
      "Epoch 5000: 49.65401077270508: 0.7054\n",
      "Training finished at 1646084181.7414956; lasted 15.287964582443237 seconds.\n",
      "70.54 % success on test data\n",
      "--- (87) ---\n",
      "Training started at 1646084182.5138364\n",
      "Epoch 0: 69.17462158203125: 0.0911\n",
      "Epoch 5000: 49.340999603271484: 0.8166\n",
      "Training finished at 1646084197.765163; lasted 15.251326560974121 seconds.\n",
      "81.66 % success on test data\n",
      "--- (88) ---\n",
      "Training started at 1646084198.535098\n",
      "Epoch 0: 69.13121795654297: 0.0983\n",
      "Epoch 5000: 48.3662223815918: 0.8824\n",
      "Training finished at 1646084213.3815837; lasted 14.846485614776611 seconds.\n",
      "88.24 % success on test data\n",
      "--- (89) ---\n",
      "Training started at 1646084214.1555097\n",
      "Epoch 0: 69.03646087646484: 0.1011\n",
      "Epoch 5000: 46.128990173339844: 0.8712\n",
      "Training finished at 1646084229.3826566; lasted 15.227146863937378 seconds.\n",
      "87.12 % success on test data\n",
      "--- (90) ---\n",
      "Training started at 1646084230.1778316\n",
      "Epoch 0: 69.21012878417969: 0.1009\n",
      "Epoch 5000: 51.63433837890625: 0.7222\n",
      "Training finished at 1646084246.4223077; lasted 16.244476079940796 seconds.\n",
      "72.22 % success on test data\n",
      "--- (91) ---\n",
      "Training started at 1646084247.260978\n",
      "Epoch 0: 69.06170654296875: 0.1135\n",
      "Epoch 5000: 47.038421630859375: 0.8699\n",
      "Training finished at 1646084262.8396056; lasted 15.578627586364746 seconds.\n",
      "86.99 % success on test data\n",
      "--- (92) ---\n",
      "Training started at 1646084263.617607\n",
      "Epoch 0: 69.22596740722656: 0.1659\n",
      "Epoch 5000: 49.554439544677734: 0.8733\n",
      "Training finished at 1646084278.796177; lasted 15.178569793701172 seconds.\n",
      "87.33 % success on test data\n",
      "--- (93) ---\n",
      "Training started at 1646084279.56839\n",
      "Epoch 0: 69.06190490722656: 0.0836\n",
      "Epoch 5000: 52.005313873291016: 0.7879\n",
      "Training finished at 1646084294.500744; lasted 14.932354211807251 seconds.\n",
      "78.79 % success on test data\n",
      "--- (94) ---\n",
      "Training started at 1646084295.2836928\n",
      "Epoch 0: 69.16695404052734: 0.101\n",
      "Epoch 5000: 47.16982650756836: 0.8691\n",
      "Training finished at 1646084310.611649; lasted 15.327956199645996 seconds.\n",
      "86.91 % success on test data\n",
      "--- (95) ---\n",
      "Training started at 1646084311.4114387\n",
      "Epoch 0: 69.06458282470703: 0.0868\n",
      "Epoch 5000: 48.31883239746094: 0.8165\n",
      "Training finished at 1646084326.553326; lasted 15.141887187957764 seconds.\n",
      "81.65 % success on test data\n",
      "--- (96) ---\n",
      "Training started at 1646084327.3012707\n",
      "Epoch 0: 69.0811767578125: 0.1596\n",
      "Epoch 5000: 52.384124755859375: 0.7268\n",
      "Training finished at 1646084342.3045294; lasted 15.00325870513916 seconds.\n",
      "72.68 % success on test data\n",
      "--- (97) ---\n",
      "Training started at 1646084343.0918746\n",
      "Epoch 0: 68.91236877441406: 0.0892\n",
      "Epoch 5000: 49.79436111450195: 0.8788\n",
      "Training finished at 1646084358.9750328; lasted 15.883158206939697 seconds.\n",
      "87.88 % success on test data\n",
      "--- (98) ---\n",
      "Training started at 1646084359.8033416\n",
      "Epoch 0: 69.00238037109375: 0.098\n",
      "Epoch 5000: 46.79341125488281: 0.8189\n",
      "Training finished at 1646084375.0751328; lasted 15.271791219711304 seconds.\n",
      "81.89 % success on test data\n",
      "--- (99) ---\n",
      "Training started at 1646084375.8627524\n",
      "Epoch 0: 69.1607894897461: 0.0757\n",
      "Epoch 5000: 46.053321838378906: 0.8615\n",
      "Training finished at 1646084391.1258376; lasted 15.263085126876831 seconds.\n",
      "86.15 % success on test data\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(f'--- ({i}) ---')\n",
    "    model = ModelManager.get_untrained(ModelType.MnistMLPNevil)\n",
    "    Coach.train(\n",
    "        model,\n",
    "        data,\n",
    "        nn.CrossEntropyLoss(reduction='sum'),\n",
    "        optim.Adagrad(model.parameters(), lr=1e-2),\n",
    "        30,\n",
    "        5001,\n",
    "        5000\n",
    "    )\n",
    "    adagrad.append(Coach.measure_performance(model, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- (0) ---\n",
      "Training started at 1646079120.7211258\n",
      "Epoch 0: 69.15100860595703: 0.1745\n",
      "Epoch 5000: 48.606529235839844: 0.8925\n",
      "Training finished at 1646079136.0581527; lasted 15.337026834487915 seconds.\n",
      "89.25 % success on test data\n",
      "--- (1) ---\n",
      "Training started at 1646079136.8270915\n",
      "Epoch 0: 69.11441802978516: 0.0974\n",
      "Epoch 5000: 48.745086669921875: 0.8969\n",
      "Training finished at 1646079152.421378; lasted 15.594286441802979 seconds.\n",
      "89.69 % success on test data\n",
      "--- (2) ---\n",
      "Training started at 1646079153.1881292\n",
      "Epoch 0: 69.14160919189453: 0.1048\n",
      "Epoch 5000: 46.385868072509766: 0.8801\n",
      "Training finished at 1646079168.7891693; lasted 15.601040124893188 seconds.\n",
      "88.01 % success on test data\n",
      "--- (3) ---\n",
      "Training started at 1646079169.5683415\n",
      "Epoch 0: 69.12525177001953: 0.0958\n",
      "Epoch 5000: 48.01694869995117: 0.9036\n",
      "Training finished at 1646079185.1105595; lasted 15.54221796989441 seconds.\n",
      "90.36 % success on test data\n",
      "--- (4) ---\n",
      "Training started at 1646079185.8845398\n",
      "Epoch 0: 68.88540649414062: 0.1009\n",
      "Epoch 5000: 52.12564468383789: 0.8191\n",
      "Training finished at 1646079201.69575; lasted 15.81121015548706 seconds.\n",
      "81.91000000000001 % success on test data\n",
      "--- (5) ---\n",
      "Training started at 1646079202.4840045\n",
      "Epoch 0: 68.95597839355469: 0.1525\n",
      "Epoch 5000: 51.846717834472656: 0.8054\n",
      "Training finished at 1646079218.0980434; lasted 15.614038944244385 seconds.\n",
      "80.54 % success on test data\n",
      "--- (6) ---\n",
      "Training started at 1646079218.8779283\n",
      "Epoch 0: 69.10520935058594: 0.1032\n",
      "Epoch 5000: 50.79444122314453: 0.8307\n",
      "Training finished at 1646079234.0893235; lasted 15.211395263671875 seconds.\n",
      "83.07 % success on test data\n",
      "--- (7) ---\n",
      "Training started at 1646079234.8885777\n",
      "Epoch 0: 69.10725402832031: 0.1009\n",
      "Epoch 5000: 47.96782302856445: 0.8306\n",
      "Training finished at 1646079250.3835778; lasted 15.495000123977661 seconds.\n",
      "83.06 % success on test data\n",
      "--- (8) ---\n",
      "Training started at 1646079251.1694813\n",
      "Epoch 0: 69.09475708007812: 0.1028\n",
      "Epoch 5000: 47.87100601196289: 0.9083\n",
      "Training finished at 1646079266.757181; lasted 15.58769965171814 seconds.\n",
      "90.83 % success on test data\n",
      "--- (9) ---\n",
      "Training started at 1646079267.534822\n",
      "Epoch 0: 69.16954803466797: 0.1009\n",
      "Epoch 5000: 46.7063102722168: 0.9044\n",
      "Training finished at 1646079283.0570886; lasted 15.522266626358032 seconds.\n",
      "90.44 % success on test data\n",
      "--- (10) ---\n",
      "Training started at 1646079283.8260965\n",
      "Epoch 0: 68.99749755859375: 0.098\n",
      "Epoch 5000: 49.764678955078125: 0.8381\n",
      "Training finished at 1646079298.8564613; lasted 15.030364751815796 seconds.\n",
      "83.81 % success on test data\n",
      "--- (11) ---\n",
      "Training started at 1646079299.6073613\n",
      "Epoch 0: 69.18790435791016: 0.1857\n",
      "Epoch 5000: 48.87261199951172: 0.8381\n",
      "Training finished at 1646079314.736674; lasted 15.129312753677368 seconds.\n",
      "83.81 % success on test data\n",
      "--- (12) ---\n",
      "Training started at 1646079315.4907904\n",
      "Epoch 0: 69.1837387084961: 0.1374\n",
      "Epoch 5000: 51.518733978271484: 0.8376\n",
      "Training finished at 1646079330.533949; lasted 15.043158531188965 seconds.\n",
      "83.76 % success on test data\n",
      "--- (13) ---\n",
      "Training started at 1646079331.288927\n",
      "Epoch 0: 68.99081420898438: 0.0892\n",
      "Epoch 5000: 49.90838623046875: 0.8329\n",
      "Training finished at 1646079346.5076547; lasted 15.218727588653564 seconds.\n",
      "83.28999999999999 % success on test data\n",
      "--- (14) ---\n",
      "Training started at 1646079347.2533607\n",
      "Epoch 0: 69.10541534423828: 0.1044\n",
      "Epoch 5000: 49.86224365234375: 0.8216\n",
      "Training finished at 1646079362.3815327; lasted 15.128171920776367 seconds.\n",
      "82.16 % success on test data\n",
      "--- (15) ---\n",
      "Training started at 1646079363.1325622\n",
      "Epoch 0: 69.18749237060547: 0.1009\n",
      "Epoch 5000: 46.889617919921875: 0.8317\n",
      "Training finished at 1646079378.534769; lasted 15.402206897735596 seconds.\n",
      "83.17 % success on test data\n",
      "--- (16) ---\n",
      "Training started at 1646079379.2854085\n",
      "Epoch 0: 69.01080322265625: 0.1051\n",
      "Epoch 5000: 48.49885177612305: 0.8248\n",
      "Training finished at 1646079394.6689649; lasted 15.383556365966797 seconds.\n",
      "82.48 % success on test data\n",
      "--- (17) ---\n",
      "Training started at 1646079395.4800794\n",
      "Epoch 0: 68.92430114746094: 0.1009\n",
      "Epoch 5000: 46.5452880859375: 0.8982\n",
      "Training finished at 1646079410.8542953; lasted 15.374215841293335 seconds.\n",
      "89.82 % success on test data\n",
      "--- (18) ---\n",
      "Training started at 1646079411.6252034\n",
      "Epoch 0: 69.04598999023438: 0.0978\n",
      "Epoch 5000: 45.409671783447266: 0.9122\n",
      "Training finished at 1646079427.0755138; lasted 15.450310468673706 seconds.\n",
      "91.22 % success on test data\n",
      "--- (19) ---\n",
      "Training started at 1646079427.8555856\n",
      "Epoch 0: 69.1953353881836: 0.0777\n",
      "Epoch 5000: 48.108177185058594: 0.8374\n",
      "Training finished at 1646079443.1759079; lasted 15.320322275161743 seconds.\n",
      "83.74000000000001 % success on test data\n",
      "--- (20) ---\n",
      "Training started at 1646079443.9909875\n",
      "Epoch 0: 68.9646224975586: 0.101\n",
      "Epoch 5000: 49.29059600830078: 0.8377\n",
      "Training finished at 1646079459.2983387; lasted 15.307351112365723 seconds.\n",
      "83.77 % success on test data\n",
      "--- (21) ---\n",
      "Training started at 1646079460.0670543\n",
      "Epoch 0: 69.10752868652344: 0.1127\n",
      "Epoch 5000: 49.95454025268555: 0.8162\n",
      "Training finished at 1646079477.062706; lasted 16.995651721954346 seconds.\n",
      "81.62 % success on test data\n",
      "--- (22) ---\n",
      "Training started at 1646079477.8754623\n",
      "Epoch 0: 69.01837921142578: 0.1028\n",
      "Epoch 5000: 45.99296569824219: 0.9064\n",
      "Training finished at 1646079496.6914718; lasted 18.816009521484375 seconds.\n",
      "90.64 % success on test data\n",
      "--- (23) ---\n",
      "Training started at 1646079497.4933946\n",
      "Epoch 0: 69.09852600097656: 0.1032\n",
      "Epoch 5000: 44.87471008300781: 0.9066\n",
      "Training finished at 1646079513.2427168; lasted 15.749322175979614 seconds.\n",
      "90.66 % success on test data\n",
      "--- (24) ---\n",
      "Training started at 1646079514.0086632\n",
      "Epoch 0: 69.06768798828125: 0.101\n",
      "Epoch 5000: 45.217674255371094: 0.9071\n",
      "Training finished at 1646079529.3034716; lasted 15.294808387756348 seconds.\n",
      "90.71000000000001 % success on test data\n",
      "--- (25) ---\n",
      "Training started at 1646079530.0661426\n",
      "Epoch 0: 68.93568420410156: 0.0652\n",
      "Epoch 5000: 47.576744079589844: 0.9047\n",
      "Training finished at 1646079545.5551572; lasted 15.489014625549316 seconds.\n",
      "90.47 % success on test data\n",
      "--- (26) ---\n",
      "Training started at 1646079546.3773015\n",
      "Epoch 0: 69.17521667480469: 0.1031\n",
      "Epoch 5000: 47.30021667480469: 0.9057\n",
      "Training finished at 1646079561.9725008; lasted 15.595199346542358 seconds.\n",
      "90.57 % success on test data\n",
      "--- (27) ---\n",
      "Training started at 1646079562.7434778\n",
      "Epoch 0: 68.9775390625: 0.0892\n",
      "Epoch 5000: 46.5151481628418: 0.9035\n",
      "Training finished at 1646079578.0887268; lasted 15.345248937606812 seconds.\n",
      "90.35 % success on test data\n",
      "--- (28) ---\n",
      "Training started at 1646079578.861738\n",
      "Epoch 0: 69.1249008178711: 0.1045\n",
      "Epoch 5000: 51.417293548583984: 0.8291\n",
      "Training finished at 1646079594.3749535; lasted 15.5132155418396 seconds.\n",
      "82.91 % success on test data\n",
      "--- (29) ---\n",
      "Training started at 1646079595.1288788\n",
      "Epoch 0: 69.0534439086914: 0.1001\n",
      "Epoch 5000: 50.065006256103516: 0.9117\n",
      "Training finished at 1646079610.3562658; lasted 15.227386951446533 seconds.\n",
      "91.17 % success on test data\n",
      "--- (30) ---\n",
      "Training started at 1646079611.1270807\n",
      "Epoch 0: 69.13651275634766: 0.1016\n",
      "Epoch 5000: 47.016536712646484: 0.8363\n",
      "Training finished at 1646079626.4245062; lasted 15.297425508499146 seconds.\n",
      "83.63000000000001 % success on test data\n",
      "--- (31) ---\n",
      "Training started at 1646079627.1743531\n",
      "Epoch 0: 69.16134643554688: 0.0974\n",
      "Epoch 5000: 49.25510025024414: 0.8288\n",
      "Training finished at 1646079642.826837; lasted 15.652483940124512 seconds.\n",
      "82.88 % success on test data\n",
      "--- (32) ---\n",
      "Training started at 1646079643.6178012\n",
      "Epoch 0: 69.13104248046875: 0.1032\n",
      "Epoch 5000: 47.778194427490234: 0.8972\n",
      "Training finished at 1646079658.823062; lasted 15.205260753631592 seconds.\n",
      "89.72 % success on test data\n",
      "--- (33) ---\n",
      "Training started at 1646079659.5922444\n",
      "Epoch 0: 69.04376220703125: 0.101\n",
      "Epoch 5000: 49.936363220214844: 0.8286\n",
      "Training finished at 1646079674.9144883; lasted 15.322243928909302 seconds.\n",
      "82.86 % success on test data\n",
      "--- (34) ---\n",
      "Training started at 1646079675.6943314\n",
      "Epoch 0: 69.15744018554688: 0.1028\n",
      "Epoch 5000: 47.7191162109375: 0.9057\n",
      "Training finished at 1646079691.2005591; lasted 15.506227731704712 seconds.\n",
      "90.57 % success on test data\n",
      "--- (35) ---\n",
      "Training started at 1646079691.9896042\n",
      "Epoch 0: 69.0470199584961: 0.097\n",
      "Epoch 5000: 48.562442779541016: 0.8208\n",
      "Training finished at 1646079707.1718214; lasted 15.182217121124268 seconds.\n",
      "82.08 % success on test data\n",
      "--- (36) ---\n",
      "Training started at 1646079707.926853\n",
      "Epoch 0: 69.06620788574219: 0.1622\n",
      "Epoch 5000: 44.97653579711914: 0.9108\n",
      "Training finished at 1646079723.0194247; lasted 15.09257173538208 seconds.\n",
      "91.08000000000001 % success on test data\n",
      "--- (37) ---\n",
      "Training started at 1646079723.7991593\n",
      "Epoch 0: 69.02120208740234: 0.1322\n",
      "Epoch 5000: 49.29542541503906: 0.837\n",
      "Training finished at 1646079738.9334028; lasted 15.134243488311768 seconds.\n",
      "83.7 % success on test data\n",
      "--- (38) ---\n",
      "Training started at 1646079739.6853907\n",
      "Epoch 0: 69.12796020507812: 0.0989\n",
      "Epoch 5000: 46.845951080322266: 0.8316\n",
      "Training finished at 1646079755.0643158; lasted 15.378925085067749 seconds.\n",
      "83.16 % success on test data\n",
      "--- (39) ---\n",
      "Training started at 1646079755.8366628\n",
      "Epoch 0: 69.03121185302734: 0.1032\n",
      "Epoch 5000: 44.922691345214844: 0.9092\n",
      "Training finished at 1646079771.0799365; lasted 15.243273735046387 seconds.\n",
      "90.92 % success on test data\n",
      "--- (40) ---\n",
      "Training started at 1646079771.8559632\n",
      "Epoch 0: 69.05107116699219: 0.0958\n",
      "Epoch 5000: 49.6301383972168: 0.8303\n",
      "Training finished at 1646079787.2512019; lasted 15.395238637924194 seconds.\n",
      "83.03 % success on test data\n",
      "--- (41) ---\n",
      "Training started at 1646079787.9943306\n",
      "Epoch 0: 68.92594909667969: 0.1667\n",
      "Epoch 5000: 47.04119873046875: 0.8949\n",
      "Training finished at 1646079803.6323812; lasted 15.638050556182861 seconds.\n",
      "89.49000000000001 % success on test data\n",
      "--- (42) ---\n",
      "Training started at 1646079804.4023666\n",
      "Epoch 0: 69.1148681640625: 0.1009\n",
      "Epoch 5000: 47.4350471496582: 0.8308\n",
      "Training finished at 1646079819.9407494; lasted 15.538382768630981 seconds.\n",
      "83.08 % success on test data\n",
      "--- (43) ---\n",
      "Training started at 1646079820.688738\n",
      "Epoch 0: 69.19488525390625: 0.1862\n",
      "Epoch 5000: 46.658447265625: 0.8362\n",
      "Training finished at 1646079836.0010548; lasted 15.312316656112671 seconds.\n",
      "83.62 % success on test data\n",
      "--- (44) ---\n",
      "Training started at 1646079836.7519429\n",
      "Epoch 0: 69.11334991455078: 0.1028\n",
      "Epoch 5000: 55.30821228027344: 0.745\n",
      "Training finished at 1646079852.187931; lasted 15.435988187789917 seconds.\n",
      "74.5 % success on test data\n",
      "--- (45) ---\n",
      "Training started at 1646079852.9538586\n",
      "Epoch 0: 68.96177673339844: 0.0974\n",
      "Epoch 5000: 50.78240966796875: 0.8221\n",
      "Training finished at 1646079868.4568555; lasted 15.502996921539307 seconds.\n",
      "82.21000000000001 % success on test data\n",
      "--- (46) ---\n",
      "Training started at 1646079869.2040243\n",
      "Epoch 0: 69.08817291259766: 0.101\n",
      "Epoch 5000: 49.93825149536133: 0.9096\n",
      "Training finished at 1646079884.4225297; lasted 15.218505382537842 seconds.\n",
      "90.96 % success on test data\n",
      "--- (47) ---\n",
      "Training started at 1646079885.1874912\n",
      "Epoch 0: 69.12811279296875: 0.0892\n",
      "Epoch 5000: 49.45246505737305: 0.821\n",
      "Training finished at 1646079900.472645; lasted 15.285153865814209 seconds.\n",
      "82.1 % success on test data\n",
      "--- (48) ---\n",
      "Training started at 1646079901.272199\n",
      "Epoch 0: 69.16741943359375: 0.0961\n",
      "Epoch 5000: 44.006378173828125: 0.9013\n",
      "Training finished at 1646079916.8096595; lasted 15.537460565567017 seconds.\n",
      "90.13 % success on test data\n",
      "--- (49) ---\n",
      "Training started at 1646079917.580897\n",
      "Epoch 0: 69.14127349853516: 0.0892\n",
      "Epoch 5000: 47.68714904785156: 0.9017\n",
      "Training finished at 1646079933.0033035; lasted 15.422406435012817 seconds.\n",
      "90.16999999999999 % success on test data\n",
      "--- (50) ---\n",
      "Training started at 1646079933.7752526\n",
      "Epoch 0: 69.12733459472656: 0.0982\n",
      "Epoch 5000: 44.868247985839844: 0.9036\n",
      "Training finished at 1646079949.0895977; lasted 15.314345121383667 seconds.\n",
      "90.36 % success on test data\n",
      "--- (51) ---\n",
      "Training started at 1646079949.861493\n",
      "Epoch 0: 68.951904296875: 0.1028\n",
      "Epoch 5000: 49.059661865234375: 0.8307\n",
      "Training finished at 1646079965.2668386; lasted 15.405345439910889 seconds.\n",
      "83.07 % success on test data\n",
      "--- (52) ---\n",
      "Training started at 1646079966.0733676\n",
      "Epoch 0: 68.98768615722656: 0.1031\n",
      "Epoch 5000: 47.385128021240234: 0.9158\n",
      "Training finished at 1646079981.454584; lasted 15.381216287612915 seconds.\n",
      "91.58 % success on test data\n",
      "--- (53) ---\n",
      "Training started at 1646079982.2270215\n",
      "Epoch 0: 69.14607238769531: 0.1817\n",
      "Epoch 5000: 46.565242767333984: 0.847\n",
      "Training finished at 1646079997.434356; lasted 15.207334518432617 seconds.\n",
      "84.7 % success on test data\n",
      "--- (54) ---\n",
      "Training started at 1646079998.2011886\n",
      "Epoch 0: 69.1550521850586: 0.1028\n",
      "Epoch 5000: 49.92161560058594: 0.7589\n",
      "Training finished at 1646080013.715595; lasted 15.514406442642212 seconds.\n",
      "75.89 % success on test data\n",
      "--- (55) ---\n",
      "Training started at 1646080014.5097733\n",
      "Epoch 0: 69.10353088378906: 0.098\n",
      "Epoch 5000: 49.35177993774414: 0.8283\n",
      "Training finished at 1646080029.5798528; lasted 15.070079565048218 seconds.\n",
      "82.83 % success on test data\n",
      "--- (56) ---\n",
      "Training started at 1646080030.3530142\n",
      "Epoch 0: 69.0775146484375: 0.1237\n",
      "Epoch 5000: 47.47821044921875: 0.9086\n",
      "Training finished at 1646080045.8011029; lasted 15.448088645935059 seconds.\n",
      "90.86 % success on test data\n",
      "--- (57) ---\n",
      "Training started at 1646080046.5726998\n",
      "Epoch 0: 68.9683609008789: 0.1552\n",
      "Epoch 5000: 50.21295166015625: 0.8451\n",
      "Training finished at 1646080061.8634064; lasted 15.290706634521484 seconds.\n",
      "84.50999999999999 % success on test data\n",
      "--- (58) ---\n",
      "Training started at 1646080062.6333146\n",
      "Epoch 0: 69.1162109375: 0.0958\n",
      "Epoch 5000: 47.40727233886719: 0.9024\n",
      "Training finished at 1646080078.0666468; lasted 15.433332204818726 seconds.\n",
      "90.24 % success on test data\n",
      "--- (59) ---\n",
      "Training started at 1646080078.8267226\n",
      "Epoch 0: 68.9477310180664: 0.0892\n",
      "Epoch 5000: 46.63431930541992: 0.8381\n",
      "Training finished at 1646080094.138675; lasted 15.311952352523804 seconds.\n",
      "83.81 % success on test data\n",
      "--- (60) ---\n",
      "Training started at 1646080094.895518\n",
      "Epoch 0: 69.23406982421875: 0.1242\n",
      "Epoch 5000: 48.12548065185547: 0.8732\n",
      "Training finished at 1646080110.3732486; lasted 15.477730512619019 seconds.\n",
      "87.32 % success on test data\n",
      "--- (61) ---\n",
      "Training started at 1646080111.1452215\n",
      "Epoch 0: 69.07476043701172: 0.1743\n",
      "Epoch 5000: 51.50127029418945: 0.7446\n",
      "Training finished at 1646080126.2483637; lasted 15.103142261505127 seconds.\n",
      "74.46000000000001 % success on test data\n",
      "--- (62) ---\n",
      "Training started at 1646080127.018468\n",
      "Epoch 0: 69.00511932373047: 0.0892\n",
      "Epoch 5000: 49.882198333740234: 0.7464\n",
      "Training finished at 1646080142.4670453; lasted 15.448577404022217 seconds.\n",
      "74.64 % success on test data\n",
      "--- (63) ---\n",
      "Training started at 1646080143.2202032\n",
      "Epoch 0: 69.19215393066406: 0.1182\n",
      "Epoch 5000: 48.80435562133789: 0.8308\n",
      "Training finished at 1646080158.3616033; lasted 15.14140009880066 seconds.\n",
      "83.08 % success on test data\n",
      "--- (64) ---\n",
      "Training started at 1646080159.1279457\n",
      "Epoch 0: 69.04914855957031: 0.1124\n",
      "Epoch 5000: 49.78703308105469: 0.8135\n",
      "Training finished at 1646080174.678299; lasted 15.550353288650513 seconds.\n",
      "81.35 % success on test data\n",
      "--- (65) ---\n",
      "Training started at 1646080175.4623625\n",
      "Epoch 0: 69.05098724365234: 0.0958\n",
      "Epoch 5000: 47.27186965942383: 0.8364\n",
      "Training finished at 1646080190.8075228; lasted 15.345160245895386 seconds.\n",
      "83.64 % success on test data\n",
      "--- (66) ---\n",
      "Training started at 1646080191.5595899\n",
      "Epoch 0: 69.15243530273438: 0.0982\n",
      "Epoch 5000: 46.35075378417969: 0.9036\n",
      "Training finished at 1646080206.7777824; lasted 15.21819257736206 seconds.\n",
      "90.36 % success on test data\n",
      "--- (67) ---\n",
      "Training started at 1646080207.5578609\n",
      "Epoch 0: 69.19020080566406: 0.1037\n",
      "Epoch 5000: 46.74274444580078: 0.9075\n",
      "Training finished at 1646080222.8038647; lasted 15.246003866195679 seconds.\n",
      "90.75 % success on test data\n",
      "--- (68) ---\n",
      "Training started at 1646080223.5688705\n",
      "Epoch 0: 69.0811767578125: 0.1126\n",
      "Epoch 5000: 47.02634811401367: 0.7788\n",
      "Training finished at 1646080238.742121; lasted 15.173250436782837 seconds.\n",
      "77.88000000000001 % success on test data\n",
      "--- (69) ---\n",
      "Training started at 1646080239.498923\n",
      "Epoch 0: 69.00310516357422: 0.0974\n",
      "Epoch 5000: 47.192901611328125: 0.8914\n",
      "Training finished at 1646080254.9522986; lasted 15.453375577926636 seconds.\n",
      "89.14 % success on test data\n",
      "--- (70) ---\n",
      "Training started at 1646080255.7250001\n",
      "Epoch 0: 69.10189056396484: 0.1034\n",
      "Epoch 5000: 48.76299285888672: 0.8209\n",
      "Training finished at 1646080270.9266114; lasted 15.201611280441284 seconds.\n",
      "82.09 % success on test data\n",
      "--- (71) ---\n",
      "Training started at 1646080271.7346125\n",
      "Epoch 0: 68.99386596679688: 0.101\n",
      "Epoch 5000: 44.954490661621094: 0.8256\n",
      "Training finished at 1646080287.0657206; lasted 15.331108093261719 seconds.\n",
      "82.56 % success on test data\n",
      "--- (72) ---\n",
      "Training started at 1646080287.8660192\n",
      "Epoch 0: 69.05120849609375: 0.0958\n",
      "Epoch 5000: 47.61565017700195: 0.8951\n",
      "Training finished at 1646080303.273612; lasted 15.4075927734375 seconds.\n",
      "89.51 % success on test data\n",
      "--- (73) ---\n",
      "Training started at 1646080304.0469124\n",
      "Epoch 0: 69.02864837646484: 0.1135\n",
      "Epoch 5000: 46.155059814453125: 0.878\n",
      "Training finished at 1646080319.1274667; lasted 15.080554246902466 seconds.\n",
      "87.8 % success on test data\n",
      "--- (74) ---\n",
      "Training started at 1646080319.9033306\n",
      "Epoch 0: 69.09416198730469: 0.1013\n",
      "Epoch 5000: 48.02012252807617: 0.8258\n",
      "Training finished at 1646080335.5159438; lasted 15.612613201141357 seconds.\n",
      "82.58 % success on test data\n",
      "--- (75) ---\n",
      "Training started at 1646080336.2667782\n",
      "Epoch 0: 69.01319122314453: 0.1241\n",
      "Epoch 5000: 45.01016616821289: 0.9019\n",
      "Training finished at 1646080351.4319572; lasted 15.165179014205933 seconds.\n",
      "90.19 % success on test data\n",
      "--- (76) ---\n",
      "Training started at 1646080352.2014492\n",
      "Epoch 0: 68.99420928955078: 0.1056\n",
      "Epoch 5000: 44.55058288574219: 0.8307\n",
      "Training finished at 1646080367.3602705; lasted 15.15882134437561 seconds.\n",
      "83.07 % success on test data\n",
      "--- (77) ---\n",
      "Training started at 1646080368.1113944\n",
      "Epoch 0: 69.11944580078125: 0.098\n",
      "Epoch 5000: 46.94886016845703: 0.9083\n",
      "Training finished at 1646080383.1424868; lasted 15.031092405319214 seconds.\n",
      "90.83 % success on test data\n",
      "--- (78) ---\n",
      "Training started at 1646080383.9064174\n",
      "Epoch 0: 69.03604888916016: 0.098\n",
      "Epoch 5000: 47.90123748779297: 0.8381\n",
      "Training finished at 1646080399.0796137; lasted 15.17319631576538 seconds.\n",
      "83.81 % success on test data\n",
      "--- (79) ---\n",
      "Training started at 1646080399.8729587\n",
      "Epoch 0: 69.18402099609375: 0.0958\n",
      "Epoch 5000: 44.93939208984375: 0.9192\n",
      "Training finished at 1646080415.2829738; lasted 15.410015106201172 seconds.\n",
      "91.92 % success on test data\n",
      "--- (80) ---\n",
      "Training started at 1646080416.045022\n",
      "Epoch 0: 69.1539535522461: 0.0815\n",
      "Epoch 5000: 47.3109130859375: 0.8588\n",
      "Training finished at 1646080431.4829924; lasted 15.437970399856567 seconds.\n",
      "85.88 % success on test data\n",
      "--- (81) ---\n",
      "Training started at 1646080432.2553015\n",
      "Epoch 0: 68.63093566894531: 0.098\n",
      "Epoch 5000: 48.18852615356445: 0.8412\n",
      "Training finished at 1646080447.3245375; lasted 15.069236040115356 seconds.\n",
      "84.11999999999999 % success on test data\n",
      "--- (82) ---\n",
      "Training started at 1646080448.0756934\n",
      "Epoch 0: 69.0831527709961: 0.1009\n",
      "Epoch 5000: 47.68746566772461: 0.8451\n",
      "Training finished at 1646080463.54779; lasted 15.472096681594849 seconds.\n",
      "84.50999999999999 % success on test data\n",
      "--- (83) ---\n",
      "Training started at 1646080464.3229923\n",
      "Epoch 0: 69.11509704589844: 0.1009\n",
      "Epoch 5000: 47.91526412963867: 0.8313\n",
      "Training finished at 1646080479.8433137; lasted 15.520321369171143 seconds.\n",
      "83.13000000000001 % success on test data\n",
      "--- (84) ---\n",
      "Training started at 1646080480.5867763\n",
      "Epoch 0: 69.01506805419922: 0.0976\n",
      "Epoch 5000: 49.04701232910156: 0.9109\n",
      "Training finished at 1646080495.716149; lasted 15.129372835159302 seconds.\n",
      "91.09 % success on test data\n",
      "--- (85) ---\n",
      "Training started at 1646080496.4834726\n",
      "Epoch 0: 68.92664337158203: 0.0974\n",
      "Epoch 5000: 45.284202575683594: 0.8957\n",
      "Training finished at 1646080511.973407; lasted 15.48993444442749 seconds.\n",
      "89.57000000000001 % success on test data\n",
      "--- (86) ---\n",
      "Training started at 1646080512.7416015\n",
      "Epoch 0: 69.28508758544922: 0.0966\n",
      "Epoch 5000: 48.118839263916016: 0.8961\n",
      "Training finished at 1646080528.3150196; lasted 15.573418140411377 seconds.\n",
      "89.61 % success on test data\n",
      "--- (87) ---\n",
      "Training started at 1646080529.0846026\n",
      "Epoch 0: 69.31922912597656: 0.1009\n",
      "Epoch 5000: 49.619842529296875: 0.8188\n",
      "Training finished at 1646080544.732845; lasted 15.648242473602295 seconds.\n",
      "81.88 % success on test data\n",
      "--- (88) ---\n",
      "Training started at 1646080545.5240228\n",
      "Epoch 0: 68.99311828613281: 0.1032\n",
      "Epoch 5000: 46.91475296020508: 0.9223\n",
      "Training finished at 1646080560.8474076; lasted 15.323384761810303 seconds.\n",
      "92.23 % success on test data\n",
      "--- (89) ---\n",
      "Training started at 1646080561.634313\n",
      "Epoch 0: 69.17338562011719: 0.1032\n",
      "Epoch 5000: 47.46019744873047: 0.8955\n",
      "Training finished at 1646080576.805054; lasted 15.170740842819214 seconds.\n",
      "89.55 % success on test data\n",
      "--- (90) ---\n",
      "Training started at 1646080577.564607\n",
      "Epoch 0: 69.09992218017578: 0.1028\n",
      "Epoch 5000: 44.18684005737305: 0.8371\n",
      "Training finished at 1646080593.0446386; lasted 15.480031728744507 seconds.\n",
      "83.71 % success on test data\n",
      "--- (91) ---\n",
      "Training started at 1646080593.818638\n",
      "Epoch 0: 69.08841705322266: 0.098\n",
      "Epoch 5000: 44.06803894042969: 0.9058\n",
      "Training finished at 1646080608.9463017; lasted 15.127663612365723 seconds.\n",
      "90.58 % success on test data\n",
      "--- (92) ---\n",
      "Training started at 1646080609.7177913\n",
      "Epoch 0: 68.97427368164062: 0.0892\n",
      "Epoch 5000: 47.09450149536133: 0.8248\n",
      "Training finished at 1646080624.9991724; lasted 15.281381130218506 seconds.\n",
      "82.48 % success on test data\n",
      "--- (93) ---\n",
      "Training started at 1646080625.7764795\n",
      "Epoch 0: 68.905029296875: 0.1035\n",
      "Epoch 5000: 45.36469268798828: 0.9042\n",
      "Training finished at 1646080641.2362452; lasted 15.459765672683716 seconds.\n",
      "90.42 % success on test data\n",
      "--- (94) ---\n",
      "Training started at 1646080642.0049205\n",
      "Epoch 0: 69.02899932861328: 0.1434\n",
      "Epoch 5000: 46.014976501464844: 0.91\n",
      "Training finished at 1646080657.5943797; lasted 15.58945918083191 seconds.\n",
      "91.0 % success on test data\n",
      "--- (95) ---\n",
      "Training started at 1646080658.384712\n",
      "Epoch 0: 69.02195739746094: 0.1926\n",
      "Epoch 5000: 49.74710464477539: 0.8448\n",
      "Training finished at 1646080673.7716475; lasted 15.386935472488403 seconds.\n",
      "84.48 % success on test data\n",
      "--- (96) ---\n",
      "Training started at 1646080674.551811\n",
      "Epoch 0: 69.02627563476562: 0.1156\n",
      "Epoch 5000: 47.174720764160156: 0.8389\n",
      "Training finished at 1646080689.8741302; lasted 15.322319269180298 seconds.\n",
      "83.89 % success on test data\n",
      "--- (97) ---\n",
      "Training started at 1646080690.6299543\n",
      "Epoch 0: 69.16812133789062: 0.0824\n",
      "Epoch 5000: 47.087955474853516: 0.9083\n",
      "Training finished at 1646080706.068962; lasted 15.439007759094238 seconds.\n",
      "90.83 % success on test data\n",
      "--- (98) ---\n",
      "Training started at 1646080706.8321328\n",
      "Epoch 0: 69.06529235839844: 0.0958\n",
      "Epoch 5000: 46.89120864868164: 0.8298\n",
      "Training finished at 1646080722.234624; lasted 15.402491092681885 seconds.\n",
      "82.98 % success on test data\n",
      "--- (99) ---\n",
      "Training started at 1646080723.0413015\n",
      "Epoch 0: 68.90462493896484: 0.0982\n",
      "Epoch 5000: 50.38014221191406: 0.8438\n",
      "Training finished at 1646080738.5139973; lasted 15.47269582748413 seconds.\n",
      "84.38 % success on test data\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(f'--- ({i}) ---')\n",
    "    model = ModelManager.get_untrained(ModelType.MnistMLPNevil)\n",
    "    Coach.train(\n",
    "        model,\n",
    "        data,\n",
    "        nn.CrossEntropyLoss(reduction='sum'),\n",
    "        optim.RMSprop(model.parameters(), lr=1e-3),\n",
    "        30,\n",
    "        5001,\n",
    "        5000\n",
    "    )\n",
    "    rmsprop.append(Coach.measure_performance(model, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- (0) ---\n",
      "Training started at 1646080739.4161644\n",
      "Epoch 0: 68.89351654052734: 0.0982\n",
      "Epoch 5000: 47.08837127685547: 0.8957\n",
      "Training finished at 1646080755.490407; lasted 16.07424259185791 seconds.\n",
      "89.57000000000001 % success on test data\n",
      "--- (1) ---\n",
      "Training started at 1646080756.259678\n",
      "Epoch 0: 69.12211608886719: 0.098\n",
      "Epoch 5000: 50.613975524902344: 0.7522\n",
      "Training finished at 1646080772.267286; lasted 16.00760817527771 seconds.\n",
      "75.22 % success on test data\n",
      "--- (2) ---\n",
      "Training started at 1646080773.035034\n",
      "Epoch 0: 69.02107238769531: 0.101\n",
      "Epoch 5000: 51.20534133911133: 0.8343\n",
      "Training finished at 1646080789.0347776; lasted 15.999743700027466 seconds.\n",
      "83.43 % success on test data\n",
      "--- (3) ---\n",
      "Training started at 1646080789.784947\n",
      "Epoch 0: 68.96357727050781: 0.0892\n",
      "Epoch 5000: 45.468406677246094: 0.9254\n",
      "Training finished at 1646080805.6207979; lasted 15.835850954055786 seconds.\n",
      "92.54 % success on test data\n",
      "--- (4) ---\n",
      "Training started at 1646080806.403254\n",
      "Epoch 0: 69.0676498413086: 0.0982\n",
      "Epoch 5000: 49.64488220214844: 0.7716\n",
      "Training finished at 1646080822.065503; lasted 15.662248849868774 seconds.\n",
      "77.16 % success on test data\n",
      "--- (5) ---\n",
      "Training started at 1646080822.8197534\n",
      "Epoch 0: 69.00565338134766: 0.098\n",
      "Epoch 5000: 45.998111724853516: 0.905\n",
      "Training finished at 1646080838.3349478; lasted 15.515194416046143 seconds.\n",
      "90.5 % success on test data\n",
      "--- (6) ---\n",
      "Training started at 1646080839.0971184\n",
      "Epoch 0: 69.10260009765625: 0.098\n",
      "Epoch 5000: 45.64999008178711: 0.9081\n",
      "Training finished at 1646080855.1450021; lasted 16.04788374900818 seconds.\n",
      "90.81 % success on test data\n",
      "--- (7) ---\n",
      "Training started at 1646080855.914253\n",
      "Epoch 0: 69.0688247680664: 0.0982\n",
      "Epoch 5000: 50.77029800415039: 0.7475\n",
      "Training finished at 1646080871.6177; lasted 15.703447103500366 seconds.\n",
      "74.75 % success on test data\n",
      "--- (8) ---\n",
      "Training started at 1646080872.3676434\n",
      "Epoch 0: 69.0790023803711: 0.0892\n",
      "Epoch 5000: 49.33943176269531: 0.8863\n",
      "Training finished at 1646080888.2984848; lasted 15.930841445922852 seconds.\n",
      "88.63 % success on test data\n",
      "--- (9) ---\n",
      "Training started at 1646080889.0857294\n",
      "Epoch 0: 69.33094787597656: 0.0892\n",
      "Epoch 5000: 48.78129959106445: 0.8347\n",
      "Training finished at 1646080904.824782; lasted 15.739052534103394 seconds.\n",
      "83.47 % success on test data\n",
      "--- (10) ---\n",
      "Training started at 1646080905.573835\n",
      "Epoch 0: 68.94683837890625: 0.0892\n",
      "Epoch 5000: 47.29513931274414: 0.9166\n",
      "Training finished at 1646080921.4149923; lasted 15.84115743637085 seconds.\n",
      "91.66 % success on test data\n",
      "--- (11) ---\n",
      "Training started at 1646080922.1859708\n",
      "Epoch 0: 69.15985107421875: 0.101\n",
      "Epoch 5000: 45.240997314453125: 0.838\n",
      "Training finished at 1646080937.898408; lasted 15.712437152862549 seconds.\n",
      "83.8 % success on test data\n",
      "--- (12) ---\n",
      "Training started at 1646080938.6513963\n",
      "Epoch 0: 68.8992919921875: 0.1143\n",
      "Epoch 5000: 49.20986557006836: 0.9074\n",
      "Training finished at 1646080954.5327427; lasted 15.881346464157104 seconds.\n",
      "90.74 % success on test data\n",
      "--- (13) ---\n",
      "Training started at 1646080955.296463\n",
      "Epoch 0: 69.08985900878906: 0.0982\n",
      "Epoch 5000: 47.617008209228516: 0.8216\n",
      "Training finished at 1646080971.0967152; lasted 15.800252199172974 seconds.\n",
      "82.16 % success on test data\n",
      "--- (14) ---\n",
      "Training started at 1646080971.8999536\n",
      "Epoch 0: 69.08743286132812: 0.1135\n",
      "Epoch 5000: 44.433509826660156: 0.8891\n",
      "Training finished at 1646080987.6879818; lasted 15.788028240203857 seconds.\n",
      "88.91 % success on test data\n",
      "--- (15) ---\n",
      "Training started at 1646080988.4458184\n",
      "Epoch 0: 69.12193298339844: 0.1011\n",
      "Epoch 5000: 45.678077697753906: 0.9021\n",
      "Training finished at 1646081004.1112401; lasted 15.665421724319458 seconds.\n",
      "90.21000000000001 % success on test data\n",
      "--- (16) ---\n",
      "Training started at 1646081004.8732147\n",
      "Epoch 0: 69.14322662353516: 0.0892\n",
      "Epoch 5000: 48.49217224121094: 0.8989\n",
      "Training finished at 1646081020.760427; lasted 15.88721227645874 seconds.\n",
      "89.89 % success on test data\n",
      "--- (17) ---\n",
      "Training started at 1646081021.5303779\n",
      "Epoch 0: 69.04830932617188: 0.0958\n",
      "Epoch 5000: 47.88275146484375: 0.8503\n",
      "Training finished at 1646081037.343553; lasted 15.813175201416016 seconds.\n",
      "85.03 % success on test data\n",
      "--- (18) ---\n",
      "Training started at 1646081038.1104891\n",
      "Epoch 0: 69.14903259277344: 0.1028\n",
      "Epoch 5000: 52.87454605102539: 0.8445\n",
      "Training finished at 1646081054.0692341; lasted 15.958745002746582 seconds.\n",
      "84.45 % success on test data\n",
      "--- (19) ---\n",
      "Training started at 1646081054.8172796\n",
      "Epoch 0: 69.01057434082031: 0.1228\n",
      "Epoch 5000: 50.81277847290039: 0.7583\n",
      "Training finished at 1646081070.4086797; lasted 15.591400146484375 seconds.\n",
      "75.83 % success on test data\n",
      "--- (20) ---\n",
      "Training started at 1646081071.2115965\n",
      "Epoch 0: 69.16921997070312: 0.0954\n",
      "Epoch 5000: 47.005462646484375: 0.8205\n",
      "Training finished at 1646081087.057415; lasted 15.845818519592285 seconds.\n",
      "82.05 % success on test data\n",
      "--- (21) ---\n",
      "Training started at 1646081087.8534126\n",
      "Epoch 0: 69.10228729248047: 0.0892\n",
      "Epoch 5000: 46.21223831176758: 0.8786\n",
      "Training finished at 1646081103.7047453; lasted 15.851332664489746 seconds.\n",
      "87.86 % success on test data\n",
      "--- (22) ---\n",
      "Training started at 1646081104.4808795\n",
      "Epoch 0: 69.0168685913086: 0.1085\n",
      "Epoch 5000: 47.70295715332031: 0.8925\n",
      "Training finished at 1646081120.323155; lasted 15.842275381088257 seconds.\n",
      "89.25 % success on test data\n",
      "--- (23) ---\n",
      "Training started at 1646081121.094578\n",
      "Epoch 0: 68.98197937011719: 0.0982\n",
      "Epoch 5000: 47.20687484741211: 0.9039\n",
      "Training finished at 1646081136.821398; lasted 15.72681999206543 seconds.\n",
      "90.39 % success on test data\n",
      "--- (24) ---\n",
      "Training started at 1646081137.5868652\n",
      "Epoch 0: 68.98566436767578: 0.0958\n",
      "Epoch 5000: 47.697349548339844: 0.8253\n",
      "Training finished at 1646081153.5001042; lasted 15.913239002227783 seconds.\n",
      "82.53 % success on test data\n",
      "--- (25) ---\n",
      "Training started at 1646081154.2490683\n",
      "Epoch 0: 69.03202819824219: 0.098\n",
      "Epoch 5000: 48.94087219238281: 0.9079\n",
      "Training finished at 1646081169.7693827; lasted 15.520314455032349 seconds.\n",
      "90.79 % success on test data\n",
      "--- (26) ---\n",
      "Training started at 1646081170.5206306\n",
      "Epoch 0: 69.02172088623047: 0.098\n",
      "Epoch 5000: 47.62483215332031: 0.8337\n",
      "Training finished at 1646081186.1636233; lasted 15.642992734909058 seconds.\n",
      "83.37 % success on test data\n",
      "--- (27) ---\n",
      "Training started at 1646081186.9145324\n",
      "Epoch 0: 69.13672637939453: 0.1037\n",
      "Epoch 5000: 45.81813049316406: 0.9099\n",
      "Training finished at 1646081202.568884; lasted 15.654351472854614 seconds.\n",
      "90.99000000000001 % success on test data\n",
      "--- (28) ---\n",
      "Training started at 1646081203.3191345\n",
      "Epoch 0: 68.97113800048828: 0.0982\n",
      "Epoch 5000: 46.29521179199219: 0.9117\n",
      "Training finished at 1646081219.1842194; lasted 15.865084886550903 seconds.\n",
      "91.17 % success on test data\n",
      "--- (29) ---\n",
      "Training started at 1646081219.9760947\n",
      "Epoch 0: 69.08045959472656: 0.1012\n",
      "Epoch 5000: 46.804107666015625: 0.834\n",
      "Training finished at 1646081235.701554; lasted 15.725459337234497 seconds.\n",
      "83.39999999999999 % success on test data\n",
      "--- (30) ---\n",
      "Training started at 1646081236.48838\n",
      "Epoch 0: 69.02449035644531: 0.1249\n",
      "Epoch 5000: 50.62705612182617: 0.8425\n",
      "Training finished at 1646081252.385622; lasted 15.897242069244385 seconds.\n",
      "84.25 % success on test data\n",
      "--- (31) ---\n",
      "Training started at 1646081253.15561\n",
      "Epoch 0: 69.03215789794922: 0.0974\n",
      "Epoch 5000: 47.21306228637695: 0.9084\n",
      "Training finished at 1646081269.3858209; lasted 16.230210781097412 seconds.\n",
      "90.84 % success on test data\n",
      "--- (32) ---\n",
      "Training started at 1646081270.1549175\n",
      "Epoch 0: 69.15238952636719: 0.0982\n",
      "Epoch 5000: 48.879722595214844: 0.7451\n",
      "Training finished at 1646081285.9820592; lasted 15.827141761779785 seconds.\n",
      "74.51 % success on test data\n",
      "--- (33) ---\n",
      "Training started at 1646081286.7070448\n",
      "Epoch 0: 69.08106994628906: 0.0892\n",
      "Epoch 5000: 53.22699737548828: 0.7377\n",
      "Training finished at 1646081302.4903417; lasted 15.783296823501587 seconds.\n",
      "73.77 % success on test data\n",
      "--- (34) ---\n",
      "Training started at 1646081303.3293095\n",
      "Epoch 0: 69.05303192138672: 0.0982\n",
      "Epoch 5000: 51.804141998291016: 0.8163\n",
      "Training finished at 1646081319.0685947; lasted 15.739285230636597 seconds.\n",
      "81.63 % success on test data\n",
      "--- (35) ---\n",
      "Training started at 1646081319.843694\n",
      "Epoch 0: 68.82786560058594: 0.0982\n",
      "Epoch 5000: 45.914154052734375: 0.8315\n",
      "Training finished at 1646081335.6208622; lasted 15.777168273925781 seconds.\n",
      "83.15 % success on test data\n",
      "--- (36) ---\n",
      "Training started at 1646081336.395786\n",
      "Epoch 0: 69.10691833496094: 0.1032\n",
      "Epoch 5000: 47.485572814941406: 0.8388\n",
      "Training finished at 1646081352.10909; lasted 15.713304042816162 seconds.\n",
      "83.88 % success on test data\n",
      "--- (37) ---\n",
      "Training started at 1646081352.8459172\n",
      "Epoch 0: 68.94339752197266: 0.098\n",
      "Epoch 5000: 53.868892669677734: 0.7591\n",
      "Training finished at 1646081368.4803336; lasted 15.634416341781616 seconds.\n",
      "75.91 % success on test data\n",
      "--- (38) ---\n",
      "Training started at 1646081369.2686174\n",
      "Epoch 0: 69.08155822753906: 0.0958\n",
      "Epoch 5000: 51.452064514160156: 0.817\n",
      "Training finished at 1646081385.2108777; lasted 15.942260265350342 seconds.\n",
      "81.69999999999999 % success on test data\n",
      "--- (39) ---\n",
      "Training started at 1646081386.0095453\n",
      "Epoch 0: 69.0560073852539: 0.0962\n",
      "Epoch 5000: 51.69781494140625: 0.754\n",
      "Training finished at 1646081401.7888327; lasted 15.779287338256836 seconds.\n",
      "75.4 % success on test data\n",
      "--- (40) ---\n",
      "Training started at 1646081402.5601244\n",
      "Epoch 0: 69.06982421875: 0.0921\n",
      "Epoch 5000: 46.082942962646484: 0.9054\n",
      "Training finished at 1646081418.4479272; lasted 15.887802839279175 seconds.\n",
      "90.53999999999999 % success on test data\n",
      "--- (41) ---\n",
      "Training started at 1646081419.2414691\n",
      "Epoch 0: 69.01128387451172: 0.1099\n",
      "Epoch 5000: 46.668548583984375: 0.9007\n",
      "Training finished at 1646081435.2742896; lasted 16.032820463180542 seconds.\n",
      "90.07 % success on test data\n",
      "--- (42) ---\n",
      "Training started at 1646081436.0275743\n",
      "Epoch 0: 69.2535629272461: 0.0982\n",
      "Epoch 5000: 49.3316650390625: 0.8317\n",
      "Training finished at 1646081451.7976758; lasted 15.770101547241211 seconds.\n",
      "83.17 % success on test data\n",
      "--- (43) ---\n",
      "Training started at 1646081452.5896995\n",
      "Epoch 0: 69.15556335449219: 0.0958\n",
      "Epoch 5000: 47.78993606567383: 0.8901\n",
      "Training finished at 1646081468.438144; lasted 15.84844446182251 seconds.\n",
      "89.01 % success on test data\n",
      "--- (44) ---\n",
      "Training started at 1646081469.1981344\n",
      "Epoch 0: 69.0396728515625: 0.1032\n",
      "Epoch 5000: 46.40153503417969: 0.9217\n",
      "Training finished at 1646081484.975708; lasted 15.777573585510254 seconds.\n",
      "92.17 % success on test data\n",
      "--- (45) ---\n",
      "Training started at 1646081485.7382596\n",
      "Epoch 0: 69.07481384277344: 0.0958\n",
      "Epoch 5000: 50.90264892578125: 0.8391\n",
      "Training finished at 1646081501.5659702; lasted 15.827710628509521 seconds.\n",
      "83.91 % success on test data\n",
      "--- (46) ---\n",
      "Training started at 1646081502.3192267\n",
      "Epoch 0: 69.12834930419922: 0.1032\n",
      "Epoch 5000: 44.129146575927734: 0.9026\n",
      "Training finished at 1646081517.9715145; lasted 15.652287721633911 seconds.\n",
      "90.25999999999999 % success on test data\n",
      "--- (47) ---\n",
      "Training started at 1646081518.7501752\n",
      "Epoch 0: 69.04618072509766: 0.1032\n",
      "Epoch 5000: 52.06006622314453: 0.8378\n",
      "Training finished at 1646081534.4594784; lasted 15.709303140640259 seconds.\n",
      "83.78 % success on test data\n",
      "--- (48) ---\n",
      "Training started at 1646081535.2104366\n",
      "Epoch 0: 69.16815185546875: 0.0984\n",
      "Epoch 5000: 46.63896179199219: 0.8982\n",
      "Training finished at 1646081551.1917033; lasted 15.981266736984253 seconds.\n",
      "89.82 % success on test data\n",
      "--- (49) ---\n",
      "Training started at 1646081551.9906995\n",
      "Epoch 0: 69.18046569824219: 0.0982\n",
      "Epoch 5000: 47.964744567871094: 0.8256\n",
      "Training finished at 1646081567.7761579; lasted 15.785458326339722 seconds.\n",
      "82.56 % success on test data\n",
      "--- (50) ---\n",
      "Training started at 1646081568.5328956\n",
      "Epoch 0: 69.05782318115234: 0.0916\n",
      "Epoch 5000: 48.335147857666016: 0.837\n",
      "Training finished at 1646081584.4476082; lasted 15.91471266746521 seconds.\n",
      "83.7 % success on test data\n",
      "--- (51) ---\n",
      "Training started at 1646081585.2023463\n",
      "Epoch 0: 69.21861267089844: 0.0974\n",
      "Epoch 5000: 46.43300247192383: 0.9056\n",
      "Training finished at 1646081601.3594148; lasted 16.157068490982056 seconds.\n",
      "90.56 % success on test data\n",
      "--- (52) ---\n",
      "Training started at 1646081602.1174998\n",
      "Epoch 0: 68.91277313232422: 0.1028\n",
      "Epoch 5000: 47.760738372802734: 0.9038\n",
      "Training finished at 1646081618.049952; lasted 15.932452201843262 seconds.\n",
      "90.38000000000001 % success on test data\n",
      "--- (53) ---\n",
      "Training started at 1646081618.8197114\n",
      "Epoch 0: 69.21082305908203: 0.098\n",
      "Epoch 5000: 49.66775894165039: 0.8229\n",
      "Training finished at 1646081634.3389306; lasted 15.519219160079956 seconds.\n",
      "82.28999999999999 % success on test data\n",
      "--- (54) ---\n",
      "Training started at 1646081635.085873\n",
      "Epoch 0: 69.05796813964844: 0.098\n",
      "Epoch 5000: 47.83562469482422: 0.8377\n",
      "Training finished at 1646081650.71854; lasted 15.632667064666748 seconds.\n",
      "83.77 % success on test data\n",
      "--- (55) ---\n",
      "Training started at 1646081651.4870262\n",
      "Epoch 0: 69.16807556152344: 0.1135\n",
      "Epoch 5000: 49.131492614746094: 0.8465\n",
      "Training finished at 1646081667.080433; lasted 15.593406677246094 seconds.\n",
      "84.65 % success on test data\n",
      "--- (56) ---\n",
      "Training started at 1646081667.8415616\n",
      "Epoch 0: 68.90853118896484: 0.0995\n",
      "Epoch 5000: 48.64213180541992: 0.902\n",
      "Training finished at 1646081683.6126962; lasted 15.771134614944458 seconds.\n",
      "90.2 % success on test data\n",
      "--- (57) ---\n",
      "Training started at 1646081684.383631\n",
      "Epoch 0: 69.2151107788086: 0.1028\n",
      "Epoch 5000: 56.421897888183594: 0.7488\n",
      "Training finished at 1646081700.263389; lasted 15.87975811958313 seconds.\n",
      "74.88 % success on test data\n",
      "--- (58) ---\n",
      "Training started at 1646081701.0308838\n",
      "Epoch 0: 69.08692169189453: 0.1009\n",
      "Epoch 5000: 44.97214126586914: 0.9194\n",
      "Training finished at 1646081717.0626402; lasted 16.03175640106201 seconds.\n",
      "91.94 % success on test data\n",
      "--- (59) ---\n",
      "Training started at 1646081717.8375502\n",
      "Epoch 0: 69.04300689697266: 0.0958\n",
      "Epoch 5000: 49.1152458190918: 0.7629\n",
      "Training finished at 1646081733.562067; lasted 15.724516868591309 seconds.\n",
      "76.29 % success on test data\n",
      "--- (60) ---\n",
      "Training started at 1646081734.315667\n",
      "Epoch 0: 68.99126434326172: 0.0995\n",
      "Epoch 5000: 50.60022735595703: 0.8343\n",
      "Training finished at 1646081750.280688; lasted 15.965021133422852 seconds.\n",
      "83.43 % success on test data\n",
      "--- (61) ---\n",
      "Training started at 1646081751.0264983\n",
      "Epoch 0: 69.08416748046875: 0.0941\n",
      "Epoch 5000: 50.50315856933594: 0.8378\n",
      "Training finished at 1646081766.8395424; lasted 15.81304407119751 seconds.\n",
      "83.78 % success on test data\n",
      "--- (62) ---\n",
      "Training started at 1646081767.5929012\n",
      "Epoch 0: 69.12565612792969: 0.098\n",
      "Epoch 5000: 44.90304946899414: 0.8967\n",
      "Training finished at 1646081783.2141604; lasted 15.621259212493896 seconds.\n",
      "89.67 % success on test data\n",
      "--- (63) ---\n",
      "Training started at 1646081784.0102456\n",
      "Epoch 0: 69.12785339355469: 0.0982\n",
      "Epoch 5000: 46.517234802246094: 0.9181\n",
      "Training finished at 1646081799.7554524; lasted 15.745206832885742 seconds.\n",
      "91.81 % success on test data\n",
      "--- (64) ---\n",
      "Training started at 1646081800.5304227\n",
      "Epoch 0: 69.06993865966797: 0.1009\n",
      "Epoch 5000: 47.11498260498047: 0.915\n",
      "Training finished at 1646081816.3037076; lasted 15.773284912109375 seconds.\n",
      "91.5 % success on test data\n",
      "--- (65) ---\n",
      "Training started at 1646081817.0836837\n",
      "Epoch 0: 69.11771392822266: 0.1135\n",
      "Epoch 5000: 45.71976852416992: 0.9031\n",
      "Training finished at 1646081832.6421416; lasted 15.558457851409912 seconds.\n",
      "90.31 % success on test data\n",
      "--- (66) ---\n",
      "Training started at 1646081833.4176638\n",
      "Epoch 0: 69.06509399414062: 0.1135\n",
      "Epoch 5000: 46.42069625854492: 0.9154\n",
      "Training finished at 1646081849.1233284; lasted 15.70566463470459 seconds.\n",
      "91.53999999999999 % success on test data\n",
      "--- (67) ---\n",
      "Training started at 1646081849.8978343\n",
      "Epoch 0: 69.0733871459961: 0.1009\n",
      "Epoch 5000: 45.787113189697266: 0.8998\n",
      "Training finished at 1646081865.9361148; lasted 16.038280487060547 seconds.\n",
      "89.98 % success on test data\n",
      "--- (68) ---\n",
      "Training started at 1646081866.6930103\n",
      "Epoch 0: 68.96625518798828: 0.1032\n",
      "Epoch 5000: 48.32691955566406: 0.9139\n",
      "Training finished at 1646081882.433355; lasted 15.740344762802124 seconds.\n",
      "91.39 % success on test data\n",
      "--- (69) ---\n",
      "Training started at 1646081883.1973343\n",
      "Epoch 0: 69.18965911865234: 0.0982\n",
      "Epoch 5000: 52.684913635253906: 0.763\n",
      "Training finished at 1646081898.8935666; lasted 15.696232318878174 seconds.\n",
      "76.3 % success on test data\n",
      "--- (70) ---\n",
      "Training started at 1646081899.654571\n",
      "Epoch 0: 68.97728729248047: 0.1135\n",
      "Epoch 5000: 49.012672424316406: 0.8282\n",
      "Training finished at 1646081915.2978537; lasted 15.643282651901245 seconds.\n",
      "82.82000000000001 % success on test data\n",
      "--- (71) ---\n",
      "Training started at 1646081916.0498633\n",
      "Epoch 0: 69.40020751953125: 0.1135\n",
      "Epoch 5000: 47.700679779052734: 0.8102\n",
      "Training finished at 1646081931.701124; lasted 15.651260614395142 seconds.\n",
      "81.02000000000001 % success on test data\n",
      "--- (72) ---\n",
      "Training started at 1646081932.446486\n",
      "Epoch 0: 69.2756576538086: 0.1135\n",
      "Epoch 5000: 46.658172607421875: 0.9024\n",
      "Training finished at 1646081948.1788049; lasted 15.732318878173828 seconds.\n",
      "90.24 % success on test data\n",
      "--- (73) ---\n",
      "Training started at 1646081948.94643\n",
      "Epoch 0: 69.25745391845703: 0.0945\n",
      "Epoch 5000: 46.94231414794922: 0.9125\n",
      "Training finished at 1646081965.1376145; lasted 16.191184520721436 seconds.\n",
      "91.25 % success on test data\n",
      "--- (74) ---\n",
      "Training started at 1646081965.912515\n",
      "Epoch 0: 69.17422485351562: 0.0982\n",
      "Epoch 5000: 44.74315643310547: 0.91\n",
      "Training finished at 1646081981.7801323; lasted 15.86761736869812 seconds.\n",
      "91.0 % success on test data\n",
      "--- (75) ---\n",
      "Training started at 1646081982.5647762\n",
      "Epoch 0: 69.1781997680664: 0.1009\n",
      "Epoch 5000: 44.25873565673828: 0.9069\n",
      "Training finished at 1646081998.5680091; lasted 16.003232955932617 seconds.\n",
      "90.69 % success on test data\n",
      "--- (76) ---\n",
      "Training started at 1646081999.3420646\n",
      "Epoch 0: 69.08731079101562: 0.105\n",
      "Epoch 5000: 47.840850830078125: 0.9035\n",
      "Training finished at 1646082015.240321; lasted 15.898256301879883 seconds.\n",
      "90.35 % success on test data\n",
      "--- (77) ---\n",
      "Training started at 1646082016.0035236\n",
      "Epoch 0: 68.9570541381836: 0.0974\n",
      "Epoch 5000: 47.831947326660156: 0.8307\n",
      "Training finished at 1646082032.0025544; lasted 15.999030828475952 seconds.\n",
      "83.07 % success on test data\n",
      "--- (78) ---\n",
      "Training started at 1646082032.7484992\n",
      "Epoch 0: 69.09600067138672: 0.0957\n",
      "Epoch 5000: 46.297393798828125: 0.7436\n",
      "Training finished at 1646082048.70283; lasted 15.954330921173096 seconds.\n",
      "74.36 % success on test data\n",
      "--- (79) ---\n",
      "Training started at 1646082049.4645202\n",
      "Epoch 0: 69.08544158935547: 0.1135\n",
      "Epoch 5000: 52.17488098144531: 0.7602\n",
      "Training finished at 1646082065.158022; lasted 15.693501710891724 seconds.\n",
      "76.02 % success on test data\n",
      "--- (80) ---\n",
      "Training started at 1646082065.9482384\n",
      "Epoch 0: 68.90672302246094: 0.1028\n",
      "Epoch 5000: 45.33482360839844: 0.911\n",
      "Training finished at 1646082081.9362252; lasted 15.98798680305481 seconds.\n",
      "91.10000000000001 % success on test data\n",
      "--- (81) ---\n",
      "Training started at 1646082082.6948965\n",
      "Epoch 0: 69.11051940917969: 0.0964\n",
      "Epoch 5000: 46.11349868774414: 0.9083\n",
      "Training finished at 1646082098.71451; lasted 16.01961350440979 seconds.\n",
      "90.83 % success on test data\n",
      "--- (82) ---\n",
      "Training started at 1646082099.4805357\n",
      "Epoch 0: 69.113037109375: 0.1012\n",
      "Epoch 5000: 52.22057342529297: 0.7525\n",
      "Training finished at 1646082115.2517626; lasted 15.77122688293457 seconds.\n",
      "75.25 % success on test data\n",
      "--- (83) ---\n",
      "Training started at 1646082116.0327508\n",
      "Epoch 0: 69.22732543945312: 0.0958\n",
      "Epoch 5000: 49.78231430053711: 0.8084\n",
      "Training finished at 1646082131.9469893; lasted 15.914238452911377 seconds.\n",
      "80.84 % success on test data\n",
      "--- (84) ---\n",
      "Training started at 1646082132.7095926\n",
      "Epoch 0: 68.98193359375: 0.0958\n",
      "Epoch 5000: 47.562889099121094: 0.9045\n",
      "Training finished at 1646082148.6812413; lasted 15.971648693084717 seconds.\n",
      "90.45 % success on test data\n",
      "--- (85) ---\n",
      "Training started at 1646082149.4596841\n",
      "Epoch 0: 68.8979263305664: 0.1011\n",
      "Epoch 5000: 48.7806510925293: 0.8953\n",
      "Training finished at 1646082165.4694803; lasted 16.009796142578125 seconds.\n",
      "89.53 % success on test data\n",
      "--- (86) ---\n",
      "Training started at 1646082166.2394652\n",
      "Epoch 0: 69.05280303955078: 0.0892\n",
      "Epoch 5000: 51.884830474853516: 0.8355\n",
      "Training finished at 1646082182.0826693; lasted 15.843204021453857 seconds.\n",
      "83.55 % success on test data\n",
      "--- (87) ---\n",
      "Training started at 1646082182.8220844\n",
      "Epoch 0: 69.04588317871094: 0.0982\n",
      "Epoch 5000: 49.805747985839844: 0.8298\n",
      "Training finished at 1646082198.6074324; lasted 15.785347938537598 seconds.\n",
      "82.98 % success on test data\n",
      "--- (88) ---\n",
      "Training started at 1646082199.3510914\n",
      "Epoch 0: 69.05233001708984: 0.0958\n",
      "Epoch 5000: 46.9488639831543: 0.8924\n",
      "Training finished at 1646082215.3233507; lasted 15.972259283065796 seconds.\n",
      "89.24 % success on test data\n",
      "--- (89) ---\n",
      "Training started at 1646082216.090784\n",
      "Epoch 0: 69.20352935791016: 0.0982\n",
      "Epoch 5000: 47.970726013183594: 0.8346\n",
      "Training finished at 1646082231.8101332; lasted 15.719349145889282 seconds.\n",
      "83.46000000000001 % success on test data\n",
      "--- (90) ---\n",
      "Training started at 1646082232.5724263\n",
      "Epoch 0: 69.14676666259766: 0.1184\n",
      "Epoch 5000: 49.5396842956543: 0.8364\n",
      "Training finished at 1646082248.344375; lasted 15.77194857597351 seconds.\n",
      "83.64 % success on test data\n",
      "--- (91) ---\n",
      "Training started at 1646082249.121644\n",
      "Epoch 0: 69.10395812988281: 0.1023\n",
      "Epoch 5000: 46.89298629760742: 0.9148\n",
      "Training finished at 1646082265.128045; lasted 16.00640106201172 seconds.\n",
      "91.47999999999999 % success on test data\n",
      "--- (92) ---\n",
      "Training started at 1646082265.8963172\n",
      "Epoch 0: 69.16371154785156: 0.1009\n",
      "Epoch 5000: 45.77845764160156: 0.9154\n",
      "Training finished at 1646082282.127926; lasted 16.231608867645264 seconds.\n",
      "91.53999999999999 % success on test data\n",
      "--- (93) ---\n",
      "Training started at 1646082282.9139023\n",
      "Epoch 0: 69.0481185913086: 0.0892\n",
      "Epoch 5000: 47.219932556152344: 0.8271\n",
      "Training finished at 1646082298.6568086; lasted 15.742906332015991 seconds.\n",
      "82.71 % success on test data\n",
      "--- (94) ---\n",
      "Training started at 1646082299.3961372\n",
      "Epoch 0: 69.03059387207031: 0.0974\n",
      "Epoch 5000: 48.33821487426758: 0.8165\n",
      "Training finished at 1646082315.4443336; lasted 16.04819631576538 seconds.\n",
      "81.65 % success on test data\n",
      "--- (95) ---\n",
      "Training started at 1646082316.2310424\n",
      "Epoch 0: 69.14754486083984: 0.1032\n",
      "Epoch 5000: 48.68617248535156: 0.8333\n",
      "Training finished at 1646082331.8678882; lasted 15.636845827102661 seconds.\n",
      "83.33 % success on test data\n",
      "--- (96) ---\n",
      "Training started at 1646082332.614745\n",
      "Epoch 0: 69.19419860839844: 0.1002\n",
      "Epoch 5000: 45.47930145263672: 0.8983\n",
      "Training finished at 1646082348.3958092; lasted 15.78106427192688 seconds.\n",
      "89.83 % success on test data\n",
      "--- (97) ---\n",
      "Training started at 1646082349.1637995\n",
      "Epoch 0: 69.15452575683594: 0.1009\n",
      "Epoch 5000: 45.53773498535156: 0.9103\n",
      "Training finished at 1646082364.983066; lasted 15.819266557693481 seconds.\n",
      "91.03 % success on test data\n",
      "--- (98) ---\n",
      "Training started at 1646082365.7450342\n",
      "Epoch 0: 68.93053436279297: 0.101\n",
      "Epoch 5000: 49.79825210571289: 0.8392\n",
      "Training finished at 1646082381.4953792; lasted 15.75034499168396 seconds.\n",
      "83.91999999999999 % success on test data\n",
      "--- (99) ---\n",
      "Training started at 1646082382.2352412\n",
      "Epoch 0: 69.20661926269531: 0.0974\n",
      "Epoch 5000: 46.4942626953125: 0.9118\n",
      "Training finished at 1646082398.468123; lasted 16.232881784439087 seconds.\n",
      "91.18 % success on test data\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(f'--- ({i}) ---')\n",
    "    model = ModelManager.get_untrained(ModelType.MnistMLPNevil)\n",
    "    Coach.train(\n",
    "        model,\n",
    "        data,\n",
    "        nn.CrossEntropyLoss(reduction='sum'),\n",
    "        optim.Adam(model.parameters(), lr=1e-3),\n",
    "        30,\n",
    "        5001,\n",
    "        5000\n",
    "    )\n",
    "    adam.append(Coach.measure_performance(model, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x18c253dd0d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for performences in [sorted(simple), sorted(momentum), sorted(nesterov)]:\n",
    "performences = sorted(simple)\n",
    "distribution = {}\n",
    "for i in range(1, len(performences) + 1):\n",
    "    distribution[performences[i - 1]] = i / len(performences)\n",
    "my_list = distribution.items()\n",
    "sorted(my_list)\n",
    "x, y = zip(*my_list)\n",
    "lab.plot(x, y, color = \"blue\", label=\"SGD\")\n",
    "\n",
    "performences = sorted(adagrad)\n",
    "distribution = {}\n",
    "for i in range(1, len(performences) + 1):\n",
    "    distribution[performences[i - 1]] = i / len(performences)\n",
    "my_list = distribution.items()\n",
    "sorted(my_list)\n",
    "x, y = zip(*my_list)\n",
    "lab.plot(x, y, color = \"red\", label=\"Adagrad\")\n",
    "\n",
    "performences = sorted(rmsprop)\n",
    "distribution = {}\n",
    "for i in range(1, len(performences) + 1):\n",
    "    distribution[performences[i - 1]] = i / len(performences)\n",
    "my_list = distribution.items()\n",
    "sorted(my_list)\n",
    "x, y = zip(*my_list)\n",
    "lab.plot(x, y, color=\"green\", label=\"RMSProp\")\n",
    "\n",
    "performences = sorted(adam)\n",
    "distribution = {}\n",
    "for i in range(1, len(performences) + 1):\n",
    "    distribution[performences[i - 1]] = i / len(performences)\n",
    "my_list = distribution.items()\n",
    "sorted(my_list)\n",
    "x, y = zip(*my_list)\n",
    "lab.plot(x, y, color=\"yellow\", label=\"Adam\")\n",
    "\n",
    "lab.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started at 1646082736.1725428\n",
      "Epoch 0: 69.04042053222656: 0.1026\n",
      "Epoch 5000: 50.19371795654297: 0.8113\n",
      "Training finished at 1646082751.0542734; lasted 14.881730556488037 seconds.\n"
     ]
    }
   ],
   "source": [
    "model = ModelManager.get_untrained(ModelType.MnistMLPNevil)\n",
    "Coach.train(\n",
    "    model,\n",
    "    data,\n",
    "    nn.CrossEntropyLoss(reduction='sum'),\n",
    "    optim.Adagrad(model.parameters(), lr=1e-2),\n",
    "    30,\n",
    "    5001,\n",
    "    5000\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9d03d93fdbeeb361cc45deae4888a34596acc9ca3c93366af240ad46910d2d2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
