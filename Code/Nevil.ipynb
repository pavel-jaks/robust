{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import Coach\n",
    "from utils import MnistData\n",
    "from models import ModelManager, ModelType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MnistData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple, adagrad, rmsprop, adam = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- (0) ---\n",
      "Training started at 1646075967.5054333\n",
      "Epoch 0: 69.15373992919922: 0.1028\n",
      "Epoch 5000: 51.699440002441406: 0.7527\n",
      "Training finished at 1646075983.0046306; lasted 15.499197244644165 seconds.\n",
      "75.27000000000001 % success on test data\n",
      "--- (1) ---\n",
      "Training started at 1646075983.7705455\n",
      "Epoch 0: 69.10469055175781: 0.0982\n",
      "Epoch 5000: 50.3803825378418: 0.8067\n",
      "Training finished at 1646075998.5445123; lasted 14.773966789245605 seconds.\n",
      "80.67 % success on test data\n",
      "--- (2) ---\n",
      "Training started at 1646075999.3016746\n",
      "Epoch 0: 68.90754699707031: 0.0892\n",
      "Epoch 5000: 50.926544189453125: 0.8317\n",
      "Training finished at 1646076014.6737518; lasted 15.372077226638794 seconds.\n",
      "83.17 % success on test data\n",
      "--- (3) ---\n",
      "Training started at 1646076015.4202065\n",
      "Epoch 0: 69.0920639038086: 0.1032\n",
      "Epoch 5000: 48.185089111328125: 0.9009\n",
      "Training finished at 1646076029.9130168; lasted 14.492810249328613 seconds.\n",
      "90.09 % success on test data\n",
      "--- (4) ---\n",
      "Training started at 1646076030.6746583\n",
      "Epoch 0: 68.94232177734375: 0.0892\n",
      "Epoch 5000: 51.827735900878906: 0.7035\n",
      "Training finished at 1646076045.4567766; lasted 14.782118320465088 seconds.\n",
      "70.35 % success on test data\n",
      "--- (5) ---\n",
      "Training started at 1646076046.230938\n",
      "Epoch 0: 69.12140655517578: 0.1032\n",
      "Epoch 5000: 48.773685455322266: 0.8297\n",
      "Training finished at 1646076061.85187; lasted 15.62093210220337 seconds.\n",
      "82.97 % success on test data\n",
      "--- (6) ---\n",
      "Training started at 1646076062.6661725\n",
      "Epoch 0: 68.974609375: 0.0974\n",
      "Epoch 5000: 53.862693786621094: 0.8235\n",
      "Training finished at 1646076077.547464; lasted 14.881291389465332 seconds.\n",
      "82.35 % success on test data\n",
      "--- (7) ---\n",
      "Training started at 1646076078.3399246\n",
      "Epoch 0: 69.06343078613281: 0.0983\n",
      "Epoch 5000: 50.805145263671875: 0.8094\n",
      "Training finished at 1646076093.3327634; lasted 14.992838859558105 seconds.\n",
      "80.94 % success on test data\n",
      "--- (8) ---\n",
      "Training started at 1646076094.111051\n",
      "Epoch 0: 69.1795883178711: 0.1103\n",
      "Epoch 5000: 46.976810455322266: 0.78\n",
      "Training finished at 1646076109.0183578; lasted 14.907306671142578 seconds.\n",
      "78.0 % success on test data\n",
      "--- (9) ---\n",
      "Training started at 1646076109.8112838\n",
      "Epoch 0: 69.22696685791016: 0.1132\n",
      "Epoch 5000: 45.8740234375: 0.8238\n",
      "Training finished at 1646076124.4855173; lasted 14.674233436584473 seconds.\n",
      "82.38 % success on test data\n",
      "--- (10) ---\n",
      "Training started at 1646076125.3181236\n",
      "Epoch 0: 69.12982940673828: 0.1021\n",
      "Epoch 5000: 54.06377410888672: 0.7611\n",
      "Training finished at 1646076139.7955482; lasted 14.477424621582031 seconds.\n",
      "76.11 % success on test data\n",
      "--- (11) ---\n",
      "Training started at 1646076140.5464349\n",
      "Epoch 0: 68.98442840576172: 0.101\n",
      "Epoch 5000: 45.79730987548828: 0.8982\n",
      "Training finished at 1646076154.9182367; lasted 14.371801853179932 seconds.\n",
      "89.82 % success on test data\n",
      "--- (12) ---\n",
      "Training started at 1646076155.7028291\n",
      "Epoch 0: 69.1893081665039: 0.1009\n",
      "Epoch 5000: 48.31568145751953: 0.8199\n",
      "Training finished at 1646076171.6871314; lasted 15.984302282333374 seconds.\n",
      "81.99 % success on test data\n",
      "--- (13) ---\n",
      "Training started at 1646076172.466307\n",
      "Epoch 0: 69.06187438964844: 0.0869\n",
      "Epoch 5000: 52.26738739013672: 0.735\n",
      "Training finished at 1646076188.1657836; lasted 15.699476718902588 seconds.\n",
      "73.5 % success on test data\n",
      "--- (14) ---\n",
      "Training started at 1646076188.9212933\n",
      "Epoch 0: 69.05337524414062: 0.0892\n",
      "Epoch 5000: 49.826072692871094: 0.817\n",
      "Training finished at 1646076203.8474076; lasted 14.926114320755005 seconds.\n",
      "81.69999999999999 % success on test data\n",
      "--- (15) ---\n",
      "Training started at 1646076204.6428196\n",
      "Epoch 0: 69.02318572998047: 0.0892\n",
      "Epoch 5000: 48.245452880859375: 0.8283\n",
      "Training finished at 1646076220.0039923; lasted 15.361172676086426 seconds.\n",
      "82.83 % success on test data\n",
      "--- (16) ---\n",
      "Training started at 1646076220.75431\n",
      "Epoch 0: 69.05287170410156: 0.0982\n",
      "Epoch 5000: 47.83522033691406: 0.8238\n",
      "Training finished at 1646076235.1694932; lasted 14.415183305740356 seconds.\n",
      "82.38 % success on test data\n",
      "--- (17) ---\n",
      "Training started at 1646076235.9656901\n",
      "Epoch 0: 69.04755401611328: 0.1046\n",
      "Epoch 5000: 48.834442138671875: 0.8261\n",
      "Training finished at 1646076250.5464091; lasted 14.580718994140625 seconds.\n",
      "82.61 % success on test data\n",
      "--- (18) ---\n",
      "Training started at 1646076251.3260856\n",
      "Epoch 0: 69.24890899658203: 0.1024\n",
      "Epoch 5000: 45.40913009643555: 0.8778\n",
      "Training finished at 1646076265.8993766; lasted 14.573291063308716 seconds.\n",
      "87.78 % success on test data\n",
      "--- (19) ---\n",
      "Training started at 1646076266.6615481\n",
      "Epoch 0: 69.1960678100586: 0.0892\n",
      "Epoch 5000: 46.05748748779297: 0.9053\n",
      "Training finished at 1646076281.0642009; lasted 14.402652740478516 seconds.\n",
      "90.53 % success on test data\n",
      "--- (20) ---\n",
      "Training started at 1646076281.8281891\n",
      "Epoch 0: 68.9679946899414: 0.1135\n",
      "Epoch 5000: 50.0283317565918: 0.7572\n",
      "Training finished at 1646076295.9985151; lasted 14.170325994491577 seconds.\n",
      "75.72 % success on test data\n",
      "--- (21) ---\n",
      "Training started at 1646076296.7736142\n",
      "Epoch 0: 69.05220031738281: 0.101\n",
      "Epoch 5000: 49.84776306152344: 0.7534\n",
      "Training finished at 1646076311.1271708; lasted 14.353556632995605 seconds.\n",
      "75.33999999999999 % success on test data\n",
      "--- (22) ---\n",
      "Training started at 1646076311.8985252\n",
      "Epoch 0: 69.15771484375: 0.101\n",
      "Epoch 5000: 52.391998291015625: 0.6799\n",
      "Training finished at 1646076326.3463838; lasted 14.447858572006226 seconds.\n",
      "67.99 % success on test data\n",
      "--- (23) ---\n",
      "Training started at 1646076327.1570513\n",
      "Epoch 0: 69.08265686035156: 0.101\n",
      "Epoch 5000: 46.91554641723633: 0.8299\n",
      "Training finished at 1646076341.503667; lasted 14.3466157913208 seconds.\n",
      "82.99 % success on test data\n",
      "--- (24) ---\n",
      "Training started at 1646076342.2645853\n",
      "Epoch 0: 69.09642791748047: 0.1372\n",
      "Epoch 5000: 51.774009704589844: 0.7878\n",
      "Training finished at 1646076356.5539618; lasted 14.289376497268677 seconds.\n",
      "78.78 % success on test data\n",
      "--- (25) ---\n",
      "Training started at 1646076357.2961166\n",
      "Epoch 0: 69.13094329833984: 0.0892\n",
      "Epoch 5000: 50.56671142578125: 0.8469\n",
      "Training finished at 1646076371.7163315; lasted 14.420214891433716 seconds.\n",
      "84.69 % success on test data\n",
      "--- (26) ---\n",
      "Training started at 1646076372.4670615\n",
      "Epoch 0: 69.26368713378906: 0.0952\n",
      "Epoch 5000: 46.93778991699219: 0.8871\n",
      "Training finished at 1646076386.7912047; lasted 14.324143171310425 seconds.\n",
      "88.71 % success on test data\n",
      "--- (27) ---\n",
      "Training started at 1646076387.5529723\n",
      "Epoch 0: 69.00162506103516: 0.1135\n",
      "Epoch 5000: 49.86111068725586: 0.8886\n",
      "Training finished at 1646076401.8159602; lasted 14.262987852096558 seconds.\n",
      "88.86 % success on test data\n",
      "--- (28) ---\n",
      "Training started at 1646076402.6034524\n",
      "Epoch 0: 69.22313690185547: 0.1028\n",
      "Epoch 5000: 45.83531188964844: 0.8655\n",
      "Training finished at 1646076417.119927; lasted 14.516474485397339 seconds.\n",
      "86.55000000000001 % success on test data\n",
      "--- (29) ---\n",
      "Training started at 1646076417.8907094\n",
      "Epoch 0: 69.0755615234375: 0.1009\n",
      "Epoch 5000: 48.8321418762207: 0.7514\n",
      "Training finished at 1646076433.289861; lasted 15.39915156364441 seconds.\n",
      "75.14 % success on test data\n",
      "--- (30) ---\n",
      "Training started at 1646076434.1149936\n",
      "Epoch 0: 68.9828109741211: 0.0799\n",
      "Epoch 5000: 48.838932037353516: 0.7853\n",
      "Training finished at 1646076448.668707; lasted 14.553713321685791 seconds.\n",
      "78.53 % success on test data\n",
      "--- (31) ---\n",
      "Training started at 1646076449.4702308\n",
      "Epoch 0: 69.08097839355469: 0.1036\n",
      "Epoch 5000: 48.82390594482422: 0.8379\n",
      "Training finished at 1646076464.0545; lasted 14.584269285202026 seconds.\n",
      "83.78999999999999 % success on test data\n",
      "--- (32) ---\n",
      "Training started at 1646076464.816469\n",
      "Epoch 0: 68.969970703125: 0.1341\n",
      "Epoch 5000: 47.95842742919922: 0.809\n",
      "Training finished at 1646076479.1667655; lasted 14.35029649734497 seconds.\n",
      "80.9 % success on test data\n",
      "--- (33) ---\n",
      "Training started at 1646076479.939747\n",
      "Epoch 0: 69.02798461914062: 0.1028\n",
      "Epoch 5000: 47.822998046875: 0.7513\n",
      "Training finished at 1646076494.5004528; lasted 14.560705661773682 seconds.\n",
      "75.13 % success on test data\n",
      "--- (34) ---\n",
      "Training started at 1646076495.2552419\n",
      "Epoch 0: 68.8893051147461: 0.0958\n",
      "Epoch 5000: 47.89772033691406: 0.8234\n",
      "Training finished at 1646076509.725469; lasted 14.470227241516113 seconds.\n",
      "82.34 % success on test data\n",
      "--- (35) ---\n",
      "Training started at 1646076510.4934134\n",
      "Epoch 0: 69.05634307861328: 0.135\n",
      "Epoch 5000: 48.28071212768555: 0.8346\n",
      "Training finished at 1646076524.9173424; lasted 14.42392897605896 seconds.\n",
      "83.46000000000001 % success on test data\n",
      "--- (36) ---\n",
      "Training started at 1646076525.716654\n",
      "Epoch 0: 69.22238159179688: 0.101\n",
      "Epoch 5000: 47.80406951904297: 0.9095\n",
      "Training finished at 1646076539.9352233; lasted 14.218569278717041 seconds.\n",
      "90.95 % success on test data\n",
      "--- (37) ---\n",
      "Training started at 1646076540.7310202\n",
      "Epoch 0: 69.10074615478516: 0.1009\n",
      "Epoch 5000: 49.846866607666016: 0.8406\n",
      "Training finished at 1646076555.3625765; lasted 14.631556272506714 seconds.\n",
      "84.06 % success on test data\n",
      "--- (38) ---\n",
      "Training started at 1646076556.1749878\n",
      "Epoch 0: 69.04528045654297: 0.0982\n",
      "Epoch 5000: 45.38331604003906: 0.9082\n",
      "Training finished at 1646076570.5109947; lasted 14.336006879806519 seconds.\n",
      "90.82000000000001 % success on test data\n",
      "--- (39) ---\n",
      "Training started at 1646076571.267422\n",
      "Epoch 0: 68.98401641845703: 0.0958\n",
      "Epoch 5000: 45.83719253540039: 0.907\n",
      "Training finished at 1646076585.7820802; lasted 14.514658212661743 seconds.\n",
      "90.7 % success on test data\n",
      "--- (40) ---\n",
      "Training started at 1646076586.5480304\n",
      "Epoch 0: 69.14967346191406: 0.0974\n",
      "Epoch 5000: 51.779537200927734: 0.767\n",
      "Training finished at 1646076601.072261; lasted 14.524230718612671 seconds.\n",
      "76.7 % success on test data\n",
      "--- (41) ---\n",
      "Training started at 1646076601.8135688\n",
      "Epoch 0: 69.08029174804688: 0.1032\n",
      "Epoch 5000: 50.87656784057617: 0.8272\n",
      "Training finished at 1646076616.0636497; lasted 14.250080823898315 seconds.\n",
      "82.72 % success on test data\n",
      "--- (42) ---\n",
      "Training started at 1646076616.811202\n",
      "Epoch 0: 69.16693115234375: 0.1028\n",
      "Epoch 5000: 48.843994140625: 0.7537\n",
      "Training finished at 1646076631.365986; lasted 14.554784059524536 seconds.\n",
      "75.37 % success on test data\n",
      "--- (43) ---\n",
      "Training started at 1646076632.1461637\n",
      "Epoch 0: 68.92964935302734: 0.1135\n",
      "Epoch 5000: 48.835994720458984: 0.7609\n",
      "Training finished at 1646076646.3177564; lasted 14.171592712402344 seconds.\n",
      "76.09 % success on test data\n",
      "--- (44) ---\n",
      "Training started at 1646076647.142357\n",
      "Epoch 0: 69.00165557861328: 0.0981\n",
      "Epoch 5000: 48.84983444213867: 0.8262\n",
      "Training finished at 1646076661.4841974; lasted 14.341840267181396 seconds.\n",
      "82.62 % success on test data\n",
      "--- (45) ---\n",
      "Training started at 1646076662.220545\n",
      "Epoch 0: 68.83041381835938: 0.0958\n",
      "Epoch 5000: 51.805747985839844: 0.7593\n",
      "Training finished at 1646076676.8345797; lasted 14.614034652709961 seconds.\n",
      "75.92999999999999 % success on test data\n",
      "--- (46) ---\n",
      "Training started at 1646076677.632747\n",
      "Epoch 0: 68.9853744506836: 0.0974\n",
      "Epoch 5000: 43.95489501953125: 0.9025\n",
      "Training finished at 1646076692.1541815; lasted 14.521434545516968 seconds.\n",
      "90.25 % success on test data\n",
      "--- (47) ---\n",
      "Training started at 1646076692.9392383\n",
      "Epoch 0: 69.12409973144531: 0.1138\n",
      "Epoch 5000: 45.79328155517578: 0.9125\n",
      "Training finished at 1646076707.216213; lasted 14.27697467803955 seconds.\n",
      "91.25 % success on test data\n",
      "--- (48) ---\n",
      "Training started at 1646076707.9819477\n",
      "Epoch 0: 69.13164520263672: 0.0958\n",
      "Epoch 5000: 51.803714752197266: 0.7128\n",
      "Training finished at 1646076722.7084162; lasted 14.726468563079834 seconds.\n",
      "71.28 % success on test data\n",
      "--- (49) ---\n",
      "Training started at 1646076723.5056179\n",
      "Epoch 0: 69.01415252685547: 0.0958\n",
      "Epoch 5000: 51.86943054199219: 0.6725\n",
      "Training finished at 1646076737.922202; lasted 14.416584253311157 seconds.\n",
      "67.25 % success on test data\n",
      "--- (50) ---\n",
      "Training started at 1646076738.6912704\n",
      "Epoch 0: 68.92491912841797: 0.1032\n",
      "Epoch 5000: 45.000587463378906: 0.8306\n",
      "Training finished at 1646076752.9617708; lasted 14.270500421524048 seconds.\n",
      "83.06 % success on test data\n",
      "--- (51) ---\n",
      "Training started at 1646076753.7250085\n",
      "Epoch 0: 69.1098403930664: 0.101\n",
      "Epoch 5000: 49.83293914794922: 0.7456\n",
      "Training finished at 1646076768.096963; lasted 14.371954441070557 seconds.\n",
      "74.56 % success on test data\n",
      "--- (52) ---\n",
      "Training started at 1646076768.8343966\n",
      "Epoch 0: 69.16410064697266: 0.1009\n",
      "Epoch 5000: 50.75611114501953: 0.7473\n",
      "Training finished at 1646076783.994544; lasted 15.160147428512573 seconds.\n",
      "74.72999999999999 % success on test data\n",
      "--- (53) ---\n",
      "Training started at 1646076784.739182\n",
      "Epoch 0: 69.01481628417969: 0.1223\n",
      "Epoch 5000: 47.78160858154297: 0.8353\n",
      "Training finished at 1646076799.7393217; lasted 15.000139713287354 seconds.\n",
      "83.53 % success on test data\n",
      "--- (54) ---\n",
      "Training started at 1646076800.485247\n",
      "Epoch 0: 69.12613677978516: 0.1009\n",
      "Epoch 5000: 48.8321418762207: 0.8367\n",
      "Training finished at 1646076815.2240772; lasted 14.738830327987671 seconds.\n",
      "83.67 % success on test data\n",
      "--- (55) ---\n",
      "Training started at 1646076815.9863698\n",
      "Epoch 0: 69.04766082763672: 0.0957\n",
      "Epoch 5000: 49.848167419433594: 0.756\n",
      "Training finished at 1646076830.6184416; lasted 14.632071733474731 seconds.\n",
      "75.6 % success on test data\n",
      "--- (56) ---\n",
      "Training started at 1646076831.413514\n",
      "Epoch 0: 69.17066955566406: 0.0905\n",
      "Epoch 5000: 46.035972595214844: 0.8337\n",
      "Training finished at 1646076845.7348356; lasted 14.321321725845337 seconds.\n",
      "83.37 % success on test data\n",
      "--- (57) ---\n",
      "Training started at 1646076846.4678402\n",
      "Epoch 0: 69.17887878417969: 0.0892\n",
      "Epoch 5000: 51.63280487060547: 0.738\n",
      "Training finished at 1646076860.8590899; lasted 14.391249656677246 seconds.\n",
      "73.8 % success on test data\n",
      "--- (58) ---\n",
      "Training started at 1646076861.612177\n",
      "Epoch 0: 69.04938507080078: 0.0883\n",
      "Epoch 5000: 48.93522262573242: 0.7443\n",
      "Training finished at 1646076876.2906744; lasted 14.678497552871704 seconds.\n",
      "74.42999999999999 % success on test data\n",
      "--- (59) ---\n",
      "Training started at 1646076877.1104405\n",
      "Epoch 0: 69.07249450683594: 0.1032\n",
      "Epoch 5000: 49.41750717163086: 0.8393\n",
      "Training finished at 1646076891.4849966; lasted 14.374556064605713 seconds.\n",
      "83.93 % success on test data\n",
      "--- (60) ---\n",
      "Training started at 1646076892.258719\n",
      "Epoch 0: 69.03327941894531: 0.098\n",
      "Epoch 5000: 53.14324951171875: 0.6759\n",
      "Training finished at 1646076906.5745819; lasted 14.315862894058228 seconds.\n",
      "67.58999999999999 % success on test data\n",
      "--- (61) ---\n",
      "Training started at 1646076907.3140795\n",
      "Epoch 0: 69.14741516113281: 0.1009\n",
      "Epoch 5000: 48.835601806640625: 0.9034\n",
      "Training finished at 1646076922.0191088; lasted 14.705029249191284 seconds.\n",
      "90.34 % success on test data\n",
      "--- (62) ---\n",
      "Training started at 1646076922.8123674\n",
      "Epoch 0: 69.0659408569336: 0.1144\n",
      "Epoch 5000: 49.07426071166992: 0.8204\n",
      "Training finished at 1646076937.0751154; lasted 14.262748003005981 seconds.\n",
      "82.04 % success on test data\n",
      "--- (63) ---\n",
      "Training started at 1646076937.8959203\n",
      "Epoch 0: 69.17536926269531: 0.065\n",
      "Epoch 5000: 51.83403015136719: 0.7527\n",
      "Training finished at 1646076952.4628081; lasted 14.566887855529785 seconds.\n",
      "75.27000000000001 % success on test data\n",
      "--- (64) ---\n",
      "Training started at 1646076953.242983\n",
      "Epoch 0: 69.03556060791016: 0.1028\n",
      "Epoch 5000: 47.81973648071289: 0.7518\n",
      "Training finished at 1646076967.8003185; lasted 14.557335376739502 seconds.\n",
      "75.18 % success on test data\n",
      "--- (65) ---\n",
      "Training started at 1646076968.577623\n",
      "Epoch 0: 69.13102722167969: 0.1031\n",
      "Epoch 5000: 49.79863357543945: 0.823\n",
      "Training finished at 1646076983.0248394; lasted 14.447216510772705 seconds.\n",
      "82.3 % success on test data\n",
      "--- (66) ---\n",
      "Training started at 1646076983.8184376\n",
      "Epoch 0: 69.04947662353516: 0.1201\n",
      "Epoch 5000: 46.79251480102539: 0.8241\n",
      "Training finished at 1646076998.1179817; lasted 14.299544095993042 seconds.\n",
      "82.41000000000001 % success on test data\n",
      "--- (67) ---\n",
      "Training started at 1646076998.886791\n",
      "Epoch 0: 69.0999984741211: 0.098\n",
      "Epoch 5000: 45.37580108642578: 0.8791\n",
      "Training finished at 1646077013.1299007; lasted 14.243109703063965 seconds.\n",
      "87.91 % success on test data\n",
      "--- (68) ---\n",
      "Training started at 1646077013.891059\n",
      "Epoch 0: 69.01564025878906: 0.0999\n",
      "Epoch 5000: 51.76431655883789: 0.7345\n",
      "Training finished at 1646077028.2198179; lasted 14.328758955001831 seconds.\n",
      "73.45 % success on test data\n",
      "--- (69) ---\n",
      "Training started at 1646077029.0005825\n",
      "Epoch 0: 69.25970458984375: 0.101\n",
      "Epoch 5000: 47.3928337097168: 0.8829\n",
      "Training finished at 1646077043.433047; lasted 14.432464599609375 seconds.\n",
      "88.29 % success on test data\n",
      "--- (70) ---\n",
      "Training started at 1646077044.1982071\n",
      "Epoch 0: 69.04507446289062: 0.1047\n",
      "Epoch 5000: 52.69929122924805: 0.7573\n",
      "Training finished at 1646077058.567172; lasted 14.368964910507202 seconds.\n",
      "75.73 % success on test data\n",
      "--- (71) ---\n",
      "Training started at 1646077059.321323\n",
      "Epoch 0: 69.0479736328125: 0.1135\n",
      "Epoch 5000: 48.040252685546875: 0.7207\n",
      "Training finished at 1646077073.7114635; lasted 14.390140533447266 seconds.\n",
      "72.07000000000001 % success on test data\n",
      "--- (72) ---\n",
      "Training started at 1646077074.4962811\n",
      "Epoch 0: 68.92634582519531: 0.0892\n",
      "Epoch 5000: 50.290977478027344: 0.8312\n",
      "Training finished at 1646077089.1058216; lasted 14.609540462493896 seconds.\n",
      "83.12 % success on test data\n",
      "--- (73) ---\n",
      "Training started at 1646077089.85164\n",
      "Epoch 0: 69.0441665649414: 0.098\n",
      "Epoch 5000: 50.110225677490234: 0.7915\n",
      "Training finished at 1646077104.0115802; lasted 14.159940242767334 seconds.\n",
      "79.14999999999999 % success on test data\n",
      "--- (74) ---\n",
      "Training started at 1646077104.7786987\n",
      "Epoch 0: 68.88925170898438: 0.1135\n",
      "Epoch 5000: 46.66321563720703: 0.8313\n",
      "Training finished at 1646077119.0680182; lasted 14.289319515228271 seconds.\n",
      "83.13000000000001 % success on test data\n",
      "--- (75) ---\n",
      "Training started at 1646077119.8331907\n",
      "Epoch 0: 69.07234954833984: 0.0958\n",
      "Epoch 5000: 51.7530632019043: 0.8323\n",
      "Training finished at 1646077134.453551; lasted 14.620360374450684 seconds.\n",
      "83.23 % success on test data\n",
      "--- (76) ---\n",
      "Training started at 1646077135.2223186\n",
      "Epoch 0: 69.06785583496094: 0.1009\n",
      "Epoch 5000: 48.78498077392578: 0.899\n",
      "Training finished at 1646077149.8657362; lasted 14.643417596817017 seconds.\n",
      "89.9 % success on test data\n",
      "--- (77) ---\n",
      "Training started at 1646077150.6366267\n",
      "Epoch 0: 69.02093505859375: 0.101\n",
      "Epoch 5000: 51.83635711669922: 0.7469\n",
      "Training finished at 1646077165.0498092; lasted 14.413182497024536 seconds.\n",
      "74.69 % success on test data\n",
      "--- (78) ---\n",
      "Training started at 1646077165.837982\n",
      "Epoch 0: 69.11090087890625: 0.0974\n",
      "Epoch 5000: 47.806884765625: 0.7534\n",
      "Training finished at 1646077180.5532756; lasted 14.715293645858765 seconds.\n",
      "75.33999999999999 % success on test data\n",
      "--- (79) ---\n",
      "Training started at 1646077181.3362918\n",
      "Epoch 0: 69.17037200927734: 0.098\n",
      "Epoch 5000: 49.853912353515625: 0.82\n",
      "Training finished at 1646077195.5945642; lasted 14.258272409439087 seconds.\n",
      "82.0 % success on test data\n",
      "--- (80) ---\n",
      "Training started at 1646077196.3385763\n",
      "Epoch 0: 69.253662109375: 0.1002\n",
      "Epoch 5000: 58.745880126953125: 0.7454\n",
      "Training finished at 1646077210.7322826; lasted 14.393706321716309 seconds.\n",
      "74.53999999999999 % success on test data\n",
      "--- (81) ---\n",
      "Training started at 1646077211.5359132\n",
      "Epoch 0: 69.12969970703125: 0.0747\n",
      "Epoch 5000: 53.78022766113281: 0.6839\n",
      "Training finished at 1646077225.810343; lasted 14.27442979812622 seconds.\n",
      "68.39 % success on test data\n",
      "--- (82) ---\n",
      "Training started at 1646077226.5835469\n",
      "Epoch 0: 69.08097839355469: 0.1258\n",
      "Epoch 5000: 50.93760299682617: 0.8916\n",
      "Training finished at 1646077240.7585454; lasted 14.17499852180481 seconds.\n",
      "89.16 % success on test data\n",
      "--- (83) ---\n",
      "Training started at 1646077241.525503\n",
      "Epoch 0: 69.03267669677734: 0.1135\n",
      "Epoch 5000: 44.832984924316406: 0.8305\n",
      "Training finished at 1646077255.794058; lasted 14.268555164337158 seconds.\n",
      "83.05 % success on test data\n",
      "--- (84) ---\n",
      "Training started at 1646077256.536767\n",
      "Epoch 0: 69.13729858398438: 0.101\n",
      "Epoch 5000: 46.10636901855469: 0.8192\n",
      "Training finished at 1646077270.9179816; lasted 14.381214618682861 seconds.\n",
      "81.92 % success on test data\n",
      "--- (85) ---\n",
      "Training started at 1646077271.6941562\n",
      "Epoch 0: 69.38103485107422: 0.101\n",
      "Epoch 5000: 49.81573486328125: 0.8186\n",
      "Training finished at 1646077286.139134; lasted 14.444977760314941 seconds.\n",
      "81.86 % success on test data\n",
      "--- (86) ---\n",
      "Training started at 1646077286.8904421\n",
      "Epoch 0: 69.14118194580078: 0.1009\n",
      "Epoch 5000: 47.219566345214844: 0.8397\n",
      "Training finished at 1646077301.5481255; lasted 14.657683372497559 seconds.\n",
      "83.97 % success on test data\n",
      "--- (87) ---\n",
      "Training started at 1646077302.3029637\n",
      "Epoch 0: 69.12175750732422: 0.1135\n",
      "Epoch 5000: 45.813209533691406: 0.8202\n",
      "Training finished at 1646077316.6151245; lasted 14.312160730361938 seconds.\n",
      "82.02000000000001 % success on test data\n",
      "--- (88) ---\n",
      "Training started at 1646077317.3649762\n",
      "Epoch 0: 69.06072235107422: 0.098\n",
      "Epoch 5000: 51.841583251953125: 0.7585\n",
      "Training finished at 1646077331.6977148; lasted 14.332738637924194 seconds.\n",
      "75.85 % success on test data\n",
      "--- (89) ---\n",
      "Training started at 1646077332.4745545\n",
      "Epoch 0: 69.04761505126953: 0.0982\n",
      "Epoch 5000: 48.82050323486328: 0.8119\n",
      "Training finished at 1646077346.925471; lasted 14.450916528701782 seconds.\n",
      "81.19 % success on test data\n",
      "--- (90) ---\n",
      "Training started at 1646077347.6716218\n",
      "Epoch 0: 69.05184936523438: 0.0934\n",
      "Epoch 5000: 50.788082122802734: 0.8314\n",
      "Training finished at 1646077362.2539933; lasted 14.582371473312378 seconds.\n",
      "83.14 % success on test data\n",
      "--- (91) ---\n",
      "Training started at 1646077363.0359955\n",
      "Epoch 0: 69.12166595458984: 0.1009\n",
      "Epoch 5000: 47.82637023925781: 0.8341\n",
      "Training finished at 1646077377.823942; lasted 14.787946462631226 seconds.\n",
      "83.41 % success on test data\n",
      "--- (92) ---\n",
      "Training started at 1646077378.622273\n",
      "Epoch 0: 69.07513427734375: 0.098\n",
      "Epoch 5000: 47.87059783935547: 0.8384\n",
      "Training finished at 1646077392.9009192; lasted 14.278646230697632 seconds.\n",
      "83.84 % success on test data\n",
      "--- (93) ---\n",
      "Training started at 1646077393.6884766\n",
      "Epoch 0: 69.0698013305664: 0.1135\n",
      "Epoch 5000: 50.82095718383789: 0.7475\n",
      "Training finished at 1646077407.9928944; lasted 14.304417848587036 seconds.\n",
      "74.75 % success on test data\n",
      "--- (94) ---\n",
      "Training started at 1646077408.888814\n",
      "Epoch 0: 69.08061218261719: 0.0982\n",
      "Epoch 5000: 46.455780029296875: 0.891\n",
      "Training finished at 1646077423.2643173; lasted 14.375503301620483 seconds.\n",
      "89.1 % success on test data\n",
      "--- (95) ---\n",
      "Training started at 1646077424.0304897\n",
      "Epoch 0: 69.10594940185547: 0.0982\n",
      "Epoch 5000: 51.219017028808594: 0.8056\n",
      "Training finished at 1646077438.7081585; lasted 14.677668809890747 seconds.\n",
      "80.56 % success on test data\n",
      "--- (96) ---\n",
      "Training started at 1646077439.4753172\n",
      "Epoch 0: 69.21792602539062: 0.1135\n",
      "Epoch 5000: 47.530479431152344: 0.8471\n",
      "Training finished at 1646077453.7745445; lasted 14.299227237701416 seconds.\n",
      "84.71 % success on test data\n",
      "--- (97) ---\n",
      "Training started at 1646077454.5375588\n",
      "Epoch 0: 69.10102081298828: 0.098\n",
      "Epoch 5000: 47.83442687988281: 0.831\n",
      "Training finished at 1646077468.8358319; lasted 14.298273086547852 seconds.\n",
      "83.1 % success on test data\n",
      "--- (98) ---\n",
      "Training started at 1646077469.5937622\n",
      "Epoch 0: 68.96118927001953: 0.1009\n",
      "Epoch 5000: 47.33107376098633: 0.9111\n",
      "Training finished at 1646077484.6252465; lasted 15.031484365463257 seconds.\n",
      "91.11 % success on test data\n",
      "--- (99) ---\n",
      "Training started at 1646077485.3984203\n",
      "Epoch 0: 69.07347106933594: 0.101\n",
      "Epoch 5000: 47.79016876220703: 0.838\n",
      "Training finished at 1646077499.8616226; lasted 14.463202238082886 seconds.\n",
      "83.8 % success on test data\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(f'--- ({i}) ---')\n",
    "    model = ModelManager.get_untrained(ModelType.MnistMLPNevil)\n",
    "    Coach.train(\n",
    "        model,\n",
    "        data,\n",
    "        nn.CrossEntropyLoss(reduction='sum'),\n",
    "        optim.SGD(model.parameters(), lr=1e-2),\n",
    "        30,\n",
    "        5001,\n",
    "        5000\n",
    "    )\n",
    "    simple.append(Coach.measure_performance(model, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- (0) ---\n",
      "Training started at 1646077500.7873144\n",
      "Epoch 0: 69.07331848144531: 0.101\n",
      "Epoch 5000: 63.242698669433594: 0.4536\n",
      "Training finished at 1646077515.9308708; lasted 15.143556356430054 seconds.\n",
      "45.36 % success on test data\n",
      "--- (1) ---\n",
      "Training started at 1646077516.7398212\n",
      "Epoch 0: 69.2611083984375: 0.1024\n",
      "Epoch 5000: 59.9005241394043: 0.5929\n",
      "Training finished at 1646077531.74728; lasted 15.007458686828613 seconds.\n",
      "59.29 % success on test data\n",
      "--- (2) ---\n",
      "Training started at 1646077532.5679095\n",
      "Epoch 0: 69.09209442138672: 0.1028\n",
      "Epoch 5000: 61.742347717285156: 0.5281\n",
      "Training finished at 1646077547.8275988; lasted 15.259689331054688 seconds.\n",
      "52.81 % success on test data\n",
      "--- (3) ---\n",
      "Training started at 1646077548.6885474\n",
      "Epoch 0: 69.22459411621094: 0.101\n",
      "Epoch 5000: 63.579891204833984: 0.4356\n",
      "Training finished at 1646077563.4739513; lasted 14.785403966903687 seconds.\n",
      "43.56 % success on test data\n",
      "--- (4) ---\n",
      "Training started at 1646077564.2380326\n",
      "Epoch 0: 69.36742401123047: 0.0817\n",
      "Epoch 5000: 59.020111083984375: 0.4107\n",
      "Training finished at 1646077579.2252097; lasted 14.98717713356018 seconds.\n",
      "41.07 % success on test data\n",
      "--- (5) ---\n",
      "Training started at 1646077580.0261848\n",
      "Epoch 0: 69.07855224609375: 0.1032\n",
      "Epoch 5000: 59.57670974731445: 0.459\n",
      "Training finished at 1646077594.9336016; lasted 14.907416820526123 seconds.\n",
      "45.9 % success on test data\n",
      "--- (6) ---\n",
      "Training started at 1646077595.6499026\n",
      "Epoch 0: 69.02059936523438: 0.1172\n",
      "Epoch 5000: 58.166988372802734: 0.542\n",
      "Training finished at 1646077610.5477858; lasted 14.897883176803589 seconds.\n",
      "54.2 % success on test data\n",
      "--- (7) ---\n",
      "Training started at 1646077611.3176353\n",
      "Epoch 0: 69.14222717285156: 0.0958\n",
      "Epoch 5000: 57.36333465576172: 0.556\n",
      "Training finished at 1646077626.3700945; lasted 15.052459239959717 seconds.\n",
      "55.60000000000001 % success on test data\n",
      "--- (8) ---\n",
      "Training started at 1646077627.174957\n",
      "Epoch 0: 69.01116180419922: 0.1009\n",
      "Epoch 5000: 62.98411178588867: 0.4898\n",
      "Training finished at 1646077642.5196767; lasted 15.344719648361206 seconds.\n",
      "48.980000000000004 % success on test data\n",
      "--- (9) ---\n",
      "Training started at 1646077643.2902899\n",
      "Epoch 0: 69.23414611816406: 0.1027\n",
      "Epoch 5000: 62.09079360961914: 0.517\n",
      "Training finished at 1646077658.305558; lasted 15.015268087387085 seconds.\n",
      "51.7 % success on test data\n",
      "--- (10) ---\n",
      "Training started at 1646077659.0655587\n",
      "Epoch 0: 69.1006851196289: 0.0972\n",
      "Epoch 5000: 59.09897232055664: 0.5971\n",
      "Training finished at 1646077674.6220086; lasted 15.556449890136719 seconds.\n",
      "59.709999999999994 % success on test data\n",
      "--- (11) ---\n",
      "Training started at 1646077675.4857514\n",
      "Epoch 0: 68.99240112304688: 0.1114\n",
      "Epoch 5000: 58.946044921875: 0.3974\n",
      "Training finished at 1646077691.9955375; lasted 16.509786128997803 seconds.\n",
      "39.739999999999995 % success on test data\n",
      "--- (12) ---\n",
      "Training started at 1646077692.7689953\n",
      "Epoch 0: 69.12161254882812: 0.0958\n",
      "Epoch 5000: 62.45393753051758: 0.3948\n",
      "Training finished at 1646077708.7272832; lasted 15.958287954330444 seconds.\n",
      "39.48 % success on test data\n",
      "--- (13) ---\n",
      "Training started at 1646077709.5612326\n",
      "Epoch 0: 68.96858978271484: 0.0982\n",
      "Epoch 5000: 56.64189147949219: 0.5795\n",
      "Training finished at 1646077725.3594704; lasted 15.798237800598145 seconds.\n",
      "57.95 % success on test data\n",
      "--- (14) ---\n",
      "Training started at 1646077726.0997345\n",
      "Epoch 0: 69.07897186279297: 0.1052\n",
      "Epoch 5000: 61.031925201416016: 0.5783\n",
      "Training finished at 1646077742.1306858; lasted 16.030951261520386 seconds.\n",
      "57.830000000000005 % success on test data\n",
      "--- (15) ---\n",
      "Training started at 1646077742.8706846\n",
      "Epoch 0: 68.9825210571289: 0.1114\n",
      "Epoch 5000: 61.15563201904297: 0.5065\n",
      "Training finished at 1646077758.7080004; lasted 15.837315797805786 seconds.\n",
      "50.64999999999999 % success on test data\n",
      "--- (16) ---\n",
      "Training started at 1646077759.4869716\n",
      "Epoch 0: 69.03546905517578: 0.0982\n",
      "Epoch 5000: 59.55046081542969: 0.5913\n",
      "Training finished at 1646077775.3417268; lasted 15.854755163192749 seconds.\n",
      "59.13 % success on test data\n",
      "--- (17) ---\n",
      "Training started at 1646077776.1312413\n",
      "Epoch 0: 69.06398010253906: 0.098\n",
      "Epoch 5000: 60.5047492980957: 0.5033\n",
      "Training finished at 1646077791.7285738; lasted 15.59733247756958 seconds.\n",
      "50.33 % success on test data\n",
      "--- (18) ---\n",
      "Training started at 1646077792.4144914\n",
      "Epoch 0: 69.10258483886719: 0.1009\n",
      "Epoch 5000: 59.62635803222656: 0.494\n",
      "Training finished at 1646077808.4377577; lasted 16.023266315460205 seconds.\n",
      "49.4 % success on test data\n",
      "--- (19) ---\n",
      "Training started at 1646077809.1367166\n",
      "Epoch 0: 69.173828125: 0.098\n",
      "Epoch 5000: 61.954341888427734: 0.3798\n",
      "Training finished at 1646077825.0450504; lasted 15.908333778381348 seconds.\n",
      "37.980000000000004 % success on test data\n",
      "--- (20) ---\n",
      "Training started at 1646077825.8269594\n",
      "Epoch 0: 68.85356140136719: 0.1135\n",
      "Epoch 5000: 64.52354431152344: 0.441\n",
      "Training finished at 1646077841.5875535; lasted 15.760594129562378 seconds.\n",
      "44.1 % success on test data\n",
      "--- (21) ---\n",
      "Training started at 1646077842.4322004\n",
      "Epoch 0: 69.04657745361328: 0.098\n",
      "Epoch 5000: 59.59843444824219: 0.5439\n",
      "Training finished at 1646077858.0880523; lasted 15.6558518409729 seconds.\n",
      "54.39000000000001 % success on test data\n",
      "--- (22) ---\n",
      "Training started at 1646077858.8574822\n",
      "Epoch 0: 69.1222915649414: 0.0946\n",
      "Epoch 5000: 61.13789367675781: 0.5028\n",
      "Training finished at 1646077874.7714221; lasted 15.913939952850342 seconds.\n",
      "50.28 % success on test data\n",
      "--- (23) ---\n",
      "Training started at 1646077875.5233326\n",
      "Epoch 0: 69.11019897460938: 0.1135\n",
      "Epoch 5000: 59.120758056640625: 0.4616\n",
      "Training finished at 1646077891.1604931; lasted 15.637160539627075 seconds.\n",
      "46.160000000000004 % success on test data\n",
      "--- (24) ---\n",
      "Training started at 1646077891.891613\n",
      "Epoch 0: 69.1178207397461: 0.0958\n",
      "Epoch 5000: 62.218833923339844: 0.4157\n",
      "Training finished at 1646077907.8919325; lasted 16.000319480895996 seconds.\n",
      "41.57 % success on test data\n",
      "--- (25) ---\n",
      "Training started at 1646077908.7448573\n",
      "Epoch 0: 68.9858169555664: 0.1028\n",
      "Epoch 5000: 60.55788803100586: 0.4798\n",
      "Training finished at 1646077924.8159986; lasted 16.071141242980957 seconds.\n",
      "47.980000000000004 % success on test data\n",
      "--- (26) ---\n",
      "Training started at 1646077925.621816\n",
      "Epoch 0: 68.96463012695312: 0.0958\n",
      "Epoch 5000: 59.68994140625: 0.509\n",
      "Training finished at 1646077941.5008116; lasted 15.878995656967163 seconds.\n",
      "50.9 % success on test data\n",
      "--- (27) ---\n",
      "Training started at 1646077942.2508278\n",
      "Epoch 0: 69.01844787597656: 0.1009\n",
      "Epoch 5000: 57.868221282958984: 0.5629\n",
      "Training finished at 1646077958.280233; lasted 16.029405117034912 seconds.\n",
      "56.28999999999999 % success on test data\n",
      "--- (28) ---\n",
      "Training started at 1646077958.9945908\n",
      "Epoch 0: 69.17951202392578: 0.103\n",
      "Epoch 5000: 62.53893280029297: 0.4903\n",
      "Training finished at 1646077974.8868883; lasted 15.892297506332397 seconds.\n",
      "49.03 % success on test data\n",
      "--- (29) ---\n",
      "Training started at 1646077975.7190428\n",
      "Epoch 0: 69.13517761230469: 0.0982\n",
      "Epoch 5000: 63.71339416503906: 0.4625\n",
      "Training finished at 1646077991.7614408; lasted 16.04239797592163 seconds.\n",
      "46.25 % success on test data\n",
      "--- (30) ---\n",
      "Training started at 1646077992.4850411\n",
      "Epoch 0: 69.21051788330078: 0.0974\n",
      "Epoch 5000: 60.359317779541016: 0.4007\n",
      "Training finished at 1646078008.464301; lasted 15.979259967803955 seconds.\n",
      "40.07 % success on test data\n",
      "--- (31) ---\n",
      "Training started at 1646078009.221515\n",
      "Epoch 0: 69.20529174804688: 0.1009\n",
      "Epoch 5000: 61.0694465637207: 0.3894\n",
      "Training finished at 1646078025.4468842; lasted 16.225369215011597 seconds.\n",
      "38.940000000000005 % success on test data\n",
      "--- (32) ---\n",
      "Training started at 1646078026.247053\n",
      "Epoch 0: 69.15042877197266: 0.1032\n",
      "Epoch 5000: 62.732666015625: 0.5822\n",
      "Training finished at 1646078042.0468225; lasted 15.799769639968872 seconds.\n",
      "58.220000000000006 % success on test data\n",
      "--- (33) ---\n",
      "Training started at 1646078042.8827643\n",
      "Epoch 0: 68.99785614013672: 0.1209\n",
      "Epoch 5000: 57.95954895019531: 0.4508\n",
      "Training finished at 1646078058.7030618; lasted 15.820297479629517 seconds.\n",
      "45.08 % success on test data\n",
      "--- (34) ---\n",
      "Training started at 1646078059.404555\n",
      "Epoch 0: 69.3384780883789: 0.0892\n",
      "Epoch 5000: 60.73278045654297: 0.4018\n",
      "Training finished at 1646078075.412292; lasted 16.007736921310425 seconds.\n",
      "40.18 % success on test data\n",
      "--- (35) ---\n",
      "Training started at 1646078076.2912598\n",
      "Epoch 0: 69.021484375: 0.1135\n",
      "Epoch 5000: 59.681396484375: 0.5126\n",
      "Training finished at 1646078091.9506972; lasted 15.659437417984009 seconds.\n",
      "51.25999999999999 % success on test data\n",
      "--- (36) ---\n",
      "Training started at 1646078092.6204946\n",
      "Epoch 0: 69.19698333740234: 0.0892\n",
      "Epoch 5000: 58.138023376464844: 0.5573\n",
      "Training finished at 1646078108.4437351; lasted 15.823240518569946 seconds.\n",
      "55.730000000000004 % success on test data\n",
      "--- (37) ---\n",
      "Training started at 1646078109.1439462\n",
      "Epoch 0: 69.03760528564453: 0.1095\n",
      "Epoch 5000: 59.06842803955078: 0.5889\n",
      "Training finished at 1646078124.96804; lasted 15.82409381866455 seconds.\n",
      "58.89 % success on test data\n",
      "--- (38) ---\n",
      "Training started at 1646078125.6725287\n",
      "Epoch 0: 68.9350357055664: 0.0982\n",
      "Epoch 5000: 60.007022857666016: 0.4323\n",
      "Training finished at 1646078141.5612683; lasted 15.888739585876465 seconds.\n",
      "43.230000000000004 % success on test data\n",
      "--- (39) ---\n",
      "Training started at 1646078142.3582711\n",
      "Epoch 0: 69.09709167480469: 0.101\n",
      "Epoch 5000: 58.19870376586914: 0.4725\n",
      "Training finished at 1646078158.1275501; lasted 15.76927900314331 seconds.\n",
      "47.25 % success on test data\n",
      "--- (40) ---\n",
      "Training started at 1646078158.8654184\n",
      "Epoch 0: 69.1197509765625: 0.1398\n",
      "Epoch 5000: 61.22887420654297: 0.5798\n",
      "Training finished at 1646078174.7118292; lasted 15.846410751342773 seconds.\n",
      "57.98 % success on test data\n",
      "--- (41) ---\n",
      "Training started at 1646078175.4736972\n",
      "Epoch 0: 68.86195373535156: 0.0982\n",
      "Epoch 5000: 58.58411407470703: 0.5123\n",
      "Training finished at 1646078191.3360379; lasted 15.862340688705444 seconds.\n",
      "51.23 % success on test data\n",
      "--- (42) ---\n",
      "Training started at 1646078192.0874963\n",
      "Epoch 0: 68.99742126464844: 0.0958\n",
      "Epoch 5000: 64.150634765625: 0.6099\n",
      "Training finished at 1646078207.9582696; lasted 15.870773315429688 seconds.\n",
      "60.99 % success on test data\n",
      "--- (43) ---\n",
      "Training started at 1646078208.685263\n",
      "Epoch 0: 69.2186279296875: 0.1009\n",
      "Epoch 5000: 57.81147384643555: 0.4633\n",
      "Training finished at 1646078224.7827117; lasted 16.09744882583618 seconds.\n",
      "46.33 % success on test data\n",
      "--- (44) ---\n",
      "Training started at 1646078225.5764468\n",
      "Epoch 0: 69.06426239013672: 0.1135\n",
      "Epoch 5000: 56.64986801147461: 0.6192\n",
      "Training finished at 1646078241.6021767; lasted 16.02572989463806 seconds.\n",
      "61.919999999999995 % success on test data\n",
      "--- (45) ---\n",
      "Training started at 1646078242.3613818\n",
      "Epoch 0: 69.04669952392578: 0.1033\n",
      "Epoch 5000: 61.01251220703125: 0.3816\n",
      "Training finished at 1646078258.142671; lasted 15.781289339065552 seconds.\n",
      "38.16 % success on test data\n",
      "--- (46) ---\n",
      "Training started at 1646078258.8856137\n",
      "Epoch 0: 69.26953887939453: 0.0977\n",
      "Epoch 5000: 58.48116683959961: 0.4929\n",
      "Training finished at 1646078274.5406551; lasted 15.655041456222534 seconds.\n",
      "49.29 % success on test data\n",
      "--- (47) ---\n",
      "Training started at 1646078275.3448327\n",
      "Epoch 0: 69.05402374267578: 0.101\n",
      "Epoch 5000: 61.4451904296875: 0.465\n",
      "Training finished at 1646078291.1291866; lasted 15.784353971481323 seconds.\n",
      "46.5 % success on test data\n",
      "--- (48) ---\n",
      "Training started at 1646078291.8711245\n",
      "Epoch 0: 69.04389953613281: 0.1136\n",
      "Epoch 5000: 60.46489715576172: 0.4867\n",
      "Training finished at 1646078307.59938; lasted 15.7282555103302 seconds.\n",
      "48.67 % success on test data\n",
      "--- (49) ---\n",
      "Training started at 1646078308.3944292\n",
      "Epoch 0: 69.0942611694336: 0.0974\n",
      "Epoch 5000: 58.01640319824219: 0.6513\n",
      "Training finished at 1646078324.454922; lasted 16.060492753982544 seconds.\n",
      "65.13 % success on test data\n",
      "--- (50) ---\n",
      "Training started at 1646078325.2378283\n",
      "Epoch 0: 68.83821868896484: 0.1009\n",
      "Epoch 5000: 59.62523651123047: 0.3979\n",
      "Training finished at 1646078341.402443; lasted 16.1646146774292 seconds.\n",
      "39.79 % success on test data\n",
      "--- (51) ---\n",
      "Training started at 1646078342.2186122\n",
      "Epoch 0: 69.08824920654297: 0.1009\n",
      "Epoch 5000: 61.66503143310547: 0.3709\n",
      "Training finished at 1646078358.1953146; lasted 15.976702451705933 seconds.\n",
      "37.09 % success on test data\n",
      "--- (52) ---\n",
      "Training started at 1646078358.8984058\n",
      "Epoch 0: 69.04364013671875: 0.1028\n",
      "Epoch 5000: 60.67390060424805: 0.5881\n",
      "Training finished at 1646078374.7023773; lasted 15.803971529006958 seconds.\n",
      "58.809999999999995 % success on test data\n",
      "--- (53) ---\n",
      "Training started at 1646078375.450373\n",
      "Epoch 0: 69.16203308105469: 0.1028\n",
      "Epoch 5000: 60.16040802001953: 0.4536\n",
      "Training finished at 1646078391.350928; lasted 15.90055513381958 seconds.\n",
      "45.36 % success on test data\n",
      "--- (54) ---\n",
      "Training started at 1646078392.1704986\n",
      "Epoch 0: 69.00840759277344: 0.0958\n",
      "Epoch 5000: 61.43906784057617: 0.472\n",
      "Training finished at 1646078407.82486; lasted 15.654361486434937 seconds.\n",
      "47.199999999999996 % success on test data\n",
      "--- (55) ---\n",
      "Training started at 1646078408.5708117\n",
      "Epoch 0: 68.85753631591797: 0.0892\n",
      "Epoch 5000: 62.24296951293945: 0.4999\n",
      "Training finished at 1646078424.2101393; lasted 15.63932752609253 seconds.\n",
      "49.99 % success on test data\n",
      "--- (56) ---\n",
      "Training started at 1646078425.0171075\n",
      "Epoch 0: 69.01802062988281: 0.1192\n",
      "Epoch 5000: 62.976959228515625: 0.4544\n",
      "Training finished at 1646078440.44473; lasted 15.427622556686401 seconds.\n",
      "45.440000000000005 % success on test data\n",
      "--- (57) ---\n",
      "Training started at 1646078441.1853383\n",
      "Epoch 0: 69.1565933227539: 0.0982\n",
      "Epoch 5000: 58.910770416259766: 0.611\n",
      "Training finished at 1646078456.5045662; lasted 15.319227933883667 seconds.\n",
      "61.1 % success on test data\n",
      "--- (58) ---\n",
      "Training started at 1646078457.2940857\n",
      "Epoch 0: 69.03606414794922: 0.098\n",
      "Epoch 5000: 54.557865142822266: 0.5856\n",
      "Training finished at 1646078472.3269172; lasted 15.03283143043518 seconds.\n",
      "58.56 % success on test data\n",
      "--- (59) ---\n",
      "Training started at 1646078473.0328834\n",
      "Epoch 0: 69.06932830810547: 0.098\n",
      "Epoch 5000: 59.99401092529297: 0.5821\n",
      "Training finished at 1646078487.9927387; lasted 14.959855318069458 seconds.\n",
      "58.209999999999994 % success on test data\n",
      "--- (60) ---\n",
      "Training started at 1646078488.7092187\n",
      "Epoch 0: 69.21397399902344: 0.1635\n",
      "Epoch 5000: 60.01594543457031: 0.5497\n",
      "Training finished at 1646078503.9844947; lasted 15.275275945663452 seconds.\n",
      "54.97 % success on test data\n",
      "--- (61) ---\n",
      "Training started at 1646078504.716458\n",
      "Epoch 0: 69.302734375: 0.1019\n",
      "Epoch 5000: 60.67289733886719: 0.4393\n",
      "Training finished at 1646078519.7699637; lasted 15.053505659103394 seconds.\n",
      "43.93 % success on test data\n",
      "--- (62) ---\n",
      "Training started at 1646078520.4745598\n",
      "Epoch 0: 69.20355224609375: 0.0983\n",
      "Epoch 5000: 58.44148635864258: 0.4539\n",
      "Training finished at 1646078535.6840599; lasted 15.209500074386597 seconds.\n",
      "45.39 % success on test data\n",
      "--- (63) ---\n",
      "Training started at 1646078536.4321945\n",
      "Epoch 0: 69.16394805908203: 0.1009\n",
      "Epoch 5000: 57.1053581237793: 0.6499\n",
      "Training finished at 1646078551.8783772; lasted 15.44618272781372 seconds.\n",
      "64.99000000000001 % success on test data\n",
      "--- (64) ---\n",
      "Training started at 1646078552.6540554\n",
      "Epoch 0: 68.90565490722656: 0.1032\n",
      "Epoch 5000: 56.19874572753906: 0.4918\n",
      "Training finished at 1646078567.6894205; lasted 15.035365104675293 seconds.\n",
      "49.18 % success on test data\n",
      "--- (65) ---\n",
      "Training started at 1646078568.5286846\n",
      "Epoch 0: 68.99927520751953: 0.1028\n",
      "Epoch 5000: 59.55803680419922: 0.5909\n",
      "Training finished at 1646078583.8158739; lasted 15.287189245223999 seconds.\n",
      "59.089999999999996 % success on test data\n",
      "--- (66) ---\n",
      "Training started at 1646078584.573632\n",
      "Epoch 0: 69.14034271240234: 0.0991\n",
      "Epoch 5000: 59.2645149230957: 0.4584\n",
      "Training finished at 1646078599.596165; lasted 15.022532939910889 seconds.\n",
      "45.839999999999996 % success on test data\n",
      "--- (67) ---\n",
      "Training started at 1646078600.396355\n",
      "Epoch 0: 69.06571197509766: 0.0982\n",
      "Epoch 5000: 58.399837493896484: 0.5123\n",
      "Training finished at 1646078615.2838564; lasted 14.88750147819519 seconds.\n",
      "51.23 % success on test data\n",
      "--- (68) ---\n",
      "Training started at 1646078615.983208\n",
      "Epoch 0: 69.04747772216797: 0.0982\n",
      "Epoch 5000: 62.822120666503906: 0.519\n",
      "Training finished at 1646078630.949635; lasted 14.966427087783813 seconds.\n",
      "51.9 % success on test data\n",
      "--- (69) ---\n",
      "Training started at 1646078631.655252\n",
      "Epoch 0: 69.17127227783203: 0.0819\n",
      "Epoch 5000: 54.682220458984375: 0.4686\n",
      "Training finished at 1646078646.3870883; lasted 14.731836318969727 seconds.\n",
      "46.86 % success on test data\n",
      "--- (70) ---\n",
      "Training started at 1646078647.0826907\n",
      "Epoch 0: 68.89432525634766: 0.1032\n",
      "Epoch 5000: 63.904258728027344: 0.4611\n",
      "Training finished at 1646078661.905983; lasted 14.823292255401611 seconds.\n",
      "46.11 % success on test data\n",
      "--- (71) ---\n",
      "Training started at 1646078662.6232827\n",
      "Epoch 0: 68.89705657958984: 0.0974\n",
      "Epoch 5000: 66.8550033569336: 0.4275\n",
      "Training finished at 1646078677.832284; lasted 15.209001302719116 seconds.\n",
      "42.75 % success on test data\n",
      "--- (72) ---\n",
      "Training started at 1646078678.6292694\n",
      "Epoch 0: 69.16920471191406: 0.0751\n",
      "Epoch 5000: 57.14997482299805: 0.5854\n",
      "Training finished at 1646078693.7286932; lasted 15.099423885345459 seconds.\n",
      "58.540000000000006 % success on test data\n",
      "--- (73) ---\n",
      "Training started at 1646078694.4499955\n",
      "Epoch 0: 69.10655975341797: 0.1042\n",
      "Epoch 5000: 63.20534133911133: 0.3891\n",
      "Training finished at 1646078709.2298763; lasted 14.77988076210022 seconds.\n",
      "38.91 % success on test data\n",
      "--- (74) ---\n",
      "Training started at 1646078709.8580654\n",
      "Epoch 0: 69.05927276611328: 0.1032\n",
      "Epoch 5000: 63.89433670043945: 0.4543\n",
      "Training finished at 1646078724.9743483; lasted 15.116282939910889 seconds.\n",
      "45.43 % success on test data\n",
      "--- (75) ---\n",
      "Training started at 1646078725.7490673\n",
      "Epoch 0: 69.06765747070312: 0.0982\n",
      "Epoch 5000: 63.02064895629883: 0.5212\n",
      "Training finished at 1646078740.7374673; lasted 14.988399982452393 seconds.\n",
      "52.12 % success on test data\n",
      "--- (76) ---\n",
      "Training started at 1646078741.4483235\n",
      "Epoch 0: 69.02716827392578: 0.1175\n",
      "Epoch 5000: 64.43988037109375: 0.4487\n",
      "Training finished at 1646078756.5976756; lasted 15.149352073669434 seconds.\n",
      "44.87 % success on test data\n",
      "--- (77) ---\n",
      "Training started at 1646078757.315896\n",
      "Epoch 0: 69.0018081665039: 0.1028\n",
      "Epoch 5000: 63.423091888427734: 0.4101\n",
      "Training finished at 1646078772.516135; lasted 15.200238943099976 seconds.\n",
      "41.010000000000005 % success on test data\n",
      "--- (78) ---\n",
      "Training started at 1646078773.368817\n",
      "Epoch 0: 69.09226989746094: 0.1028\n",
      "Epoch 5000: 61.83348083496094: 0.4897\n",
      "Training finished at 1646078788.68624; lasted 15.317422866821289 seconds.\n",
      "48.97 % success on test data\n",
      "--- (79) ---\n",
      "Training started at 1646078789.471192\n",
      "Epoch 0: 68.97366333007812: 0.1069\n",
      "Epoch 5000: 62.05742645263672: 0.4649\n",
      "Training finished at 1646078804.6308966; lasted 15.159704685211182 seconds.\n",
      "46.489999999999995 % success on test data\n",
      "--- (80) ---\n",
      "Training started at 1646078805.4154596\n",
      "Epoch 0: 69.13825225830078: 0.0958\n",
      "Epoch 5000: 62.40850067138672: 0.381\n",
      "Training finished at 1646078820.4217274; lasted 15.006267786026001 seconds.\n",
      "38.1 % success on test data\n",
      "--- (81) ---\n",
      "Training started at 1646078821.1379142\n",
      "Epoch 0: 69.13175964355469: 0.1169\n",
      "Epoch 5000: 58.96177291870117: 0.5649\n",
      "Training finished at 1646078836.2737417; lasted 15.135827541351318 seconds.\n",
      "56.489999999999995 % success on test data\n",
      "--- (82) ---\n",
      "Training started at 1646078837.0240254\n",
      "Epoch 0: 69.0587387084961: 0.1309\n",
      "Epoch 5000: 58.71782684326172: 0.5603\n",
      "Training finished at 1646078852.042383; lasted 15.018357515335083 seconds.\n",
      "56.03 % success on test data\n",
      "--- (83) ---\n",
      "Training started at 1646078852.7261643\n",
      "Epoch 0: 68.91142272949219: 0.1359\n",
      "Epoch 5000: 60.487159729003906: 0.3723\n",
      "Training finished at 1646078867.653856; lasted 14.92769169807434 seconds.\n",
      "37.230000000000004 % success on test data\n",
      "--- (84) ---\n",
      "Training started at 1646078868.3785954\n",
      "Epoch 0: 69.21725463867188: 0.0958\n",
      "Epoch 5000: 59.284515380859375: 0.5782\n",
      "Training finished at 1646078883.594939; lasted 15.216343641281128 seconds.\n",
      "57.82000000000001 % success on test data\n",
      "--- (85) ---\n",
      "Training started at 1646078884.4228957\n",
      "Epoch 0: 69.10160827636719: 0.1343\n",
      "Epoch 5000: 59.12799072265625: 0.5968\n",
      "Training finished at 1646078899.3792472; lasted 14.956351518630981 seconds.\n",
      "59.68 % success on test data\n",
      "--- (86) ---\n",
      "Training started at 1646078900.090631\n",
      "Epoch 0: 69.17745208740234: 0.1141\n",
      "Epoch 5000: 60.48696517944336: 0.5405\n",
      "Training finished at 1646078915.1574967; lasted 15.066865682601929 seconds.\n",
      "54.05 % success on test data\n",
      "--- (87) ---\n",
      "Training started at 1646078915.8934486\n",
      "Epoch 0: 69.14353942871094: 0.0958\n",
      "Epoch 5000: 61.961944580078125: 0.5087\n",
      "Training finished at 1646078930.9981856; lasted 15.104737043380737 seconds.\n",
      "50.870000000000005 % success on test data\n",
      "--- (88) ---\n",
      "Training started at 1646078931.7769377\n",
      "Epoch 0: 69.26322937011719: 0.1028\n",
      "Epoch 5000: 63.850730895996094: 0.5102\n",
      "Training finished at 1646078946.9180398; lasted 15.141102075576782 seconds.\n",
      "51.019999999999996 % success on test data\n",
      "--- (89) ---\n",
      "Training started at 1646078947.606018\n",
      "Epoch 0: 69.04920196533203: 0.1135\n",
      "Epoch 5000: 59.01167297363281: 0.4463\n",
      "Training finished at 1646078962.3813357; lasted 14.775317668914795 seconds.\n",
      "44.629999999999995 % success on test data\n",
      "--- (90) ---\n",
      "Training started at 1646078963.114403\n",
      "Epoch 0: 69.17426300048828: 0.1135\n",
      "Epoch 5000: 58.90145492553711: 0.5441\n",
      "Training finished at 1646078978.0037637; lasted 14.889360666275024 seconds.\n",
      "54.410000000000004 % success on test data\n",
      "--- (91) ---\n",
      "Training started at 1646078978.7543688\n",
      "Epoch 0: 69.11161041259766: 0.0976\n",
      "Epoch 5000: 61.23095703125: 0.3847\n",
      "Training finished at 1646078993.9028692; lasted 15.148500442504883 seconds.\n",
      "38.47 % success on test data\n",
      "--- (92) ---\n",
      "Training started at 1646078994.6024733\n",
      "Epoch 0: 68.80817413330078: 0.1032\n",
      "Epoch 5000: 61.85881805419922: 0.4436\n",
      "Training finished at 1646079009.5238638; lasted 14.921390533447266 seconds.\n",
      "44.36 % success on test data\n",
      "--- (93) ---\n",
      "Training started at 1646079010.3328702\n",
      "Epoch 0: 69.09467315673828: 0.1171\n",
      "Epoch 5000: 59.66319274902344: 0.5012\n",
      "Training finished at 1646079025.377104; lasted 15.044233798980713 seconds.\n",
      "50.12 % success on test data\n",
      "--- (94) ---\n",
      "Training started at 1646079026.13008\n",
      "Epoch 0: 69.15308380126953: 0.1042\n",
      "Epoch 5000: 65.28675842285156: 0.3789\n",
      "Training finished at 1646079041.0264485; lasted 14.896368503570557 seconds.\n",
      "37.89 % success on test data\n",
      "--- (95) ---\n",
      "Training started at 1646079041.7591162\n",
      "Epoch 0: 69.14942932128906: 0.1172\n",
      "Epoch 5000: 62.90464401245117: 0.364\n",
      "Training finished at 1646079056.5347393; lasted 14.775623083114624 seconds.\n",
      "36.4 % success on test data\n",
      "--- (96) ---\n",
      "Training started at 1646079057.3093972\n",
      "Epoch 0: 69.12410736083984: 0.1009\n",
      "Epoch 5000: 62.42365264892578: 0.4455\n",
      "Training finished at 1646079072.5496712; lasted 15.24027395248413 seconds.\n",
      "44.55 % success on test data\n",
      "--- (97) ---\n",
      "Training started at 1646079073.3821359\n",
      "Epoch 0: 69.12879180908203: 0.0872\n",
      "Epoch 5000: 60.54804611206055: 0.4198\n",
      "Training finished at 1646079088.454575; lasted 15.072439193725586 seconds.\n",
      "41.980000000000004 % success on test data\n",
      "--- (98) ---\n",
      "Training started at 1646079089.1491566\n",
      "Epoch 0: 69.06558990478516: 0.1028\n",
      "Epoch 5000: 65.45758819580078: 0.4209\n",
      "Training finished at 1646079104.3315961; lasted 15.18243956565857 seconds.\n",
      "42.089999999999996 % success on test data\n",
      "--- (99) ---\n",
      "Training started at 1646079105.0416412\n",
      "Epoch 0: 69.13765716552734: 0.1135\n",
      "Epoch 5000: 59.930416107177734: 0.4417\n",
      "Training finished at 1646079119.8138368; lasted 14.77219557762146 seconds.\n",
      "44.17 % success on test data\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(f'--- ({i}) ---')\n",
    "    model = ModelManager.get_untrained(ModelType.MnistMLPNevil)\n",
    "    Coach.train(\n",
    "        model,\n",
    "        data,\n",
    "        nn.CrossEntropyLoss(reduction='sum'),\n",
    "        optim.Adagrad(model.parameters(), lr=1e-3),\n",
    "        30,\n",
    "        5001,\n",
    "        5000\n",
    "    )\n",
    "    adagrad.append(Coach.measure_performance(model, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- (0) ---\n",
      "Training started at 1646079120.7211258\n",
      "Epoch 0: 69.15100860595703: 0.1745\n",
      "Epoch 5000: 48.606529235839844: 0.8925\n",
      "Training finished at 1646079136.0581527; lasted 15.337026834487915 seconds.\n",
      "89.25 % success on test data\n",
      "--- (1) ---\n",
      "Training started at 1646079136.8270915\n",
      "Epoch 0: 69.11441802978516: 0.0974\n",
      "Epoch 5000: 48.745086669921875: 0.8969\n",
      "Training finished at 1646079152.421378; lasted 15.594286441802979 seconds.\n",
      "89.69 % success on test data\n",
      "--- (2) ---\n",
      "Training started at 1646079153.1881292\n",
      "Epoch 0: 69.14160919189453: 0.1048\n",
      "Epoch 5000: 46.385868072509766: 0.8801\n",
      "Training finished at 1646079168.7891693; lasted 15.601040124893188 seconds.\n",
      "88.01 % success on test data\n",
      "--- (3) ---\n",
      "Training started at 1646079169.5683415\n",
      "Epoch 0: 69.12525177001953: 0.0958\n",
      "Epoch 5000: 48.01694869995117: 0.9036\n",
      "Training finished at 1646079185.1105595; lasted 15.54221796989441 seconds.\n",
      "90.36 % success on test data\n",
      "--- (4) ---\n",
      "Training started at 1646079185.8845398\n",
      "Epoch 0: 68.88540649414062: 0.1009\n",
      "Epoch 5000: 52.12564468383789: 0.8191\n",
      "Training finished at 1646079201.69575; lasted 15.81121015548706 seconds.\n",
      "81.91000000000001 % success on test data\n",
      "--- (5) ---\n",
      "Training started at 1646079202.4840045\n",
      "Epoch 0: 68.95597839355469: 0.1525\n",
      "Epoch 5000: 51.846717834472656: 0.8054\n",
      "Training finished at 1646079218.0980434; lasted 15.614038944244385 seconds.\n",
      "80.54 % success on test data\n",
      "--- (6) ---\n",
      "Training started at 1646079218.8779283\n",
      "Epoch 0: 69.10520935058594: 0.1032\n",
      "Epoch 5000: 50.79444122314453: 0.8307\n",
      "Training finished at 1646079234.0893235; lasted 15.211395263671875 seconds.\n",
      "83.07 % success on test data\n",
      "--- (7) ---\n",
      "Training started at 1646079234.8885777\n",
      "Epoch 0: 69.10725402832031: 0.1009\n",
      "Epoch 5000: 47.96782302856445: 0.8306\n",
      "Training finished at 1646079250.3835778; lasted 15.495000123977661 seconds.\n",
      "83.06 % success on test data\n",
      "--- (8) ---\n",
      "Training started at 1646079251.1694813\n",
      "Epoch 0: 69.09475708007812: 0.1028\n",
      "Epoch 5000: 47.87100601196289: 0.9083\n",
      "Training finished at 1646079266.757181; lasted 15.58769965171814 seconds.\n",
      "90.83 % success on test data\n",
      "--- (9) ---\n",
      "Training started at 1646079267.534822\n",
      "Epoch 0: 69.16954803466797: 0.1009\n",
      "Epoch 5000: 46.7063102722168: 0.9044\n",
      "Training finished at 1646079283.0570886; lasted 15.522266626358032 seconds.\n",
      "90.44 % success on test data\n",
      "--- (10) ---\n",
      "Training started at 1646079283.8260965\n",
      "Epoch 0: 68.99749755859375: 0.098\n",
      "Epoch 5000: 49.764678955078125: 0.8381\n",
      "Training finished at 1646079298.8564613; lasted 15.030364751815796 seconds.\n",
      "83.81 % success on test data\n",
      "--- (11) ---\n",
      "Training started at 1646079299.6073613\n",
      "Epoch 0: 69.18790435791016: 0.1857\n",
      "Epoch 5000: 48.87261199951172: 0.8381\n",
      "Training finished at 1646079314.736674; lasted 15.129312753677368 seconds.\n",
      "83.81 % success on test data\n",
      "--- (12) ---\n",
      "Training started at 1646079315.4907904\n",
      "Epoch 0: 69.1837387084961: 0.1374\n",
      "Epoch 5000: 51.518733978271484: 0.8376\n",
      "Training finished at 1646079330.533949; lasted 15.043158531188965 seconds.\n",
      "83.76 % success on test data\n",
      "--- (13) ---\n",
      "Training started at 1646079331.288927\n",
      "Epoch 0: 68.99081420898438: 0.0892\n",
      "Epoch 5000: 49.90838623046875: 0.8329\n",
      "Training finished at 1646079346.5076547; lasted 15.218727588653564 seconds.\n",
      "83.28999999999999 % success on test data\n",
      "--- (14) ---\n",
      "Training started at 1646079347.2533607\n",
      "Epoch 0: 69.10541534423828: 0.1044\n",
      "Epoch 5000: 49.86224365234375: 0.8216\n",
      "Training finished at 1646079362.3815327; lasted 15.128171920776367 seconds.\n",
      "82.16 % success on test data\n",
      "--- (15) ---\n",
      "Training started at 1646079363.1325622\n",
      "Epoch 0: 69.18749237060547: 0.1009\n",
      "Epoch 5000: 46.889617919921875: 0.8317\n",
      "Training finished at 1646079378.534769; lasted 15.402206897735596 seconds.\n",
      "83.17 % success on test data\n",
      "--- (16) ---\n",
      "Training started at 1646079379.2854085\n",
      "Epoch 0: 69.01080322265625: 0.1051\n",
      "Epoch 5000: 48.49885177612305: 0.8248\n",
      "Training finished at 1646079394.6689649; lasted 15.383556365966797 seconds.\n",
      "82.48 % success on test data\n",
      "--- (17) ---\n",
      "Training started at 1646079395.4800794\n",
      "Epoch 0: 68.92430114746094: 0.1009\n",
      "Epoch 5000: 46.5452880859375: 0.8982\n",
      "Training finished at 1646079410.8542953; lasted 15.374215841293335 seconds.\n",
      "89.82 % success on test data\n",
      "--- (18) ---\n",
      "Training started at 1646079411.6252034\n",
      "Epoch 0: 69.04598999023438: 0.0978\n",
      "Epoch 5000: 45.409671783447266: 0.9122\n",
      "Training finished at 1646079427.0755138; lasted 15.450310468673706 seconds.\n",
      "91.22 % success on test data\n",
      "--- (19) ---\n",
      "Training started at 1646079427.8555856\n",
      "Epoch 0: 69.1953353881836: 0.0777\n",
      "Epoch 5000: 48.108177185058594: 0.8374\n",
      "Training finished at 1646079443.1759079; lasted 15.320322275161743 seconds.\n",
      "83.74000000000001 % success on test data\n",
      "--- (20) ---\n",
      "Training started at 1646079443.9909875\n",
      "Epoch 0: 68.9646224975586: 0.101\n",
      "Epoch 5000: 49.29059600830078: 0.8377\n",
      "Training finished at 1646079459.2983387; lasted 15.307351112365723 seconds.\n",
      "83.77 % success on test data\n",
      "--- (21) ---\n",
      "Training started at 1646079460.0670543\n",
      "Epoch 0: 69.10752868652344: 0.1127\n",
      "Epoch 5000: 49.95454025268555: 0.8162\n",
      "Training finished at 1646079477.062706; lasted 16.995651721954346 seconds.\n",
      "81.62 % success on test data\n",
      "--- (22) ---\n",
      "Training started at 1646079477.8754623\n",
      "Epoch 0: 69.01837921142578: 0.1028\n",
      "Epoch 5000: 45.99296569824219: 0.9064\n",
      "Training finished at 1646079496.6914718; lasted 18.816009521484375 seconds.\n",
      "90.64 % success on test data\n",
      "--- (23) ---\n",
      "Training started at 1646079497.4933946\n",
      "Epoch 0: 69.09852600097656: 0.1032\n",
      "Epoch 5000: 44.87471008300781: 0.9066\n",
      "Training finished at 1646079513.2427168; lasted 15.749322175979614 seconds.\n",
      "90.66 % success on test data\n",
      "--- (24) ---\n",
      "Training started at 1646079514.0086632\n",
      "Epoch 0: 69.06768798828125: 0.101\n",
      "Epoch 5000: 45.217674255371094: 0.9071\n",
      "Training finished at 1646079529.3034716; lasted 15.294808387756348 seconds.\n",
      "90.71000000000001 % success on test data\n",
      "--- (25) ---\n",
      "Training started at 1646079530.0661426\n",
      "Epoch 0: 68.93568420410156: 0.0652\n",
      "Epoch 5000: 47.576744079589844: 0.9047\n",
      "Training finished at 1646079545.5551572; lasted 15.489014625549316 seconds.\n",
      "90.47 % success on test data\n",
      "--- (26) ---\n",
      "Training started at 1646079546.3773015\n",
      "Epoch 0: 69.17521667480469: 0.1031\n",
      "Epoch 5000: 47.30021667480469: 0.9057\n",
      "Training finished at 1646079561.9725008; lasted 15.595199346542358 seconds.\n",
      "90.57 % success on test data\n",
      "--- (27) ---\n",
      "Training started at 1646079562.7434778\n",
      "Epoch 0: 68.9775390625: 0.0892\n",
      "Epoch 5000: 46.5151481628418: 0.9035\n",
      "Training finished at 1646079578.0887268; lasted 15.345248937606812 seconds.\n",
      "90.35 % success on test data\n",
      "--- (28) ---\n",
      "Training started at 1646079578.861738\n",
      "Epoch 0: 69.1249008178711: 0.1045\n",
      "Epoch 5000: 51.417293548583984: 0.8291\n",
      "Training finished at 1646079594.3749535; lasted 15.5132155418396 seconds.\n",
      "82.91 % success on test data\n",
      "--- (29) ---\n",
      "Training started at 1646079595.1288788\n",
      "Epoch 0: 69.0534439086914: 0.1001\n",
      "Epoch 5000: 50.065006256103516: 0.9117\n",
      "Training finished at 1646079610.3562658; lasted 15.227386951446533 seconds.\n",
      "91.17 % success on test data\n",
      "--- (30) ---\n",
      "Training started at 1646079611.1270807\n",
      "Epoch 0: 69.13651275634766: 0.1016\n",
      "Epoch 5000: 47.016536712646484: 0.8363\n",
      "Training finished at 1646079626.4245062; lasted 15.297425508499146 seconds.\n",
      "83.63000000000001 % success on test data\n",
      "--- (31) ---\n",
      "Training started at 1646079627.1743531\n",
      "Epoch 0: 69.16134643554688: 0.0974\n",
      "Epoch 5000: 49.25510025024414: 0.8288\n",
      "Training finished at 1646079642.826837; lasted 15.652483940124512 seconds.\n",
      "82.88 % success on test data\n",
      "--- (32) ---\n",
      "Training started at 1646079643.6178012\n",
      "Epoch 0: 69.13104248046875: 0.1032\n",
      "Epoch 5000: 47.778194427490234: 0.8972\n",
      "Training finished at 1646079658.823062; lasted 15.205260753631592 seconds.\n",
      "89.72 % success on test data\n",
      "--- (33) ---\n",
      "Training started at 1646079659.5922444\n",
      "Epoch 0: 69.04376220703125: 0.101\n",
      "Epoch 5000: 49.936363220214844: 0.8286\n",
      "Training finished at 1646079674.9144883; lasted 15.322243928909302 seconds.\n",
      "82.86 % success on test data\n",
      "--- (34) ---\n",
      "Training started at 1646079675.6943314\n",
      "Epoch 0: 69.15744018554688: 0.1028\n",
      "Epoch 5000: 47.7191162109375: 0.9057\n",
      "Training finished at 1646079691.2005591; lasted 15.506227731704712 seconds.\n",
      "90.57 % success on test data\n",
      "--- (35) ---\n",
      "Training started at 1646079691.9896042\n",
      "Epoch 0: 69.0470199584961: 0.097\n",
      "Epoch 5000: 48.562442779541016: 0.8208\n",
      "Training finished at 1646079707.1718214; lasted 15.182217121124268 seconds.\n",
      "82.08 % success on test data\n",
      "--- (36) ---\n",
      "Training started at 1646079707.926853\n",
      "Epoch 0: 69.06620788574219: 0.1622\n",
      "Epoch 5000: 44.97653579711914: 0.9108\n",
      "Training finished at 1646079723.0194247; lasted 15.09257173538208 seconds.\n",
      "91.08000000000001 % success on test data\n",
      "--- (37) ---\n",
      "Training started at 1646079723.7991593\n",
      "Epoch 0: 69.02120208740234: 0.1322\n",
      "Epoch 5000: 49.29542541503906: 0.837\n",
      "Training finished at 1646079738.9334028; lasted 15.134243488311768 seconds.\n",
      "83.7 % success on test data\n",
      "--- (38) ---\n",
      "Training started at 1646079739.6853907\n",
      "Epoch 0: 69.12796020507812: 0.0989\n",
      "Epoch 5000: 46.845951080322266: 0.8316\n",
      "Training finished at 1646079755.0643158; lasted 15.378925085067749 seconds.\n",
      "83.16 % success on test data\n",
      "--- (39) ---\n",
      "Training started at 1646079755.8366628\n",
      "Epoch 0: 69.03121185302734: 0.1032\n",
      "Epoch 5000: 44.922691345214844: 0.9092\n",
      "Training finished at 1646079771.0799365; lasted 15.243273735046387 seconds.\n",
      "90.92 % success on test data\n",
      "--- (40) ---\n",
      "Training started at 1646079771.8559632\n",
      "Epoch 0: 69.05107116699219: 0.0958\n",
      "Epoch 5000: 49.6301383972168: 0.8303\n",
      "Training finished at 1646079787.2512019; lasted 15.395238637924194 seconds.\n",
      "83.03 % success on test data\n",
      "--- (41) ---\n",
      "Training started at 1646079787.9943306\n",
      "Epoch 0: 68.92594909667969: 0.1667\n",
      "Epoch 5000: 47.04119873046875: 0.8949\n",
      "Training finished at 1646079803.6323812; lasted 15.638050556182861 seconds.\n",
      "89.49000000000001 % success on test data\n",
      "--- (42) ---\n",
      "Training started at 1646079804.4023666\n",
      "Epoch 0: 69.1148681640625: 0.1009\n",
      "Epoch 5000: 47.4350471496582: 0.8308\n",
      "Training finished at 1646079819.9407494; lasted 15.538382768630981 seconds.\n",
      "83.08 % success on test data\n",
      "--- (43) ---\n",
      "Training started at 1646079820.688738\n",
      "Epoch 0: 69.19488525390625: 0.1862\n",
      "Epoch 5000: 46.658447265625: 0.8362\n",
      "Training finished at 1646079836.0010548; lasted 15.312316656112671 seconds.\n",
      "83.62 % success on test data\n",
      "--- (44) ---\n",
      "Training started at 1646079836.7519429\n",
      "Epoch 0: 69.11334991455078: 0.1028\n",
      "Epoch 5000: 55.30821228027344: 0.745\n",
      "Training finished at 1646079852.187931; lasted 15.435988187789917 seconds.\n",
      "74.5 % success on test data\n",
      "--- (45) ---\n",
      "Training started at 1646079852.9538586\n",
      "Epoch 0: 68.96177673339844: 0.0974\n",
      "Epoch 5000: 50.78240966796875: 0.8221\n",
      "Training finished at 1646079868.4568555; lasted 15.502996921539307 seconds.\n",
      "82.21000000000001 % success on test data\n",
      "--- (46) ---\n",
      "Training started at 1646079869.2040243\n",
      "Epoch 0: 69.08817291259766: 0.101\n",
      "Epoch 5000: 49.93825149536133: 0.9096\n",
      "Training finished at 1646079884.4225297; lasted 15.218505382537842 seconds.\n",
      "90.96 % success on test data\n",
      "--- (47) ---\n",
      "Training started at 1646079885.1874912\n",
      "Epoch 0: 69.12811279296875: 0.0892\n",
      "Epoch 5000: 49.45246505737305: 0.821\n",
      "Training finished at 1646079900.472645; lasted 15.285153865814209 seconds.\n",
      "82.1 % success on test data\n",
      "--- (48) ---\n",
      "Training started at 1646079901.272199\n",
      "Epoch 0: 69.16741943359375: 0.0961\n",
      "Epoch 5000: 44.006378173828125: 0.9013\n",
      "Training finished at 1646079916.8096595; lasted 15.537460565567017 seconds.\n",
      "90.13 % success on test data\n",
      "--- (49) ---\n",
      "Training started at 1646079917.580897\n",
      "Epoch 0: 69.14127349853516: 0.0892\n",
      "Epoch 5000: 47.68714904785156: 0.9017\n",
      "Training finished at 1646079933.0033035; lasted 15.422406435012817 seconds.\n",
      "90.16999999999999 % success on test data\n",
      "--- (50) ---\n",
      "Training started at 1646079933.7752526\n",
      "Epoch 0: 69.12733459472656: 0.0982\n",
      "Epoch 5000: 44.868247985839844: 0.9036\n",
      "Training finished at 1646079949.0895977; lasted 15.314345121383667 seconds.\n",
      "90.36 % success on test data\n",
      "--- (51) ---\n",
      "Training started at 1646079949.861493\n",
      "Epoch 0: 68.951904296875: 0.1028\n",
      "Epoch 5000: 49.059661865234375: 0.8307\n",
      "Training finished at 1646079965.2668386; lasted 15.405345439910889 seconds.\n",
      "83.07 % success on test data\n",
      "--- (52) ---\n",
      "Training started at 1646079966.0733676\n",
      "Epoch 0: 68.98768615722656: 0.1031\n",
      "Epoch 5000: 47.385128021240234: 0.9158\n",
      "Training finished at 1646079981.454584; lasted 15.381216287612915 seconds.\n",
      "91.58 % success on test data\n",
      "--- (53) ---\n",
      "Training started at 1646079982.2270215\n",
      "Epoch 0: 69.14607238769531: 0.1817\n",
      "Epoch 5000: 46.565242767333984: 0.847\n",
      "Training finished at 1646079997.434356; lasted 15.207334518432617 seconds.\n",
      "84.7 % success on test data\n",
      "--- (54) ---\n",
      "Training started at 1646079998.2011886\n",
      "Epoch 0: 69.1550521850586: 0.1028\n",
      "Epoch 5000: 49.92161560058594: 0.7589\n",
      "Training finished at 1646080013.715595; lasted 15.514406442642212 seconds.\n",
      "75.89 % success on test data\n",
      "--- (55) ---\n",
      "Training started at 1646080014.5097733\n",
      "Epoch 0: 69.10353088378906: 0.098\n",
      "Epoch 5000: 49.35177993774414: 0.8283\n",
      "Training finished at 1646080029.5798528; lasted 15.070079565048218 seconds.\n",
      "82.83 % success on test data\n",
      "--- (56) ---\n",
      "Training started at 1646080030.3530142\n",
      "Epoch 0: 69.0775146484375: 0.1237\n",
      "Epoch 5000: 47.47821044921875: 0.9086\n",
      "Training finished at 1646080045.8011029; lasted 15.448088645935059 seconds.\n",
      "90.86 % success on test data\n",
      "--- (57) ---\n",
      "Training started at 1646080046.5726998\n",
      "Epoch 0: 68.9683609008789: 0.1552\n",
      "Epoch 5000: 50.21295166015625: 0.8451\n",
      "Training finished at 1646080061.8634064; lasted 15.290706634521484 seconds.\n",
      "84.50999999999999 % success on test data\n",
      "--- (58) ---\n",
      "Training started at 1646080062.6333146\n",
      "Epoch 0: 69.1162109375: 0.0958\n",
      "Epoch 5000: 47.40727233886719: 0.9024\n",
      "Training finished at 1646080078.0666468; lasted 15.433332204818726 seconds.\n",
      "90.24 % success on test data\n",
      "--- (59) ---\n",
      "Training started at 1646080078.8267226\n",
      "Epoch 0: 68.9477310180664: 0.0892\n",
      "Epoch 5000: 46.63431930541992: 0.8381\n",
      "Training finished at 1646080094.138675; lasted 15.311952352523804 seconds.\n",
      "83.81 % success on test data\n",
      "--- (60) ---\n",
      "Training started at 1646080094.895518\n",
      "Epoch 0: 69.23406982421875: 0.1242\n",
      "Epoch 5000: 48.12548065185547: 0.8732\n",
      "Training finished at 1646080110.3732486; lasted 15.477730512619019 seconds.\n",
      "87.32 % success on test data\n",
      "--- (61) ---\n",
      "Training started at 1646080111.1452215\n",
      "Epoch 0: 69.07476043701172: 0.1743\n",
      "Epoch 5000: 51.50127029418945: 0.7446\n",
      "Training finished at 1646080126.2483637; lasted 15.103142261505127 seconds.\n",
      "74.46000000000001 % success on test data\n",
      "--- (62) ---\n",
      "Training started at 1646080127.018468\n",
      "Epoch 0: 69.00511932373047: 0.0892\n",
      "Epoch 5000: 49.882198333740234: 0.7464\n",
      "Training finished at 1646080142.4670453; lasted 15.448577404022217 seconds.\n",
      "74.64 % success on test data\n",
      "--- (63) ---\n",
      "Training started at 1646080143.2202032\n",
      "Epoch 0: 69.19215393066406: 0.1182\n",
      "Epoch 5000: 48.80435562133789: 0.8308\n",
      "Training finished at 1646080158.3616033; lasted 15.14140009880066 seconds.\n",
      "83.08 % success on test data\n",
      "--- (64) ---\n",
      "Training started at 1646080159.1279457\n",
      "Epoch 0: 69.04914855957031: 0.1124\n",
      "Epoch 5000: 49.78703308105469: 0.8135\n",
      "Training finished at 1646080174.678299; lasted 15.550353288650513 seconds.\n",
      "81.35 % success on test data\n",
      "--- (65) ---\n",
      "Training started at 1646080175.4623625\n",
      "Epoch 0: 69.05098724365234: 0.0958\n",
      "Epoch 5000: 47.27186965942383: 0.8364\n",
      "Training finished at 1646080190.8075228; lasted 15.345160245895386 seconds.\n",
      "83.64 % success on test data\n",
      "--- (66) ---\n",
      "Training started at 1646080191.5595899\n",
      "Epoch 0: 69.15243530273438: 0.0982\n",
      "Epoch 5000: 46.35075378417969: 0.9036\n",
      "Training finished at 1646080206.7777824; lasted 15.21819257736206 seconds.\n",
      "90.36 % success on test data\n",
      "--- (67) ---\n",
      "Training started at 1646080207.5578609\n",
      "Epoch 0: 69.19020080566406: 0.1037\n",
      "Epoch 5000: 46.74274444580078: 0.9075\n",
      "Training finished at 1646080222.8038647; lasted 15.246003866195679 seconds.\n",
      "90.75 % success on test data\n",
      "--- (68) ---\n",
      "Training started at 1646080223.5688705\n",
      "Epoch 0: 69.0811767578125: 0.1126\n",
      "Epoch 5000: 47.02634811401367: 0.7788\n",
      "Training finished at 1646080238.742121; lasted 15.173250436782837 seconds.\n",
      "77.88000000000001 % success on test data\n",
      "--- (69) ---\n",
      "Training started at 1646080239.498923\n",
      "Epoch 0: 69.00310516357422: 0.0974\n",
      "Epoch 5000: 47.192901611328125: 0.8914\n",
      "Training finished at 1646080254.9522986; lasted 15.453375577926636 seconds.\n",
      "89.14 % success on test data\n",
      "--- (70) ---\n",
      "Training started at 1646080255.7250001\n",
      "Epoch 0: 69.10189056396484: 0.1034\n",
      "Epoch 5000: 48.76299285888672: 0.8209\n",
      "Training finished at 1646080270.9266114; lasted 15.201611280441284 seconds.\n",
      "82.09 % success on test data\n",
      "--- (71) ---\n",
      "Training started at 1646080271.7346125\n",
      "Epoch 0: 68.99386596679688: 0.101\n",
      "Epoch 5000: 44.954490661621094: 0.8256\n",
      "Training finished at 1646080287.0657206; lasted 15.331108093261719 seconds.\n",
      "82.56 % success on test data\n",
      "--- (72) ---\n",
      "Training started at 1646080287.8660192\n",
      "Epoch 0: 69.05120849609375: 0.0958\n",
      "Epoch 5000: 47.61565017700195: 0.8951\n",
      "Training finished at 1646080303.273612; lasted 15.4075927734375 seconds.\n",
      "89.51 % success on test data\n",
      "--- (73) ---\n",
      "Training started at 1646080304.0469124\n",
      "Epoch 0: 69.02864837646484: 0.1135\n",
      "Epoch 5000: 46.155059814453125: 0.878\n",
      "Training finished at 1646080319.1274667; lasted 15.080554246902466 seconds.\n",
      "87.8 % success on test data\n",
      "--- (74) ---\n",
      "Training started at 1646080319.9033306\n",
      "Epoch 0: 69.09416198730469: 0.1013\n",
      "Epoch 5000: 48.02012252807617: 0.8258\n",
      "Training finished at 1646080335.5159438; lasted 15.612613201141357 seconds.\n",
      "82.58 % success on test data\n",
      "--- (75) ---\n",
      "Training started at 1646080336.2667782\n",
      "Epoch 0: 69.01319122314453: 0.1241\n",
      "Epoch 5000: 45.01016616821289: 0.9019\n",
      "Training finished at 1646080351.4319572; lasted 15.165179014205933 seconds.\n",
      "90.19 % success on test data\n",
      "--- (76) ---\n",
      "Training started at 1646080352.2014492\n",
      "Epoch 0: 68.99420928955078: 0.1056\n",
      "Epoch 5000: 44.55058288574219: 0.8307\n",
      "Training finished at 1646080367.3602705; lasted 15.15882134437561 seconds.\n",
      "83.07 % success on test data\n",
      "--- (77) ---\n",
      "Training started at 1646080368.1113944\n",
      "Epoch 0: 69.11944580078125: 0.098\n",
      "Epoch 5000: 46.94886016845703: 0.9083\n",
      "Training finished at 1646080383.1424868; lasted 15.031092405319214 seconds.\n",
      "90.83 % success on test data\n",
      "--- (78) ---\n",
      "Training started at 1646080383.9064174\n",
      "Epoch 0: 69.03604888916016: 0.098\n",
      "Epoch 5000: 47.90123748779297: 0.8381\n",
      "Training finished at 1646080399.0796137; lasted 15.17319631576538 seconds.\n",
      "83.81 % success on test data\n",
      "--- (79) ---\n",
      "Training started at 1646080399.8729587\n",
      "Epoch 0: 69.18402099609375: 0.0958\n",
      "Epoch 5000: 44.93939208984375: 0.9192\n",
      "Training finished at 1646080415.2829738; lasted 15.410015106201172 seconds.\n",
      "91.92 % success on test data\n",
      "--- (80) ---\n",
      "Training started at 1646080416.045022\n",
      "Epoch 0: 69.1539535522461: 0.0815\n",
      "Epoch 5000: 47.3109130859375: 0.8588\n",
      "Training finished at 1646080431.4829924; lasted 15.437970399856567 seconds.\n",
      "85.88 % success on test data\n",
      "--- (81) ---\n",
      "Training started at 1646080432.2553015\n",
      "Epoch 0: 68.63093566894531: 0.098\n",
      "Epoch 5000: 48.18852615356445: 0.8412\n",
      "Training finished at 1646080447.3245375; lasted 15.069236040115356 seconds.\n",
      "84.11999999999999 % success on test data\n",
      "--- (82) ---\n",
      "Training started at 1646080448.0756934\n",
      "Epoch 0: 69.0831527709961: 0.1009\n",
      "Epoch 5000: 47.68746566772461: 0.8451\n",
      "Training finished at 1646080463.54779; lasted 15.472096681594849 seconds.\n",
      "84.50999999999999 % success on test data\n",
      "--- (83) ---\n",
      "Training started at 1646080464.3229923\n",
      "Epoch 0: 69.11509704589844: 0.1009\n",
      "Epoch 5000: 47.91526412963867: 0.8313\n",
      "Training finished at 1646080479.8433137; lasted 15.520321369171143 seconds.\n",
      "83.13000000000001 % success on test data\n",
      "--- (84) ---\n",
      "Training started at 1646080480.5867763\n",
      "Epoch 0: 69.01506805419922: 0.0976\n",
      "Epoch 5000: 49.04701232910156: 0.9109\n",
      "Training finished at 1646080495.716149; lasted 15.129372835159302 seconds.\n",
      "91.09 % success on test data\n",
      "--- (85) ---\n",
      "Training started at 1646080496.4834726\n",
      "Epoch 0: 68.92664337158203: 0.0974\n",
      "Epoch 5000: 45.284202575683594: 0.8957\n",
      "Training finished at 1646080511.973407; lasted 15.48993444442749 seconds.\n",
      "89.57000000000001 % success on test data\n",
      "--- (86) ---\n",
      "Training started at 1646080512.7416015\n",
      "Epoch 0: 69.28508758544922: 0.0966\n",
      "Epoch 5000: 48.118839263916016: 0.8961\n",
      "Training finished at 1646080528.3150196; lasted 15.573418140411377 seconds.\n",
      "89.61 % success on test data\n",
      "--- (87) ---\n",
      "Training started at 1646080529.0846026\n",
      "Epoch 0: 69.31922912597656: 0.1009\n",
      "Epoch 5000: 49.619842529296875: 0.8188\n",
      "Training finished at 1646080544.732845; lasted 15.648242473602295 seconds.\n",
      "81.88 % success on test data\n",
      "--- (88) ---\n",
      "Training started at 1646080545.5240228\n",
      "Epoch 0: 68.99311828613281: 0.1032\n",
      "Epoch 5000: 46.91475296020508: 0.9223\n",
      "Training finished at 1646080560.8474076; lasted 15.323384761810303 seconds.\n",
      "92.23 % success on test data\n",
      "--- (89) ---\n",
      "Training started at 1646080561.634313\n",
      "Epoch 0: 69.17338562011719: 0.1032\n",
      "Epoch 5000: 47.46019744873047: 0.8955\n",
      "Training finished at 1646080576.805054; lasted 15.170740842819214 seconds.\n",
      "89.55 % success on test data\n",
      "--- (90) ---\n",
      "Training started at 1646080577.564607\n",
      "Epoch 0: 69.09992218017578: 0.1028\n",
      "Epoch 5000: 44.18684005737305: 0.8371\n",
      "Training finished at 1646080593.0446386; lasted 15.480031728744507 seconds.\n",
      "83.71 % success on test data\n",
      "--- (91) ---\n",
      "Training started at 1646080593.818638\n",
      "Epoch 0: 69.08841705322266: 0.098\n",
      "Epoch 5000: 44.06803894042969: 0.9058\n",
      "Training finished at 1646080608.9463017; lasted 15.127663612365723 seconds.\n",
      "90.58 % success on test data\n",
      "--- (92) ---\n",
      "Training started at 1646080609.7177913\n",
      "Epoch 0: 68.97427368164062: 0.0892\n",
      "Epoch 5000: 47.09450149536133: 0.8248\n",
      "Training finished at 1646080624.9991724; lasted 15.281381130218506 seconds.\n",
      "82.48 % success on test data\n",
      "--- (93) ---\n",
      "Training started at 1646080625.7764795\n",
      "Epoch 0: 68.905029296875: 0.1035\n",
      "Epoch 5000: 45.36469268798828: 0.9042\n",
      "Training finished at 1646080641.2362452; lasted 15.459765672683716 seconds.\n",
      "90.42 % success on test data\n",
      "--- (94) ---\n",
      "Training started at 1646080642.0049205\n",
      "Epoch 0: 69.02899932861328: 0.1434\n",
      "Epoch 5000: 46.014976501464844: 0.91\n",
      "Training finished at 1646080657.5943797; lasted 15.58945918083191 seconds.\n",
      "91.0 % success on test data\n",
      "--- (95) ---\n",
      "Training started at 1646080658.384712\n",
      "Epoch 0: 69.02195739746094: 0.1926\n",
      "Epoch 5000: 49.74710464477539: 0.8448\n",
      "Training finished at 1646080673.7716475; lasted 15.386935472488403 seconds.\n",
      "84.48 % success on test data\n",
      "--- (96) ---\n",
      "Training started at 1646080674.551811\n",
      "Epoch 0: 69.02627563476562: 0.1156\n",
      "Epoch 5000: 47.174720764160156: 0.8389\n",
      "Training finished at 1646080689.8741302; lasted 15.322319269180298 seconds.\n",
      "83.89 % success on test data\n",
      "--- (97) ---\n",
      "Training started at 1646080690.6299543\n",
      "Epoch 0: 69.16812133789062: 0.0824\n",
      "Epoch 5000: 47.087955474853516: 0.9083\n",
      "Training finished at 1646080706.068962; lasted 15.439007759094238 seconds.\n",
      "90.83 % success on test data\n",
      "--- (98) ---\n",
      "Training started at 1646080706.8321328\n",
      "Epoch 0: 69.06529235839844: 0.0958\n",
      "Epoch 5000: 46.89120864868164: 0.8298\n",
      "Training finished at 1646080722.234624; lasted 15.402491092681885 seconds.\n",
      "82.98 % success on test data\n",
      "--- (99) ---\n",
      "Training started at 1646080723.0413015\n",
      "Epoch 0: 68.90462493896484: 0.0982\n",
      "Epoch 5000: 50.38014221191406: 0.8438\n",
      "Training finished at 1646080738.5139973; lasted 15.47269582748413 seconds.\n",
      "84.38 % success on test data\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(f'--- ({i}) ---')\n",
    "    model = ModelManager.get_untrained(ModelType.MnistMLPNevil)\n",
    "    Coach.train(\n",
    "        model,\n",
    "        data,\n",
    "        nn.CrossEntropyLoss(reduction='sum'),\n",
    "        optim.RMSprop(model.parameters(), lr=1e-3),\n",
    "        30,\n",
    "        5001,\n",
    "        5000\n",
    "    )\n",
    "    rmsprop.append(Coach.measure_performance(model, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- (0) ---\n",
      "Training started at 1646080739.4161644\n",
      "Epoch 0: 68.89351654052734: 0.0982\n",
      "Epoch 5000: 47.08837127685547: 0.8957\n",
      "Training finished at 1646080755.490407; lasted 16.07424259185791 seconds.\n",
      "89.57000000000001 % success on test data\n",
      "--- (1) ---\n",
      "Training started at 1646080756.259678\n",
      "Epoch 0: 69.12211608886719: 0.098\n",
      "Epoch 5000: 50.613975524902344: 0.7522\n",
      "Training finished at 1646080772.267286; lasted 16.00760817527771 seconds.\n",
      "75.22 % success on test data\n",
      "--- (2) ---\n",
      "Training started at 1646080773.035034\n",
      "Epoch 0: 69.02107238769531: 0.101\n",
      "Epoch 5000: 51.20534133911133: 0.8343\n",
      "Training finished at 1646080789.0347776; lasted 15.999743700027466 seconds.\n",
      "83.43 % success on test data\n",
      "--- (3) ---\n",
      "Training started at 1646080789.784947\n",
      "Epoch 0: 68.96357727050781: 0.0892\n",
      "Epoch 5000: 45.468406677246094: 0.9254\n",
      "Training finished at 1646080805.6207979; lasted 15.835850954055786 seconds.\n",
      "92.54 % success on test data\n",
      "--- (4) ---\n",
      "Training started at 1646080806.403254\n",
      "Epoch 0: 69.0676498413086: 0.0982\n",
      "Epoch 5000: 49.64488220214844: 0.7716\n",
      "Training finished at 1646080822.065503; lasted 15.662248849868774 seconds.\n",
      "77.16 % success on test data\n",
      "--- (5) ---\n",
      "Training started at 1646080822.8197534\n",
      "Epoch 0: 69.00565338134766: 0.098\n",
      "Epoch 5000: 45.998111724853516: 0.905\n",
      "Training finished at 1646080838.3349478; lasted 15.515194416046143 seconds.\n",
      "90.5 % success on test data\n",
      "--- (6) ---\n",
      "Training started at 1646080839.0971184\n",
      "Epoch 0: 69.10260009765625: 0.098\n",
      "Epoch 5000: 45.64999008178711: 0.9081\n",
      "Training finished at 1646080855.1450021; lasted 16.04788374900818 seconds.\n",
      "90.81 % success on test data\n",
      "--- (7) ---\n",
      "Training started at 1646080855.914253\n",
      "Epoch 0: 69.0688247680664: 0.0982\n",
      "Epoch 5000: 50.77029800415039: 0.7475\n",
      "Training finished at 1646080871.6177; lasted 15.703447103500366 seconds.\n",
      "74.75 % success on test data\n",
      "--- (8) ---\n",
      "Training started at 1646080872.3676434\n",
      "Epoch 0: 69.0790023803711: 0.0892\n",
      "Epoch 5000: 49.33943176269531: 0.8863\n",
      "Training finished at 1646080888.2984848; lasted 15.930841445922852 seconds.\n",
      "88.63 % success on test data\n",
      "--- (9) ---\n",
      "Training started at 1646080889.0857294\n",
      "Epoch 0: 69.33094787597656: 0.0892\n",
      "Epoch 5000: 48.78129959106445: 0.8347\n",
      "Training finished at 1646080904.824782; lasted 15.739052534103394 seconds.\n",
      "83.47 % success on test data\n",
      "--- (10) ---\n",
      "Training started at 1646080905.573835\n",
      "Epoch 0: 68.94683837890625: 0.0892\n",
      "Epoch 5000: 47.29513931274414: 0.9166\n",
      "Training finished at 1646080921.4149923; lasted 15.84115743637085 seconds.\n",
      "91.66 % success on test data\n",
      "--- (11) ---\n",
      "Training started at 1646080922.1859708\n",
      "Epoch 0: 69.15985107421875: 0.101\n",
      "Epoch 5000: 45.240997314453125: 0.838\n",
      "Training finished at 1646080937.898408; lasted 15.712437152862549 seconds.\n",
      "83.8 % success on test data\n",
      "--- (12) ---\n",
      "Training started at 1646080938.6513963\n",
      "Epoch 0: 68.8992919921875: 0.1143\n",
      "Epoch 5000: 49.20986557006836: 0.9074\n",
      "Training finished at 1646080954.5327427; lasted 15.881346464157104 seconds.\n",
      "90.74 % success on test data\n",
      "--- (13) ---\n",
      "Training started at 1646080955.296463\n",
      "Epoch 0: 69.08985900878906: 0.0982\n",
      "Epoch 5000: 47.617008209228516: 0.8216\n",
      "Training finished at 1646080971.0967152; lasted 15.800252199172974 seconds.\n",
      "82.16 % success on test data\n",
      "--- (14) ---\n",
      "Training started at 1646080971.8999536\n",
      "Epoch 0: 69.08743286132812: 0.1135\n",
      "Epoch 5000: 44.433509826660156: 0.8891\n",
      "Training finished at 1646080987.6879818; lasted 15.788028240203857 seconds.\n",
      "88.91 % success on test data\n",
      "--- (15) ---\n",
      "Training started at 1646080988.4458184\n",
      "Epoch 0: 69.12193298339844: 0.1011\n",
      "Epoch 5000: 45.678077697753906: 0.9021\n",
      "Training finished at 1646081004.1112401; lasted 15.665421724319458 seconds.\n",
      "90.21000000000001 % success on test data\n",
      "--- (16) ---\n",
      "Training started at 1646081004.8732147\n",
      "Epoch 0: 69.14322662353516: 0.0892\n",
      "Epoch 5000: 48.49217224121094: 0.8989\n",
      "Training finished at 1646081020.760427; lasted 15.88721227645874 seconds.\n",
      "89.89 % success on test data\n",
      "--- (17) ---\n",
      "Training started at 1646081021.5303779\n",
      "Epoch 0: 69.04830932617188: 0.0958\n",
      "Epoch 5000: 47.88275146484375: 0.8503\n",
      "Training finished at 1646081037.343553; lasted 15.813175201416016 seconds.\n",
      "85.03 % success on test data\n",
      "--- (18) ---\n",
      "Training started at 1646081038.1104891\n",
      "Epoch 0: 69.14903259277344: 0.1028\n",
      "Epoch 5000: 52.87454605102539: 0.8445\n",
      "Training finished at 1646081054.0692341; lasted 15.958745002746582 seconds.\n",
      "84.45 % success on test data\n",
      "--- (19) ---\n",
      "Training started at 1646081054.8172796\n",
      "Epoch 0: 69.01057434082031: 0.1228\n",
      "Epoch 5000: 50.81277847290039: 0.7583\n",
      "Training finished at 1646081070.4086797; lasted 15.591400146484375 seconds.\n",
      "75.83 % success on test data\n",
      "--- (20) ---\n",
      "Training started at 1646081071.2115965\n",
      "Epoch 0: 69.16921997070312: 0.0954\n",
      "Epoch 5000: 47.005462646484375: 0.8205\n",
      "Training finished at 1646081087.057415; lasted 15.845818519592285 seconds.\n",
      "82.05 % success on test data\n",
      "--- (21) ---\n",
      "Training started at 1646081087.8534126\n",
      "Epoch 0: 69.10228729248047: 0.0892\n",
      "Epoch 5000: 46.21223831176758: 0.8786\n",
      "Training finished at 1646081103.7047453; lasted 15.851332664489746 seconds.\n",
      "87.86 % success on test data\n",
      "--- (22) ---\n",
      "Training started at 1646081104.4808795\n",
      "Epoch 0: 69.0168685913086: 0.1085\n",
      "Epoch 5000: 47.70295715332031: 0.8925\n",
      "Training finished at 1646081120.323155; lasted 15.842275381088257 seconds.\n",
      "89.25 % success on test data\n",
      "--- (23) ---\n",
      "Training started at 1646081121.094578\n",
      "Epoch 0: 68.98197937011719: 0.0982\n",
      "Epoch 5000: 47.20687484741211: 0.9039\n",
      "Training finished at 1646081136.821398; lasted 15.72681999206543 seconds.\n",
      "90.39 % success on test data\n",
      "--- (24) ---\n",
      "Training started at 1646081137.5868652\n",
      "Epoch 0: 68.98566436767578: 0.0958\n",
      "Epoch 5000: 47.697349548339844: 0.8253\n",
      "Training finished at 1646081153.5001042; lasted 15.913239002227783 seconds.\n",
      "82.53 % success on test data\n",
      "--- (25) ---\n",
      "Training started at 1646081154.2490683\n",
      "Epoch 0: 69.03202819824219: 0.098\n",
      "Epoch 5000: 48.94087219238281: 0.9079\n",
      "Training finished at 1646081169.7693827; lasted 15.520314455032349 seconds.\n",
      "90.79 % success on test data\n",
      "--- (26) ---\n",
      "Training started at 1646081170.5206306\n",
      "Epoch 0: 69.02172088623047: 0.098\n",
      "Epoch 5000: 47.62483215332031: 0.8337\n",
      "Training finished at 1646081186.1636233; lasted 15.642992734909058 seconds.\n",
      "83.37 % success on test data\n",
      "--- (27) ---\n",
      "Training started at 1646081186.9145324\n",
      "Epoch 0: 69.13672637939453: 0.1037\n",
      "Epoch 5000: 45.81813049316406: 0.9099\n",
      "Training finished at 1646081202.568884; lasted 15.654351472854614 seconds.\n",
      "90.99000000000001 % success on test data\n",
      "--- (28) ---\n",
      "Training started at 1646081203.3191345\n",
      "Epoch 0: 68.97113800048828: 0.0982\n",
      "Epoch 5000: 46.29521179199219: 0.9117\n",
      "Training finished at 1646081219.1842194; lasted 15.865084886550903 seconds.\n",
      "91.17 % success on test data\n",
      "--- (29) ---\n",
      "Training started at 1646081219.9760947\n",
      "Epoch 0: 69.08045959472656: 0.1012\n",
      "Epoch 5000: 46.804107666015625: 0.834\n",
      "Training finished at 1646081235.701554; lasted 15.725459337234497 seconds.\n",
      "83.39999999999999 % success on test data\n",
      "--- (30) ---\n",
      "Training started at 1646081236.48838\n",
      "Epoch 0: 69.02449035644531: 0.1249\n",
      "Epoch 5000: 50.62705612182617: 0.8425\n",
      "Training finished at 1646081252.385622; lasted 15.897242069244385 seconds.\n",
      "84.25 % success on test data\n",
      "--- (31) ---\n",
      "Training started at 1646081253.15561\n",
      "Epoch 0: 69.03215789794922: 0.0974\n",
      "Epoch 5000: 47.21306228637695: 0.9084\n",
      "Training finished at 1646081269.3858209; lasted 16.230210781097412 seconds.\n",
      "90.84 % success on test data\n",
      "--- (32) ---\n",
      "Training started at 1646081270.1549175\n",
      "Epoch 0: 69.15238952636719: 0.0982\n",
      "Epoch 5000: 48.879722595214844: 0.7451\n",
      "Training finished at 1646081285.9820592; lasted 15.827141761779785 seconds.\n",
      "74.51 % success on test data\n",
      "--- (33) ---\n",
      "Training started at 1646081286.7070448\n",
      "Epoch 0: 69.08106994628906: 0.0892\n",
      "Epoch 5000: 53.22699737548828: 0.7377\n",
      "Training finished at 1646081302.4903417; lasted 15.783296823501587 seconds.\n",
      "73.77 % success on test data\n",
      "--- (34) ---\n",
      "Training started at 1646081303.3293095\n",
      "Epoch 0: 69.05303192138672: 0.0982\n",
      "Epoch 5000: 51.804141998291016: 0.8163\n",
      "Training finished at 1646081319.0685947; lasted 15.739285230636597 seconds.\n",
      "81.63 % success on test data\n",
      "--- (35) ---\n",
      "Training started at 1646081319.843694\n",
      "Epoch 0: 68.82786560058594: 0.0982\n",
      "Epoch 5000: 45.914154052734375: 0.8315\n",
      "Training finished at 1646081335.6208622; lasted 15.777168273925781 seconds.\n",
      "83.15 % success on test data\n",
      "--- (36) ---\n",
      "Training started at 1646081336.395786\n",
      "Epoch 0: 69.10691833496094: 0.1032\n",
      "Epoch 5000: 47.485572814941406: 0.8388\n",
      "Training finished at 1646081352.10909; lasted 15.713304042816162 seconds.\n",
      "83.88 % success on test data\n",
      "--- (37) ---\n",
      "Training started at 1646081352.8459172\n",
      "Epoch 0: 68.94339752197266: 0.098\n",
      "Epoch 5000: 53.868892669677734: 0.7591\n",
      "Training finished at 1646081368.4803336; lasted 15.634416341781616 seconds.\n",
      "75.91 % success on test data\n",
      "--- (38) ---\n",
      "Training started at 1646081369.2686174\n",
      "Epoch 0: 69.08155822753906: 0.0958\n",
      "Epoch 5000: 51.452064514160156: 0.817\n",
      "Training finished at 1646081385.2108777; lasted 15.942260265350342 seconds.\n",
      "81.69999999999999 % success on test data\n",
      "--- (39) ---\n",
      "Training started at 1646081386.0095453\n",
      "Epoch 0: 69.0560073852539: 0.0962\n",
      "Epoch 5000: 51.69781494140625: 0.754\n",
      "Training finished at 1646081401.7888327; lasted 15.779287338256836 seconds.\n",
      "75.4 % success on test data\n",
      "--- (40) ---\n",
      "Training started at 1646081402.5601244\n",
      "Epoch 0: 69.06982421875: 0.0921\n",
      "Epoch 5000: 46.082942962646484: 0.9054\n",
      "Training finished at 1646081418.4479272; lasted 15.887802839279175 seconds.\n",
      "90.53999999999999 % success on test data\n",
      "--- (41) ---\n",
      "Training started at 1646081419.2414691\n",
      "Epoch 0: 69.01128387451172: 0.1099\n",
      "Epoch 5000: 46.668548583984375: 0.9007\n",
      "Training finished at 1646081435.2742896; lasted 16.032820463180542 seconds.\n",
      "90.07 % success on test data\n",
      "--- (42) ---\n",
      "Training started at 1646081436.0275743\n",
      "Epoch 0: 69.2535629272461: 0.0982\n",
      "Epoch 5000: 49.3316650390625: 0.8317\n",
      "Training finished at 1646081451.7976758; lasted 15.770101547241211 seconds.\n",
      "83.17 % success on test data\n",
      "--- (43) ---\n",
      "Training started at 1646081452.5896995\n",
      "Epoch 0: 69.15556335449219: 0.0958\n",
      "Epoch 5000: 47.78993606567383: 0.8901\n",
      "Training finished at 1646081468.438144; lasted 15.84844446182251 seconds.\n",
      "89.01 % success on test data\n",
      "--- (44) ---\n",
      "Training started at 1646081469.1981344\n",
      "Epoch 0: 69.0396728515625: 0.1032\n",
      "Epoch 5000: 46.40153503417969: 0.9217\n",
      "Training finished at 1646081484.975708; lasted 15.777573585510254 seconds.\n",
      "92.17 % success on test data\n",
      "--- (45) ---\n",
      "Training started at 1646081485.7382596\n",
      "Epoch 0: 69.07481384277344: 0.0958\n",
      "Epoch 5000: 50.90264892578125: 0.8391\n",
      "Training finished at 1646081501.5659702; lasted 15.827710628509521 seconds.\n",
      "83.91 % success on test data\n",
      "--- (46) ---\n",
      "Training started at 1646081502.3192267\n",
      "Epoch 0: 69.12834930419922: 0.1032\n",
      "Epoch 5000: 44.129146575927734: 0.9026\n",
      "Training finished at 1646081517.9715145; lasted 15.652287721633911 seconds.\n",
      "90.25999999999999 % success on test data\n",
      "--- (47) ---\n",
      "Training started at 1646081518.7501752\n",
      "Epoch 0: 69.04618072509766: 0.1032\n",
      "Epoch 5000: 52.06006622314453: 0.8378\n",
      "Training finished at 1646081534.4594784; lasted 15.709303140640259 seconds.\n",
      "83.78 % success on test data\n",
      "--- (48) ---\n",
      "Training started at 1646081535.2104366\n",
      "Epoch 0: 69.16815185546875: 0.0984\n",
      "Epoch 5000: 46.63896179199219: 0.8982\n",
      "Training finished at 1646081551.1917033; lasted 15.981266736984253 seconds.\n",
      "89.82 % success on test data\n",
      "--- (49) ---\n",
      "Training started at 1646081551.9906995\n",
      "Epoch 0: 69.18046569824219: 0.0982\n",
      "Epoch 5000: 47.964744567871094: 0.8256\n",
      "Training finished at 1646081567.7761579; lasted 15.785458326339722 seconds.\n",
      "82.56 % success on test data\n",
      "--- (50) ---\n",
      "Training started at 1646081568.5328956\n",
      "Epoch 0: 69.05782318115234: 0.0916\n",
      "Epoch 5000: 48.335147857666016: 0.837\n",
      "Training finished at 1646081584.4476082; lasted 15.91471266746521 seconds.\n",
      "83.7 % success on test data\n",
      "--- (51) ---\n",
      "Training started at 1646081585.2023463\n",
      "Epoch 0: 69.21861267089844: 0.0974\n",
      "Epoch 5000: 46.43300247192383: 0.9056\n",
      "Training finished at 1646081601.3594148; lasted 16.157068490982056 seconds.\n",
      "90.56 % success on test data\n",
      "--- (52) ---\n",
      "Training started at 1646081602.1174998\n",
      "Epoch 0: 68.91277313232422: 0.1028\n",
      "Epoch 5000: 47.760738372802734: 0.9038\n",
      "Training finished at 1646081618.049952; lasted 15.932452201843262 seconds.\n",
      "90.38000000000001 % success on test data\n",
      "--- (53) ---\n",
      "Training started at 1646081618.8197114\n",
      "Epoch 0: 69.21082305908203: 0.098\n",
      "Epoch 5000: 49.66775894165039: 0.8229\n",
      "Training finished at 1646081634.3389306; lasted 15.519219160079956 seconds.\n",
      "82.28999999999999 % success on test data\n",
      "--- (54) ---\n",
      "Training started at 1646081635.085873\n",
      "Epoch 0: 69.05796813964844: 0.098\n",
      "Epoch 5000: 47.83562469482422: 0.8377\n",
      "Training finished at 1646081650.71854; lasted 15.632667064666748 seconds.\n",
      "83.77 % success on test data\n",
      "--- (55) ---\n",
      "Training started at 1646081651.4870262\n",
      "Epoch 0: 69.16807556152344: 0.1135\n",
      "Epoch 5000: 49.131492614746094: 0.8465\n",
      "Training finished at 1646081667.080433; lasted 15.593406677246094 seconds.\n",
      "84.65 % success on test data\n",
      "--- (56) ---\n",
      "Training started at 1646081667.8415616\n",
      "Epoch 0: 68.90853118896484: 0.0995\n",
      "Epoch 5000: 48.64213180541992: 0.902\n",
      "Training finished at 1646081683.6126962; lasted 15.771134614944458 seconds.\n",
      "90.2 % success on test data\n",
      "--- (57) ---\n",
      "Training started at 1646081684.383631\n",
      "Epoch 0: 69.2151107788086: 0.1028\n",
      "Epoch 5000: 56.421897888183594: 0.7488\n",
      "Training finished at 1646081700.263389; lasted 15.87975811958313 seconds.\n",
      "74.88 % success on test data\n",
      "--- (58) ---\n",
      "Training started at 1646081701.0308838\n",
      "Epoch 0: 69.08692169189453: 0.1009\n",
      "Epoch 5000: 44.97214126586914: 0.9194\n",
      "Training finished at 1646081717.0626402; lasted 16.03175640106201 seconds.\n",
      "91.94 % success on test data\n",
      "--- (59) ---\n",
      "Training started at 1646081717.8375502\n",
      "Epoch 0: 69.04300689697266: 0.0958\n",
      "Epoch 5000: 49.1152458190918: 0.7629\n",
      "Training finished at 1646081733.562067; lasted 15.724516868591309 seconds.\n",
      "76.29 % success on test data\n",
      "--- (60) ---\n",
      "Training started at 1646081734.315667\n",
      "Epoch 0: 68.99126434326172: 0.0995\n",
      "Epoch 5000: 50.60022735595703: 0.8343\n",
      "Training finished at 1646081750.280688; lasted 15.965021133422852 seconds.\n",
      "83.43 % success on test data\n",
      "--- (61) ---\n",
      "Training started at 1646081751.0264983\n",
      "Epoch 0: 69.08416748046875: 0.0941\n",
      "Epoch 5000: 50.50315856933594: 0.8378\n",
      "Training finished at 1646081766.8395424; lasted 15.81304407119751 seconds.\n",
      "83.78 % success on test data\n",
      "--- (62) ---\n",
      "Training started at 1646081767.5929012\n",
      "Epoch 0: 69.12565612792969: 0.098\n",
      "Epoch 5000: 44.90304946899414: 0.8967\n",
      "Training finished at 1646081783.2141604; lasted 15.621259212493896 seconds.\n",
      "89.67 % success on test data\n",
      "--- (63) ---\n",
      "Training started at 1646081784.0102456\n",
      "Epoch 0: 69.12785339355469: 0.0982\n",
      "Epoch 5000: 46.517234802246094: 0.9181\n",
      "Training finished at 1646081799.7554524; lasted 15.745206832885742 seconds.\n",
      "91.81 % success on test data\n",
      "--- (64) ---\n",
      "Training started at 1646081800.5304227\n",
      "Epoch 0: 69.06993865966797: 0.1009\n",
      "Epoch 5000: 47.11498260498047: 0.915\n",
      "Training finished at 1646081816.3037076; lasted 15.773284912109375 seconds.\n",
      "91.5 % success on test data\n",
      "--- (65) ---\n",
      "Training started at 1646081817.0836837\n",
      "Epoch 0: 69.11771392822266: 0.1135\n",
      "Epoch 5000: 45.71976852416992: 0.9031\n",
      "Training finished at 1646081832.6421416; lasted 15.558457851409912 seconds.\n",
      "90.31 % success on test data\n",
      "--- (66) ---\n",
      "Training started at 1646081833.4176638\n",
      "Epoch 0: 69.06509399414062: 0.1135\n",
      "Epoch 5000: 46.42069625854492: 0.9154\n",
      "Training finished at 1646081849.1233284; lasted 15.70566463470459 seconds.\n",
      "91.53999999999999 % success on test data\n",
      "--- (67) ---\n",
      "Training started at 1646081849.8978343\n",
      "Epoch 0: 69.0733871459961: 0.1009\n",
      "Epoch 5000: 45.787113189697266: 0.8998\n",
      "Training finished at 1646081865.9361148; lasted 16.038280487060547 seconds.\n",
      "89.98 % success on test data\n",
      "--- (68) ---\n",
      "Training started at 1646081866.6930103\n",
      "Epoch 0: 68.96625518798828: 0.1032\n",
      "Epoch 5000: 48.32691955566406: 0.9139\n",
      "Training finished at 1646081882.433355; lasted 15.740344762802124 seconds.\n",
      "91.39 % success on test data\n",
      "--- (69) ---\n",
      "Training started at 1646081883.1973343\n",
      "Epoch 0: 69.18965911865234: 0.0982\n",
      "Epoch 5000: 52.684913635253906: 0.763\n",
      "Training finished at 1646081898.8935666; lasted 15.696232318878174 seconds.\n",
      "76.3 % success on test data\n",
      "--- (70) ---\n",
      "Training started at 1646081899.654571\n",
      "Epoch 0: 68.97728729248047: 0.1135\n",
      "Epoch 5000: 49.012672424316406: 0.8282\n",
      "Training finished at 1646081915.2978537; lasted 15.643282651901245 seconds.\n",
      "82.82000000000001 % success on test data\n",
      "--- (71) ---\n",
      "Training started at 1646081916.0498633\n",
      "Epoch 0: 69.40020751953125: 0.1135\n",
      "Epoch 5000: 47.700679779052734: 0.8102\n",
      "Training finished at 1646081931.701124; lasted 15.651260614395142 seconds.\n",
      "81.02000000000001 % success on test data\n",
      "--- (72) ---\n",
      "Training started at 1646081932.446486\n",
      "Epoch 0: 69.2756576538086: 0.1135\n",
      "Epoch 5000: 46.658172607421875: 0.9024\n",
      "Training finished at 1646081948.1788049; lasted 15.732318878173828 seconds.\n",
      "90.24 % success on test data\n",
      "--- (73) ---\n",
      "Training started at 1646081948.94643\n",
      "Epoch 0: 69.25745391845703: 0.0945\n",
      "Epoch 5000: 46.94231414794922: 0.9125\n",
      "Training finished at 1646081965.1376145; lasted 16.191184520721436 seconds.\n",
      "91.25 % success on test data\n",
      "--- (74) ---\n",
      "Training started at 1646081965.912515\n",
      "Epoch 0: 69.17422485351562: 0.0982\n",
      "Epoch 5000: 44.74315643310547: 0.91\n",
      "Training finished at 1646081981.7801323; lasted 15.86761736869812 seconds.\n",
      "91.0 % success on test data\n",
      "--- (75) ---\n",
      "Training started at 1646081982.5647762\n",
      "Epoch 0: 69.1781997680664: 0.1009\n",
      "Epoch 5000: 44.25873565673828: 0.9069\n",
      "Training finished at 1646081998.5680091; lasted 16.003232955932617 seconds.\n",
      "90.69 % success on test data\n",
      "--- (76) ---\n",
      "Training started at 1646081999.3420646\n",
      "Epoch 0: 69.08731079101562: 0.105\n",
      "Epoch 5000: 47.840850830078125: 0.9035\n",
      "Training finished at 1646082015.240321; lasted 15.898256301879883 seconds.\n",
      "90.35 % success on test data\n",
      "--- (77) ---\n",
      "Training started at 1646082016.0035236\n",
      "Epoch 0: 68.9570541381836: 0.0974\n",
      "Epoch 5000: 47.831947326660156: 0.8307\n",
      "Training finished at 1646082032.0025544; lasted 15.999030828475952 seconds.\n",
      "83.07 % success on test data\n",
      "--- (78) ---\n",
      "Training started at 1646082032.7484992\n",
      "Epoch 0: 69.09600067138672: 0.0957\n",
      "Epoch 5000: 46.297393798828125: 0.7436\n",
      "Training finished at 1646082048.70283; lasted 15.954330921173096 seconds.\n",
      "74.36 % success on test data\n",
      "--- (79) ---\n",
      "Training started at 1646082049.4645202\n",
      "Epoch 0: 69.08544158935547: 0.1135\n",
      "Epoch 5000: 52.17488098144531: 0.7602\n",
      "Training finished at 1646082065.158022; lasted 15.693501710891724 seconds.\n",
      "76.02 % success on test data\n",
      "--- (80) ---\n",
      "Training started at 1646082065.9482384\n",
      "Epoch 0: 68.90672302246094: 0.1028\n",
      "Epoch 5000: 45.33482360839844: 0.911\n",
      "Training finished at 1646082081.9362252; lasted 15.98798680305481 seconds.\n",
      "91.10000000000001 % success on test data\n",
      "--- (81) ---\n",
      "Training started at 1646082082.6948965\n",
      "Epoch 0: 69.11051940917969: 0.0964\n",
      "Epoch 5000: 46.11349868774414: 0.9083\n",
      "Training finished at 1646082098.71451; lasted 16.01961350440979 seconds.\n",
      "90.83 % success on test data\n",
      "--- (82) ---\n",
      "Training started at 1646082099.4805357\n",
      "Epoch 0: 69.113037109375: 0.1012\n",
      "Epoch 5000: 52.22057342529297: 0.7525\n",
      "Training finished at 1646082115.2517626; lasted 15.77122688293457 seconds.\n",
      "75.25 % success on test data\n",
      "--- (83) ---\n",
      "Training started at 1646082116.0327508\n",
      "Epoch 0: 69.22732543945312: 0.0958\n",
      "Epoch 5000: 49.78231430053711: 0.8084\n",
      "Training finished at 1646082131.9469893; lasted 15.914238452911377 seconds.\n",
      "80.84 % success on test data\n",
      "--- (84) ---\n",
      "Training started at 1646082132.7095926\n",
      "Epoch 0: 68.98193359375: 0.0958\n",
      "Epoch 5000: 47.562889099121094: 0.9045\n",
      "Training finished at 1646082148.6812413; lasted 15.971648693084717 seconds.\n",
      "90.45 % success on test data\n",
      "--- (85) ---\n",
      "Training started at 1646082149.4596841\n",
      "Epoch 0: 68.8979263305664: 0.1011\n",
      "Epoch 5000: 48.7806510925293: 0.8953\n",
      "Training finished at 1646082165.4694803; lasted 16.009796142578125 seconds.\n",
      "89.53 % success on test data\n",
      "--- (86) ---\n",
      "Training started at 1646082166.2394652\n",
      "Epoch 0: 69.05280303955078: 0.0892\n",
      "Epoch 5000: 51.884830474853516: 0.8355\n",
      "Training finished at 1646082182.0826693; lasted 15.843204021453857 seconds.\n",
      "83.55 % success on test data\n",
      "--- (87) ---\n",
      "Training started at 1646082182.8220844\n",
      "Epoch 0: 69.04588317871094: 0.0982\n",
      "Epoch 5000: 49.805747985839844: 0.8298\n",
      "Training finished at 1646082198.6074324; lasted 15.785347938537598 seconds.\n",
      "82.98 % success on test data\n",
      "--- (88) ---\n",
      "Training started at 1646082199.3510914\n",
      "Epoch 0: 69.05233001708984: 0.0958\n",
      "Epoch 5000: 46.9488639831543: 0.8924\n",
      "Training finished at 1646082215.3233507; lasted 15.972259283065796 seconds.\n",
      "89.24 % success on test data\n",
      "--- (89) ---\n",
      "Training started at 1646082216.090784\n",
      "Epoch 0: 69.20352935791016: 0.0982\n",
      "Epoch 5000: 47.970726013183594: 0.8346\n",
      "Training finished at 1646082231.8101332; lasted 15.719349145889282 seconds.\n",
      "83.46000000000001 % success on test data\n",
      "--- (90) ---\n",
      "Training started at 1646082232.5724263\n",
      "Epoch 0: 69.14676666259766: 0.1184\n",
      "Epoch 5000: 49.5396842956543: 0.8364\n",
      "Training finished at 1646082248.344375; lasted 15.77194857597351 seconds.\n",
      "83.64 % success on test data\n",
      "--- (91) ---\n",
      "Training started at 1646082249.121644\n",
      "Epoch 0: 69.10395812988281: 0.1023\n",
      "Epoch 5000: 46.89298629760742: 0.9148\n",
      "Training finished at 1646082265.128045; lasted 16.00640106201172 seconds.\n",
      "91.47999999999999 % success on test data\n",
      "--- (92) ---\n",
      "Training started at 1646082265.8963172\n",
      "Epoch 0: 69.16371154785156: 0.1009\n",
      "Epoch 5000: 45.77845764160156: 0.9154\n",
      "Training finished at 1646082282.127926; lasted 16.231608867645264 seconds.\n",
      "91.53999999999999 % success on test data\n",
      "--- (93) ---\n",
      "Training started at 1646082282.9139023\n",
      "Epoch 0: 69.0481185913086: 0.0892\n",
      "Epoch 5000: 47.219932556152344: 0.8271\n",
      "Training finished at 1646082298.6568086; lasted 15.742906332015991 seconds.\n",
      "82.71 % success on test data\n",
      "--- (94) ---\n",
      "Training started at 1646082299.3961372\n",
      "Epoch 0: 69.03059387207031: 0.0974\n",
      "Epoch 5000: 48.33821487426758: 0.8165\n",
      "Training finished at 1646082315.4443336; lasted 16.04819631576538 seconds.\n",
      "81.65 % success on test data\n",
      "--- (95) ---\n",
      "Training started at 1646082316.2310424\n",
      "Epoch 0: 69.14754486083984: 0.1032\n",
      "Epoch 5000: 48.68617248535156: 0.8333\n",
      "Training finished at 1646082331.8678882; lasted 15.636845827102661 seconds.\n",
      "83.33 % success on test data\n",
      "--- (96) ---\n",
      "Training started at 1646082332.614745\n",
      "Epoch 0: 69.19419860839844: 0.1002\n",
      "Epoch 5000: 45.47930145263672: 0.8983\n",
      "Training finished at 1646082348.3958092; lasted 15.78106427192688 seconds.\n",
      "89.83 % success on test data\n",
      "--- (97) ---\n",
      "Training started at 1646082349.1637995\n",
      "Epoch 0: 69.15452575683594: 0.1009\n",
      "Epoch 5000: 45.53773498535156: 0.9103\n",
      "Training finished at 1646082364.983066; lasted 15.819266557693481 seconds.\n",
      "91.03 % success on test data\n",
      "--- (98) ---\n",
      "Training started at 1646082365.7450342\n",
      "Epoch 0: 68.93053436279297: 0.101\n",
      "Epoch 5000: 49.79825210571289: 0.8392\n",
      "Training finished at 1646082381.4953792; lasted 15.75034499168396 seconds.\n",
      "83.91999999999999 % success on test data\n",
      "--- (99) ---\n",
      "Training started at 1646082382.2352412\n",
      "Epoch 0: 69.20661926269531: 0.0974\n",
      "Epoch 5000: 46.4942626953125: 0.9118\n",
      "Training finished at 1646082398.468123; lasted 16.232881784439087 seconds.\n",
      "91.18 % success on test data\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(f'--- ({i}) ---')\n",
    "    model = ModelManager.get_untrained(ModelType.MnistMLPNevil)\n",
    "    Coach.train(\n",
    "        model,\n",
    "        data,\n",
    "        nn.CrossEntropyLoss(reduction='sum'),\n",
    "        optim.Adam(model.parameters(), lr=1e-3),\n",
    "        30,\n",
    "        5001,\n",
    "        5000\n",
    "    )\n",
    "    adam.append(Coach.measure_performance(model, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x18c2544bf40>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for performences in [sorted(simple), sorted(momentum), sorted(nesterov)]:\n",
    "performences = sorted(simple)\n",
    "distribution = {}\n",
    "for i in range(1, len(performences) + 1):\n",
    "    distribution[performences[i - 1]] = i / len(performences)\n",
    "my_list = distribution.items()\n",
    "sorted(my_list)\n",
    "x, y = zip(*my_list)\n",
    "lab.plot(x, y, color = \"blue\", label=\"SGD\")\n",
    "\n",
    "performences = sorted(adagrad)\n",
    "distribution = {}\n",
    "for i in range(1, len(performences) + 1):\n",
    "    distribution[performences[i - 1]] = i / len(performences)\n",
    "my_list = distribution.items()\n",
    "sorted(my_list)\n",
    "x, y = zip(*my_list)\n",
    "lab.plot(x, y, color = \"red\", label=\"Adagrad\")\n",
    "\n",
    "performences = sorted(rmsprop)\n",
    "distribution = {}\n",
    "for i in range(1, len(performences) + 1):\n",
    "    distribution[performences[i - 1]] = i / len(performences)\n",
    "my_list = distribution.items()\n",
    "sorted(my_list)\n",
    "x, y = zip(*my_list)\n",
    "lab.plot(x, y, color=\"green\", label=\"RMSProp\")\n",
    "\n",
    "performences = sorted(adam)\n",
    "distribution = {}\n",
    "for i in range(1, len(performences) + 1):\n",
    "    distribution[performences[i - 1]] = i / len(performences)\n",
    "my_list = distribution.items()\n",
    "sorted(my_list)\n",
    "x, y = zip(*my_list)\n",
    "lab.plot(x, y, color=\"yellow\", label=\"Adam\")\n",
    "\n",
    "lab.legend()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9d03d93fdbeeb361cc45deae4888a34596acc9ca3c93366af240ad46910d2d2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
