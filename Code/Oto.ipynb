{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import Coach\n",
    "from utils import MnistData\n",
    "from models import ModelManager, ModelType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MnistData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple, adagrad, rmsprop, adam = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- (0) ---\n",
      "Training started at 1646148902.8487957\n",
      "Epoch 0: 69.19628143310547: 0.0951\n",
      "Epoch 5000: 43.85758972167969: 0.958\n",
      "Training finished at 1646148940.8139017; lasted 37.96510601043701 seconds.\n",
      "95.8 % success on test data\n",
      "--- (1) ---\n",
      "Training started at 1646148941.867013\n",
      "Epoch 0: 69.07324981689453: 0.1135\n",
      "Epoch 5000: 45.03499221801758: 0.9645\n",
      "Training finished at 1646148979.2819812; lasted 37.41496825218201 seconds.\n",
      "96.45 % success on test data\n",
      "--- (2) ---\n",
      "Training started at 1646148980.3341572\n",
      "Epoch 0: 68.95417022705078: 0.0958\n",
      "Epoch 5000: 43.83464813232422: 0.9652\n",
      "Training finished at 1646149018.0974371; lasted 37.76327991485596 seconds.\n",
      "96.52 % success on test data\n",
      "--- (3) ---\n",
      "Training started at 1646149019.1602802\n",
      "Epoch 0: 69.12506103515625: 0.101\n",
      "Epoch 5000: 44.51701354980469: 0.9659\n",
      "Training finished at 1646149057.2805028; lasted 38.12022256851196 seconds.\n",
      "96.59 % success on test data\n",
      "--- (4) ---\n",
      "Training started at 1646149058.3500872\n",
      "Epoch 0: 69.04450225830078: 0.1032\n",
      "Epoch 5000: 44.305145263671875: 0.9616\n",
      "Training finished at 1646149095.920612; lasted 37.57052493095398 seconds.\n",
      "96.16 % success on test data\n",
      "--- (5) ---\n",
      "Training started at 1646149097.063177\n",
      "Epoch 0: 69.02365112304688: 0.0974\n",
      "Epoch 5000: 43.83522415161133: 0.9555\n",
      "Training finished at 1646149134.9973242; lasted 37.934147119522095 seconds.\n",
      "95.55 % success on test data\n",
      "--- (6) ---\n",
      "Training started at 1646149136.090454\n",
      "Epoch 0: 69.08209228515625: 0.0982\n",
      "Epoch 5000: 44.01951599121094: 0.9631\n",
      "Training finished at 1646149173.5168853; lasted 37.42643117904663 seconds.\n",
      "96.31 % success on test data\n",
      "--- (7) ---\n",
      "Training started at 1646149174.6195443\n",
      "Epoch 0: 69.10625457763672: 0.0922\n",
      "Epoch 5000: 48.01192092895508: 0.8785\n",
      "Training finished at 1646149211.919425; lasted 37.29988074302673 seconds.\n",
      "87.85 % success on test data\n",
      "--- (8) ---\n",
      "Training started at 1646149212.9537354\n",
      "Epoch 0: 69.139892578125: 0.0779\n",
      "Epoch 5000: 45.293548583984375: 0.9579\n",
      "Training finished at 1646149251.1799476; lasted 38.2262122631073 seconds.\n",
      "95.78999999999999 % success on test data\n",
      "--- (9) ---\n",
      "Training started at 1646149252.3062346\n",
      "Epoch 0: 69.01163482666016: 0.0892\n",
      "Epoch 5000: 45.638938903808594: 0.9675\n",
      "Training finished at 1646149290.105977; lasted 37.799742460250854 seconds.\n",
      "96.75 % success on test data\n",
      "--- (10) ---\n",
      "Training started at 1646149291.1663117\n",
      "Epoch 0: 68.97752380371094: 0.0974\n",
      "Epoch 5000: 45.86616134643555: 0.9653\n",
      "Training finished at 1646149329.508847; lasted 38.34253525733948 seconds.\n",
      "96.53 % success on test data\n",
      "--- (11) ---\n",
      "Training started at 1646149330.5582223\n",
      "Epoch 0: 69.05607604980469: 0.1028\n",
      "Epoch 5000: 43.835693359375: 0.9704\n",
      "Training finished at 1646149368.4737284; lasted 37.91550612449646 seconds.\n",
      "97.04 % success on test data\n",
      "--- (12) ---\n",
      "Training started at 1646149369.593096\n",
      "Epoch 0: 69.08502197265625: 0.098\n",
      "Epoch 5000: 46.4970703125: 0.9588\n",
      "Training finished at 1646149407.1751122; lasted 37.58201622962952 seconds.\n",
      "95.88 % success on test data\n",
      "--- (13) ---\n",
      "Training started at 1646149408.2949693\n",
      "Epoch 0: 68.90593719482422: 0.0924\n",
      "Epoch 5000: 45.76921081542969: 0.9635\n",
      "Training finished at 1646149446.1466608; lasted 37.851691484451294 seconds.\n",
      "96.35000000000001 % success on test data\n",
      "--- (14) ---\n",
      "Training started at 1646149447.1990123\n",
      "Epoch 0: 69.04642486572266: 0.101\n",
      "Epoch 5000: 44.833797454833984: 0.9705\n",
      "Training finished at 1646149485.2331657; lasted 38.0341534614563 seconds.\n",
      "97.05 % success on test data\n",
      "--- (15) ---\n",
      "Training started at 1646149486.302475\n",
      "Epoch 0: 69.1108169555664: 0.1009\n",
      "Epoch 5000: 46.230979919433594: 0.9643\n",
      "Training finished at 1646149523.7295442; lasted 37.42706918716431 seconds.\n",
      "96.43 % success on test data\n",
      "--- (16) ---\n",
      "Training started at 1646149524.7768617\n",
      "Epoch 0: 69.0591049194336: 0.1009\n",
      "Epoch 5000: 43.88133239746094: 0.9678\n",
      "Training finished at 1646149562.1739304; lasted 37.39706873893738 seconds.\n",
      "96.78 % success on test data\n",
      "--- (17) ---\n",
      "Training started at 1646149563.2170537\n",
      "Epoch 0: 69.01020812988281: 0.0892\n",
      "Epoch 5000: 44.81821060180664: 0.9642\n",
      "Training finished at 1646149600.482427; lasted 37.26537322998047 seconds.\n",
      "96.41999999999999 % success on test data\n",
      "--- (18) ---\n",
      "Training started at 1646149601.5415268\n",
      "Epoch 0: 69.2197265625: 0.1115\n",
      "Epoch 5000: 44.244354248046875: 0.9645\n",
      "Training finished at 1646149638.8848858; lasted 37.34335899353027 seconds.\n",
      "96.45 % success on test data\n",
      "--- (19) ---\n",
      "Training started at 1646149639.9289048\n",
      "Epoch 0: 69.20835876464844: 0.1009\n",
      "Epoch 5000: 45.0821533203125: 0.9654\n",
      "Training finished at 1646149677.4019985; lasted 37.47309374809265 seconds.\n",
      "96.54 % success on test data\n",
      "--- (20) ---\n",
      "Training started at 1646149678.4313557\n",
      "Epoch 0: 69.00580596923828: 0.1028\n",
      "Epoch 5000: 43.83556365966797: 0.9646\n",
      "Training finished at 1646149715.8176694; lasted 37.386313676834106 seconds.\n",
      "96.46000000000001 % success on test data\n",
      "--- (21) ---\n",
      "Training started at 1646149716.877568\n",
      "Epoch 0: 69.15631866455078: 0.0892\n",
      "Epoch 5000: 44.837791442871094: 0.9578\n",
      "Training finished at 1646149754.4802928; lasted 37.60272479057312 seconds.\n",
      "95.78 % success on test data\n",
      "--- (22) ---\n",
      "Training started at 1646149755.5090268\n",
      "Epoch 0: 69.2575912475586: 0.1342\n",
      "Epoch 5000: 45.840858459472656: 0.9697\n",
      "Training finished at 1646149792.8323681; lasted 37.323341369628906 seconds.\n",
      "96.97 % success on test data\n",
      "--- (23) ---\n",
      "Training started at 1646149793.8903763\n",
      "Epoch 0: 69.01338958740234: 0.0892\n",
      "Epoch 5000: 46.79277420043945: 0.9642\n",
      "Training finished at 1646149831.103755; lasted 37.21337866783142 seconds.\n",
      "96.41999999999999 % success on test data\n",
      "--- (24) ---\n",
      "Training started at 1646149832.149334\n",
      "Epoch 0: 69.10557556152344: 0.0893\n",
      "Epoch 5000: 43.834564208984375: 0.9695\n",
      "Training finished at 1646149869.4272196; lasted 37.2778856754303 seconds.\n",
      "96.95 % success on test data\n",
      "--- (25) ---\n",
      "Training started at 1646149870.4763417\n",
      "Epoch 0: 69.10122680664062: 0.098\n",
      "Epoch 5000: 44.153114318847656: 0.9626\n",
      "Training finished at 1646149907.5598214; lasted 37.08347964286804 seconds.\n",
      "96.26 % success on test data\n",
      "--- (26) ---\n",
      "Training started at 1646149908.6179042\n",
      "Epoch 0: 69.00745391845703: 0.0892\n",
      "Epoch 5000: 44.5606575012207: 0.9698\n",
      "Training finished at 1646149945.8321242; lasted 37.21422004699707 seconds.\n",
      "96.98 % success on test data\n",
      "--- (27) ---\n",
      "Training started at 1646149946.9162042\n",
      "Epoch 0: 69.11176300048828: 0.0892\n",
      "Epoch 5000: 45.8337516784668: 0.8771\n",
      "Training finished at 1646149984.4365268; lasted 37.52032256126404 seconds.\n",
      "87.71 % success on test data\n",
      "--- (28) ---\n",
      "Training started at 1646149985.454124\n",
      "Epoch 0: 69.10970306396484: 0.1135\n",
      "Epoch 5000: 44.838645935058594: 0.9538\n",
      "Training finished at 1646150022.474612; lasted 37.020488023757935 seconds.\n",
      "95.38 % success on test data\n",
      "--- (29) ---\n",
      "Training started at 1646150023.52001\n",
      "Epoch 0: 69.0946044921875: 0.1025\n",
      "Epoch 5000: 44.84026336669922: 0.9542\n",
      "Training finished at 1646150060.898232; lasted 37.37822198867798 seconds.\n",
      "95.42 % success on test data\n",
      "--- (30) ---\n",
      "Training started at 1646150061.9217937\n",
      "Epoch 0: 69.15655517578125: 0.1142\n",
      "Epoch 5000: 46.091285705566406: 0.9583\n",
      "Training finished at 1646150098.9721825; lasted 37.0503888130188 seconds.\n",
      "95.83 % success on test data\n",
      "--- (31) ---\n",
      "Training started at 1646150100.0172222\n",
      "Epoch 0: 69.11211395263672: 0.1009\n",
      "Epoch 5000: 43.84684753417969: 0.9627\n",
      "Training finished at 1646150137.8715656; lasted 37.85434341430664 seconds.\n",
      "96.27 % success on test data\n",
      "--- (32) ---\n",
      "Training started at 1646150138.9003658\n",
      "Epoch 0: 69.17481231689453: 0.0982\n",
      "Epoch 5000: 43.83838653564453: 0.9674\n",
      "Training finished at 1646150176.100485; lasted 37.20011925697327 seconds.\n",
      "96.74000000000001 % success on test data\n",
      "--- (33) ---\n",
      "Training started at 1646150177.153561\n",
      "Epoch 0: 68.95081329345703: 0.1021\n",
      "Epoch 5000: 47.93085479736328: 0.967\n",
      "Training finished at 1646150214.3381631; lasted 37.18460202217102 seconds.\n",
      "96.7 % success on test data\n",
      "--- (34) ---\n",
      "Training started at 1646150215.3698616\n",
      "Epoch 0: 69.06758117675781: 0.1052\n",
      "Epoch 5000: 47.0797233581543: 0.9648\n",
      "Training finished at 1646150252.8044207; lasted 37.43455910682678 seconds.\n",
      "96.48 % success on test data\n",
      "--- (35) ---\n",
      "Training started at 1646150253.844999\n",
      "Epoch 0: 69.27552795410156: 0.1147\n",
      "Epoch 5000: 43.83519744873047: 0.9607\n",
      "Training finished at 1646150290.9994774; lasted 37.154478311538696 seconds.\n",
      "96.07 % success on test data\n",
      "--- (36) ---\n",
      "Training started at 1646150292.0296059\n",
      "Epoch 0: 69.24417114257812: 0.098\n",
      "Epoch 5000: 44.49993133544922: 0.9676\n",
      "Training finished at 1646150329.1899693; lasted 37.16036343574524 seconds.\n",
      "96.76 % success on test data\n",
      "--- (37) ---\n",
      "Training started at 1646150330.2308373\n",
      "Epoch 0: 69.2369155883789: 0.098\n",
      "Epoch 5000: 48.555076599121094: 0.8746\n",
      "Training finished at 1646150367.7138102; lasted 37.482972860336304 seconds.\n",
      "87.46000000000001 % success on test data\n",
      "--- (38) ---\n",
      "Training started at 1646150368.7821665\n",
      "Epoch 0: 69.02438354492188: 0.1007\n",
      "Epoch 5000: 44.17449951171875: 0.9682\n",
      "Training finished at 1646150406.0454469; lasted 37.263280391693115 seconds.\n",
      "96.82 % success on test data\n",
      "--- (39) ---\n",
      "Training started at 1646150407.0889876\n",
      "Epoch 0: 69.04387664794922: 0.1197\n",
      "Epoch 5000: 44.858238220214844: 0.9607\n",
      "Training finished at 1646150444.6150622; lasted 37.52607464790344 seconds.\n",
      "96.07 % success on test data\n",
      "--- (40) ---\n",
      "Training started at 1646150445.6509688\n",
      "Epoch 0: 68.92739868164062: 0.1028\n",
      "Epoch 5000: 44.0635986328125: 0.9646\n",
      "Training finished at 1646150483.122231; lasted 37.47126221656799 seconds.\n",
      "96.46000000000001 % success on test data\n",
      "--- (41) ---\n",
      "Training started at 1646150484.1653814\n",
      "Epoch 0: 69.0589370727539: 0.1135\n",
      "Epoch 5000: 44.947166442871094: 0.9606\n",
      "Training finished at 1646150521.6744652; lasted 37.50908374786377 seconds.\n",
      "96.06 % success on test data\n",
      "--- (42) ---\n",
      "Training started at 1646150522.7995164\n",
      "Epoch 0: 68.96704864501953: 0.0896\n",
      "Epoch 5000: 47.194908142089844: 0.9274\n",
      "Training finished at 1646150560.1857147; lasted 37.38619828224182 seconds.\n",
      "92.74 % success on test data\n",
      "--- (43) ---\n",
      "Training started at 1646150561.2341406\n",
      "Epoch 0: 69.09445190429688: 0.0863\n",
      "Epoch 5000: 45.35536193847656: 0.9575\n",
      "Training finished at 1646150598.956288; lasted 37.7221474647522 seconds.\n",
      "95.75 % success on test data\n",
      "--- (44) ---\n",
      "Training started at 1646150600.031969\n",
      "Epoch 0: 68.96654510498047: 0.0958\n",
      "Epoch 5000: 43.835113525390625: 0.971\n",
      "Training finished at 1646150637.5036814; lasted 37.47171235084534 seconds.\n",
      "97.1 % success on test data\n",
      "--- (45) ---\n",
      "Training started at 1646150638.5924826\n",
      "Epoch 0: 69.09366607666016: 0.1135\n",
      "Epoch 5000: 45.57090377807617: 0.9723\n",
      "Training finished at 1646150676.1598895; lasted 37.56740689277649 seconds.\n",
      "97.23 % success on test data\n",
      "--- (46) ---\n",
      "Training started at 1646150677.2425237\n",
      "Epoch 0: 68.9354476928711: 0.1009\n",
      "Epoch 5000: 43.98500061035156: 0.9746\n",
      "Training finished at 1646150715.06074; lasted 37.81821632385254 seconds.\n",
      "97.46000000000001 % success on test data\n",
      "--- (47) ---\n",
      "Training started at 1646150716.1277843\n",
      "Epoch 0: 69.08150482177734: 0.1186\n",
      "Epoch 5000: 45.676239013671875: 0.9648\n",
      "Training finished at 1646150753.7291932; lasted 37.60140895843506 seconds.\n",
      "96.48 % success on test data\n",
      "--- (48) ---\n",
      "Training started at 1646150754.7741323\n",
      "Epoch 0: 69.1305923461914: 0.0958\n",
      "Epoch 5000: 44.83749771118164: 0.9709\n",
      "Training finished at 1646150792.2987332; lasted 37.524600982666016 seconds.\n",
      "97.09 % success on test data\n",
      "--- (49) ---\n",
      "Training started at 1646150793.3885999\n",
      "Epoch 0: 69.04891204833984: 0.0905\n",
      "Epoch 5000: 43.83881759643555: 0.9663\n",
      "Training finished at 1646150831.0114362; lasted 37.62283635139465 seconds.\n",
      "96.63000000000001 % success on test data\n",
      "--- (50) ---\n",
      "Training started at 1646150832.0679054\n",
      "Epoch 0: 68.96260833740234: 0.096\n",
      "Epoch 5000: 45.671905517578125: 0.9453\n",
      "Training finished at 1646150869.5716825; lasted 37.50377702713013 seconds.\n",
      "94.53 % success on test data\n",
      "--- (51) ---\n",
      "Training started at 1646150870.6684854\n",
      "Epoch 0: 68.98297882080078: 0.1032\n",
      "Epoch 5000: 45.77971267700195: 0.9658\n",
      "Training finished at 1646150908.3769941; lasted 37.70850872993469 seconds.\n",
      "96.58 % success on test data\n",
      "--- (52) ---\n",
      "Training started at 1646150909.4117324\n",
      "Epoch 0: 69.06068420410156: 0.0927\n",
      "Epoch 5000: 44.834564208984375: 0.9715\n",
      "Training finished at 1646150946.944947; lasted 37.5332145690918 seconds.\n",
      "97.15 % success on test data\n",
      "--- (53) ---\n",
      "Training started at 1646150948.0088725\n",
      "Epoch 0: 69.08635711669922: 0.098\n",
      "Epoch 5000: 45.338748931884766: 0.9639\n",
      "Training finished at 1646150985.2366564; lasted 37.22778391838074 seconds.\n",
      "96.39 % success on test data\n",
      "--- (54) ---\n",
      "Training started at 1646150986.2744243\n",
      "Epoch 0: 69.0930404663086: 0.1556\n",
      "Epoch 5000: 44.23740768432617: 0.9671\n",
      "Training finished at 1646151024.0577538; lasted 37.783329486846924 seconds.\n",
      "96.71 % success on test data\n",
      "--- (55) ---\n",
      "Training started at 1646151025.186795\n",
      "Epoch 0: 69.04517364501953: 0.1158\n",
      "Epoch 5000: 45.80381774902344: 0.9663\n",
      "Training finished at 1646151062.578706; lasted 37.391911029815674 seconds.\n",
      "96.63000000000001 % success on test data\n",
      "--- (56) ---\n",
      "Training started at 1646151063.6278226\n",
      "Epoch 0: 69.06153106689453: 0.1037\n",
      "Epoch 5000: 45.71705627441406: 0.9617\n",
      "Training finished at 1646151101.0862048; lasted 37.45838212966919 seconds.\n",
      "96.17 % success on test data\n",
      "--- (57) ---\n",
      "Training started at 1646151102.1280704\n",
      "Epoch 0: 68.99913024902344: 0.1137\n",
      "Epoch 5000: 47.027286529541016: 0.9657\n",
      "Training finished at 1646151139.496948; lasted 37.36887764930725 seconds.\n",
      "96.57 % success on test data\n",
      "--- (58) ---\n",
      "Training started at 1646151140.5603535\n",
      "Epoch 0: 69.03921508789062: 0.0892\n",
      "Epoch 5000: 48.192405700683594: 0.8732\n",
      "Training finished at 1646151178.0861804; lasted 37.525826930999756 seconds.\n",
      "87.32 % success on test data\n",
      "--- (59) ---\n",
      "Training started at 1646151179.1642008\n",
      "Epoch 0: 69.04518127441406: 0.0892\n",
      "Epoch 5000: 46.39077377319336: 0.9535\n",
      "Training finished at 1646151216.7236423; lasted 37.559441566467285 seconds.\n",
      "95.35 % success on test data\n",
      "--- (60) ---\n",
      "Training started at 1646151217.7858627\n",
      "Epoch 0: 69.22362518310547: 0.0892\n",
      "Epoch 5000: 43.83450698852539: 0.968\n",
      "Training finished at 1646151255.3086753; lasted 37.522812604904175 seconds.\n",
      "96.8 % success on test data\n",
      "--- (61) ---\n",
      "Training started at 1646151256.3610377\n",
      "Epoch 0: 69.0401611328125: 0.098\n",
      "Epoch 5000: 44.860862731933594: 0.9651\n",
      "Training finished at 1646151293.5889752; lasted 37.22793745994568 seconds.\n",
      "96.50999999999999 % success on test data\n",
      "--- (62) ---\n",
      "Training started at 1646151294.6901548\n",
      "Epoch 0: 68.98597717285156: 0.1706\n",
      "Epoch 5000: 44.55893325805664: 0.965\n",
      "Training finished at 1646151331.963465; lasted 37.27331018447876 seconds.\n",
      "96.5 % success on test data\n",
      "--- (63) ---\n",
      "Training started at 1646151333.0102174\n",
      "Epoch 0: 69.20573425292969: 0.1032\n",
      "Epoch 5000: 46.335845947265625: 0.964\n",
      "Training finished at 1646151370.397551; lasted 37.3873336315155 seconds.\n",
      "96.39999999999999 % success on test data\n",
      "--- (64) ---\n",
      "Training started at 1646151371.4516823\n",
      "Epoch 0: 69.31922912597656: 0.0893\n",
      "Epoch 5000: 44.83449935913086: 0.9615\n",
      "Training finished at 1646151409.0066888; lasted 37.55500650405884 seconds.\n",
      "96.15 % success on test data\n",
      "--- (65) ---\n",
      "Training started at 1646151410.054792\n",
      "Epoch 0: 69.0738296508789: 0.1227\n",
      "Epoch 5000: 43.8357048034668: 0.9657\n",
      "Training finished at 1646151447.6140218; lasted 37.55922985076904 seconds.\n",
      "96.57 % success on test data\n",
      "--- (66) ---\n",
      "Training started at 1646151448.6510167\n",
      "Epoch 0: 69.20045471191406: 0.1092\n",
      "Epoch 5000: 44.85179138183594: 0.9526\n",
      "Training finished at 1646151486.3359675; lasted 37.684950828552246 seconds.\n",
      "95.26 % success on test data\n",
      "--- (67) ---\n",
      "Training started at 1646151487.3940234\n",
      "Epoch 0: 69.05858612060547: 0.1009\n",
      "Epoch 5000: 43.834503173828125: 0.9598\n",
      "Training finished at 1646151525.2864287; lasted 37.89240527153015 seconds.\n",
      "95.98 % success on test data\n",
      "--- (68) ---\n",
      "Training started at 1646151526.3603725\n",
      "Epoch 0: 69.13511657714844: 0.1009\n",
      "Epoch 5000: 43.861141204833984: 0.9645\n",
      "Training finished at 1646151564.1184297; lasted 37.75805711746216 seconds.\n",
      "96.45 % success on test data\n",
      "--- (69) ---\n",
      "Training started at 1646151565.173515\n",
      "Epoch 0: 68.98872375488281: 0.098\n",
      "Epoch 5000: 43.83515167236328: 0.9659\n",
      "Training finished at 1646151602.6167293; lasted 37.44321417808533 seconds.\n",
      "96.59 % success on test data\n",
      "--- (70) ---\n",
      "Training started at 1646151603.6541555\n",
      "Epoch 0: 69.06835174560547: 0.1078\n",
      "Epoch 5000: 45.85935974121094: 0.9684\n",
      "Training finished at 1646151641.1402361; lasted 37.48608064651489 seconds.\n",
      "96.84 % success on test data\n",
      "--- (71) ---\n",
      "Training started at 1646151642.1969588\n",
      "Epoch 0: 68.98390197753906: 0.0875\n",
      "Epoch 5000: 44.35474395751953: 0.9608\n",
      "Training finished at 1646151679.8792202; lasted 37.68226146697998 seconds.\n",
      "96.08 % success on test data\n",
      "--- (72) ---\n",
      "Training started at 1646151680.9692318\n",
      "Epoch 0: 69.14185333251953: 0.1136\n",
      "Epoch 5000: 44.834510803222656: 0.9715\n",
      "Training finished at 1646151718.2875154; lasted 37.318283557891846 seconds.\n",
      "97.15 % success on test data\n",
      "--- (73) ---\n",
      "Training started at 1646151719.324491\n",
      "Epoch 0: 69.1037826538086: 0.0892\n",
      "Epoch 5000: 44.658485412597656: 0.9589\n",
      "Training finished at 1646151757.644009; lasted 38.319518089294434 seconds.\n",
      "95.89 % success on test data\n",
      "--- (74) ---\n",
      "Training started at 1646151758.7277317\n",
      "Epoch 0: 69.03629302978516: 0.0982\n",
      "Epoch 5000: 45.83500671386719: 0.969\n",
      "Training finished at 1646151796.4781747; lasted 37.75044298171997 seconds.\n",
      "96.89999999999999 % success on test data\n",
      "--- (75) ---\n",
      "Training started at 1646151797.546366\n",
      "Epoch 0: 69.16935729980469: 0.0892\n",
      "Epoch 5000: 49.52766418457031: 0.8694\n",
      "Training finished at 1646151835.2222736; lasted 37.675907611846924 seconds.\n",
      "86.94 % success on test data\n",
      "--- (76) ---\n",
      "Training started at 1646151836.2302866\n",
      "Epoch 0: 69.02127838134766: 0.1135\n",
      "Epoch 5000: 43.857757568359375: 0.9651\n",
      "Training finished at 1646151873.6673512; lasted 37.43706464767456 seconds.\n",
      "96.50999999999999 % success on test data\n",
      "--- (77) ---\n",
      "Training started at 1646151874.7146702\n",
      "Epoch 0: 69.09418487548828: 0.1009\n",
      "Epoch 5000: 43.83655548095703: 0.9634\n",
      "Training finished at 1646151912.660443; lasted 37.945772886276245 seconds.\n",
      "96.34 % success on test data\n",
      "--- (78) ---\n",
      "Training started at 1646151913.7034664\n",
      "Epoch 0: 69.10820007324219: 0.1168\n",
      "Epoch 5000: 46.83303451538086: 0.9659\n",
      "Training finished at 1646151952.1178057; lasted 38.41433930397034 seconds.\n",
      "96.59 % success on test data\n",
      "--- (79) ---\n",
      "Training started at 1646151953.1737778\n",
      "Epoch 0: 69.13645935058594: 0.0974\n",
      "Epoch 5000: 44.83660888671875: 0.9669\n",
      "Training finished at 1646151991.0190651; lasted 37.84528732299805 seconds.\n",
      "96.69 % success on test data\n",
      "--- (80) ---\n",
      "Training started at 1646151992.0910113\n",
      "Epoch 0: 69.16477966308594: 0.0856\n",
      "Epoch 5000: 46.20627975463867: 0.9661\n",
      "Training finished at 1646152029.5572329; lasted 37.46622157096863 seconds.\n",
      "96.61 % success on test data\n",
      "--- (81) ---\n",
      "Training started at 1646152030.6553571\n",
      "Epoch 0: 69.1364517211914: 0.0974\n",
      "Epoch 5000: 44.869476318359375: 0.9638\n",
      "Training finished at 1646152068.8742247; lasted 38.2188675403595 seconds.\n",
      "96.38 % success on test data\n",
      "--- (82) ---\n",
      "Training started at 1646152069.8981214\n",
      "Epoch 0: 69.11920928955078: 0.1082\n",
      "Epoch 5000: 44.840904235839844: 0.8737\n",
      "Training finished at 1646152108.0407827; lasted 38.142661333084106 seconds.\n",
      "87.37 % success on test data\n",
      "--- (83) ---\n",
      "Training started at 1646152109.1437347\n",
      "Epoch 0: 69.08106994628906: 0.1135\n",
      "Epoch 5000: 46.68687057495117: 0.9679\n",
      "Training finished at 1646152146.7353146; lasted 37.59157991409302 seconds.\n",
      "96.78999999999999 % success on test data\n",
      "--- (84) ---\n",
      "Training started at 1646152147.79653\n",
      "Epoch 0: 69.1410140991211: 0.1021\n",
      "Epoch 5000: 46.8089599609375: 0.8763\n",
      "Training finished at 1646152185.3197901; lasted 37.52326011657715 seconds.\n",
      "87.63 % success on test data\n",
      "--- (85) ---\n",
      "Training started at 1646152186.3318586\n",
      "Epoch 0: 69.09745025634766: 0.1168\n",
      "Epoch 5000: 44.994911193847656: 0.9688\n",
      "Training finished at 1646152224.4037116; lasted 38.071852922439575 seconds.\n",
      "96.88 % success on test data\n",
      "--- (86) ---\n",
      "Training started at 1646152225.473348\n",
      "Epoch 0: 69.1109848022461: 0.1323\n",
      "Epoch 5000: 44.837162017822266: 0.9699\n",
      "Training finished at 1646152263.1948538; lasted 37.721505880355835 seconds.\n",
      "96.99 % success on test data\n",
      "--- (87) ---\n",
      "Training started at 1646152264.2479053\n",
      "Epoch 0: 68.910400390625: 0.1048\n",
      "Epoch 5000: 46.23857879638672: 0.8699\n",
      "Training finished at 1646152302.060446; lasted 37.812540769577026 seconds.\n",
      "86.99 % success on test data\n",
      "--- (88) ---\n",
      "Training started at 1646152303.0954885\n",
      "Epoch 0: 69.16529846191406: 0.0962\n",
      "Epoch 5000: 44.82013702392578: 0.963\n",
      "Training finished at 1646152340.752353; lasted 37.656864404678345 seconds.\n",
      "96.3 % success on test data\n",
      "--- (89) ---\n",
      "Training started at 1646152341.822027\n",
      "Epoch 0: 69.22166442871094: 0.0981\n",
      "Epoch 5000: 43.840450286865234: 0.9669\n",
      "Training finished at 1646152379.799757; lasted 37.97773003578186 seconds.\n",
      "96.69 % success on test data\n"
     ]
    }
   ],
   "source": [
    "for i in range(90):\n",
    "    print(f'--- ({i}) ---')\n",
    "    model = ModelManager.get_untrained(ModelType.MnistCnnOto)\n",
    "    Coach.train(\n",
    "        model,\n",
    "        data,\n",
    "        nn.CrossEntropyLoss(reduction='sum'),\n",
    "        optim.SGD(model.parameters(), lr=1e-2),\n",
    "        30,\n",
    "        5001,\n",
    "        5000\n",
    "    )\n",
    "    simple.append(Coach.measure_performance(model, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- (0) ---\n",
      "Training started at 1646152498.4693916\n",
      "Epoch 0: 69.09528350830078: 0.1028\n",
      "Epoch 5000: 50.49538040161133: 0.7741\n",
      "Training finished at 1646152539.1223001; lasted 40.65290856361389 seconds.\n",
      "77.41 % success on test data\n",
      "--- (1) ---\n",
      "Training started at 1646152540.210963\n",
      "Epoch 0: 69.16072082519531: 0.1801\n",
      "Epoch 5000: 45.585594177246094: 0.9412\n",
      "Training finished at 1646152583.3941262; lasted 43.18316316604614 seconds.\n",
      "94.12 % success on test data\n",
      "--- (2) ---\n",
      "Training started at 1646152584.459887\n",
      "Epoch 0: 69.046875: 0.1054\n",
      "Epoch 5000: 46.95692825317383: 0.8695\n",
      "Training finished at 1646152627.0693274; lasted 42.609440326690674 seconds.\n",
      "86.95 % success on test data\n",
      "--- (3) ---\n",
      "Training started at 1646152628.1036007\n",
      "Epoch 0: 69.03410339355469: 0.0996\n",
      "Epoch 5000: 52.681880950927734: 0.7732\n",
      "Training finished at 1646152670.1947443; lasted 42.09114360809326 seconds.\n",
      "77.32 % success on test data\n",
      "--- (4) ---\n",
      "Training started at 1646152671.3206873\n",
      "Epoch 0: 69.11729431152344: 0.2176\n",
      "Epoch 5000: 50.013755798339844: 0.7907\n",
      "Training finished at 1646152713.504543; lasted 42.18385577201843 seconds.\n",
      "79.07 % success on test data\n",
      "--- (5) ---\n",
      "Training started at 1646152714.5169349\n",
      "Epoch 0: 69.08016967773438: 0.2333\n",
      "Epoch 5000: 48.82124328613281: 0.7776\n",
      "Training finished at 1646152756.9087634; lasted 42.391828536987305 seconds.\n",
      "77.75999999999999 % success on test data\n",
      "--- (6) ---\n",
      "Training started at 1646152757.964153\n",
      "Epoch 0: 69.1603012084961: 0.1063\n",
      "Epoch 5000: 48.010597229003906: 0.7827\n",
      "Training finished at 1646152799.7972925; lasted 41.833139419555664 seconds.\n",
      "78.27 % success on test data\n",
      "--- (7) ---\n",
      "Training started at 1646152800.8326416\n",
      "Epoch 0: 69.05795288085938: 0.0982\n",
      "Epoch 5000: 51.60057830810547: 0.7718\n",
      "Training finished at 1646152842.1488326; lasted 41.31619095802307 seconds.\n",
      "77.18 % success on test data\n",
      "--- (8) ---\n",
      "Training started at 1646152843.2130864\n",
      "Epoch 0: 69.05972290039062: 0.1264\n",
      "Epoch 5000: 53.89640426635742: 0.7639\n",
      "Training finished at 1646152884.9847732; lasted 41.77168679237366 seconds.\n",
      "76.39 % success on test data\n",
      "--- (9) ---\n",
      "Training started at 1646152886.1184573\n",
      "Epoch 0: 69.10977935791016: 0.1683\n",
      "Epoch 5000: 52.70146560668945: 0.8716\n",
      "Training finished at 1646152928.2658007; lasted 42.1473433971405 seconds.\n",
      "87.16000000000001 % success on test data\n",
      "--- (10) ---\n",
      "Training started at 1646152929.3120813\n",
      "Epoch 0: 68.9788589477539: 0.1108\n",
      "Epoch 5000: 46.71891784667969: 0.8445\n",
      "Training finished at 1646152971.4019265; lasted 42.089845180511475 seconds.\n",
      "84.45 % success on test data\n",
      "--- (11) ---\n",
      "Training started at 1646152972.4912772\n",
      "Epoch 0: 69.00260162353516: 0.1036\n",
      "Epoch 5000: 51.856258392333984: 0.782\n",
      "Training finished at 1646153013.897404; lasted 41.406126737594604 seconds.\n",
      "78.2 % success on test data\n",
      "--- (12) ---\n",
      "Training started at 1646153014.9767177\n",
      "Epoch 0: 69.1340560913086: 0.0958\n",
      "Epoch 5000: 47.83845901489258: 0.9484\n",
      "Training finished at 1646153057.1479063; lasted 42.17118859291077 seconds.\n",
      "94.84 % success on test data\n",
      "--- (13) ---\n",
      "Training started at 1646153058.2262156\n",
      "Epoch 0: 69.01301574707031: 0.101\n",
      "Epoch 5000: 51.54862976074219: 0.7779\n",
      "Training finished at 1646153099.724398; lasted 41.49818229675293 seconds.\n",
      "77.79 % success on test data\n",
      "--- (14) ---\n",
      "Training started at 1646153100.7766924\n",
      "Epoch 0: 69.15193176269531: 0.098\n",
      "Epoch 5000: 43.91624069213867: 0.9562\n",
      "Training finished at 1646153142.1008742; lasted 41.32418179512024 seconds.\n",
      "95.62 % success on test data\n",
      "--- (15) ---\n",
      "Training started at 1646153143.1585045\n",
      "Epoch 0: 68.94776916503906: 0.1028\n",
      "Epoch 5000: 45.79400634765625: 0.8707\n",
      "Training finished at 1646153185.3255801; lasted 42.167075634002686 seconds.\n",
      "87.07000000000001 % success on test data\n",
      "--- (16) ---\n",
      "Training started at 1646153186.4303713\n",
      "Epoch 0: 68.98877716064453: 0.1135\n",
      "Epoch 5000: 46.534461975097656: 0.8747\n",
      "Training finished at 1646153228.1614516; lasted 41.731080293655396 seconds.\n",
      "87.47 % success on test data\n",
      "--- (17) ---\n",
      "Training started at 1646153229.234178\n",
      "Epoch 0: 69.02301025390625: 0.1036\n",
      "Epoch 5000: 50.92897415161133: 0.8532\n",
      "Training finished at 1646153271.2703354; lasted 42.03615736961365 seconds.\n",
      "85.32 % success on test data\n",
      "--- (18) ---\n",
      "Training started at 1646153272.3694005\n",
      "Epoch 0: 69.1321029663086: 0.1135\n",
      "Epoch 5000: 46.86935043334961: 0.8683\n",
      "Training finished at 1646153313.9119344; lasted 41.54253387451172 seconds.\n",
      "86.83 % success on test data\n",
      "--- (19) ---\n",
      "Training started at 1646153314.9619336\n",
      "Epoch 0: 69.09614562988281: 0.2375\n",
      "Epoch 5000: 52.35289001464844: 0.8576\n",
      "Training finished at 1646153356.9671018; lasted 42.005168199539185 seconds.\n",
      "85.76 % success on test data\n",
      "--- (20) ---\n",
      "Training started at 1646153358.037084\n",
      "Epoch 0: 69.08611297607422: 0.0858\n",
      "Epoch 5000: 46.284732818603516: 0.8765\n",
      "Training finished at 1646153399.8735802; lasted 41.836496114730835 seconds.\n",
      "87.64999999999999 % success on test data\n",
      "--- (21) ---\n",
      "Training started at 1646153400.9345624\n",
      "Epoch 0: 69.00174713134766: 0.0959\n",
      "Epoch 5000: 57.87918472290039: 0.7717\n",
      "Training finished at 1646153443.194464; lasted 42.25990152359009 seconds.\n",
      "77.17 % success on test data\n",
      "--- (22) ---\n",
      "Training started at 1646153444.2400596\n",
      "Epoch 0: 69.08979797363281: 0.1211\n",
      "Epoch 5000: 50.225868225097656: 0.8633\n",
      "Training finished at 1646153486.2818642; lasted 42.04180455207825 seconds.\n",
      "86.33 % success on test data\n",
      "--- (23) ---\n",
      "Training started at 1646153487.3284724\n",
      "Epoch 0: 69.08907318115234: 0.0958\n",
      "Epoch 5000: 47.041412353515625: 0.866\n",
      "Training finished at 1646153528.8810556; lasted 41.55258321762085 seconds.\n",
      "86.6 % success on test data\n",
      "--- (24) ---\n",
      "Training started at 1646153529.957224\n",
      "Epoch 0: 68.99805450439453: 0.1695\n",
      "Epoch 5000: 47.407623291015625: 0.874\n",
      "Training finished at 1646153572.1152792; lasted 42.15805530548096 seconds.\n",
      "87.4 % success on test data\n",
      "--- (25) ---\n",
      "Training started at 1646153573.1705718\n",
      "Epoch 0: 69.0260238647461: 0.0974\n",
      "Epoch 5000: 54.646095275878906: 0.7812\n",
      "Training finished at 1646153615.644083; lasted 42.47351121902466 seconds.\n",
      "78.12 % success on test data\n",
      "--- (26) ---\n",
      "Training started at 1646153616.6993363\n",
      "Epoch 0: 69.06001281738281: 0.1028\n",
      "Epoch 5000: 44.60121536254883: 0.8661\n",
      "Training finished at 1646153658.8073394; lasted 42.10800313949585 seconds.\n",
      "86.61 % success on test data\n",
      "--- (27) ---\n",
      "Training started at 1646153659.8440826\n",
      "Epoch 0: 68.98660278320312: 0.2052\n",
      "Epoch 5000: 50.378658294677734: 0.7465\n",
      "Training finished at 1646153702.0780523; lasted 42.23396968841553 seconds.\n",
      "74.65 % success on test data\n",
      "--- (28) ---\n",
      "Training started at 1646153703.1126602\n",
      "Epoch 0: 69.22357177734375: 0.098\n",
      "Epoch 5000: 45.83699035644531: 0.9571\n",
      "Training finished at 1646153744.7741985; lasted 41.66153836250305 seconds.\n",
      "95.71 % success on test data\n",
      "--- (29) ---\n",
      "Training started at 1646153745.8371913\n",
      "Epoch 0: 69.1760482788086: 0.1032\n",
      "Epoch 5000: 55.79475402832031: 0.7833\n",
      "Training finished at 1646153787.2546625; lasted 41.417471170425415 seconds.\n",
      "78.33 % success on test data\n",
      "--- (30) ---\n",
      "Training started at 1646153788.2827082\n",
      "Epoch 0: 69.06716918945312: 0.1605\n",
      "Epoch 5000: 44.195098876953125: 0.9548\n",
      "Training finished at 1646153830.4773464; lasted 42.1946382522583 seconds.\n",
      "95.48 % success on test data\n",
      "--- (31) ---\n",
      "Training started at 1646153831.5651894\n",
      "Epoch 0: 69.09107971191406: 0.1658\n",
      "Epoch 5000: 48.99553680419922: 0.7798\n",
      "Training finished at 1646153873.5750358; lasted 42.009846448898315 seconds.\n",
      "77.98 % success on test data\n",
      "--- (32) ---\n",
      "Training started at 1646153874.6459384\n",
      "Epoch 0: 69.06085205078125: 0.107\n",
      "Epoch 5000: 49.79236602783203: 0.8559\n",
      "Training finished at 1646153916.8782828; lasted 42.23234438896179 seconds.\n",
      "85.59 % success on test data\n",
      "--- (33) ---\n",
      "Training started at 1646153917.9504483\n",
      "Epoch 0: 69.09622192382812: 0.1032\n",
      "Epoch 5000: 43.87396240234375: 0.9602\n",
      "Training finished at 1646153960.1389813; lasted 42.18853306770325 seconds.\n",
      "96.02000000000001 % success on test data\n",
      "--- (34) ---\n",
      "Training started at 1646153961.2086341\n",
      "Epoch 0: 68.98489379882812: 0.1148\n",
      "Epoch 5000: 44.927223205566406: 0.9674\n",
      "Training finished at 1646154003.170542; lasted 41.96190786361694 seconds.\n",
      "96.74000000000001 % success on test data\n",
      "--- (35) ---\n",
      "Training started at 1646154004.2251294\n",
      "Epoch 0: 69.13338470458984: 0.0982\n",
      "Epoch 5000: 54.75034713745117: 0.7708\n",
      "Training finished at 1646154046.0632718; lasted 41.83814239501953 seconds.\n",
      "77.08 % success on test data\n",
      "--- (36) ---\n",
      "Training started at 1646154047.1150208\n",
      "Epoch 0: 69.1231460571289: 0.1028\n",
      "Epoch 5000: 47.38336181640625: 0.8705\n",
      "Training finished at 1646154089.0508323; lasted 41.9358115196228 seconds.\n",
      "87.05000000000001 % success on test data\n",
      "--- (37) ---\n",
      "Training started at 1646154090.0857463\n",
      "Epoch 0: 69.0479507446289: 0.0982\n",
      "Epoch 5000: 50.788795471191406: 0.7874\n",
      "Training finished at 1646154131.7135837; lasted 41.62783741950989 seconds.\n",
      "78.74 % success on test data\n",
      "--- (38) ---\n",
      "Training started at 1646154132.74465\n",
      "Epoch 0: 69.05711364746094: 0.0852\n",
      "Epoch 5000: 43.850284576416016: 0.9601\n",
      "Training finished at 1646154174.7942436; lasted 42.049593687057495 seconds.\n",
      "96.00999999999999 % success on test data\n",
      "--- (39) ---\n",
      "Training started at 1646154175.8532555\n",
      "Epoch 0: 68.99679565429688: 0.098\n",
      "Epoch 5000: 49.659175872802734: 0.7811\n",
      "Training finished at 1646154219.6046474; lasted 43.751391887664795 seconds.\n",
      "78.11 % success on test data\n",
      "--- (40) ---\n",
      "Training started at 1646154220.7711713\n",
      "Epoch 0: 69.08760070800781: 0.101\n",
      "Epoch 5000: 45.035491943359375: 0.9496\n",
      "Training finished at 1646154263.7339146; lasted 42.962743282318115 seconds.\n",
      "94.96 % success on test data\n",
      "--- (41) ---\n",
      "Training started at 1646154264.7768443\n",
      "Epoch 0: 69.11131286621094: 0.1328\n",
      "Epoch 5000: 49.79773712158203: 0.772\n",
      "Training finished at 1646154306.5305529; lasted 41.753708600997925 seconds.\n",
      "77.2 % success on test data\n",
      "--- (42) ---\n",
      "Training started at 1646154307.6025307\n",
      "Epoch 0: 69.13150787353516: 0.102\n",
      "Epoch 5000: 45.15042495727539: 0.8589\n",
      "Training finished at 1646154349.2234418; lasted 41.62091112136841 seconds.\n",
      "85.89 % success on test data\n",
      "--- (43) ---\n",
      "Training started at 1646154350.279034\n",
      "Epoch 0: 69.24484252929688: 0.2057\n",
      "Epoch 5000: 53.62705993652344: 0.7631\n",
      "Training finished at 1646154392.106882; lasted 41.82784819602966 seconds.\n",
      "76.31 % success on test data\n",
      "--- (44) ---\n",
      "Training started at 1646154393.2056637\n",
      "Epoch 0: 69.14998626708984: 0.1203\n",
      "Epoch 5000: 54.72540283203125: 0.7811\n",
      "Training finished at 1646154435.5402298; lasted 42.33456611633301 seconds.\n",
      "78.11 % success on test data\n",
      "--- (45) ---\n",
      "Training started at 1646154436.6168504\n",
      "Epoch 0: 69.07172393798828: 0.1009\n",
      "Epoch 5000: 46.40220260620117: 0.8649\n",
      "Training finished at 1646154478.264388; lasted 41.64753770828247 seconds.\n",
      "86.49 % success on test data\n",
      "--- (46) ---\n",
      "Training started at 1646154479.365227\n",
      "Epoch 0: 69.05824279785156: 0.1028\n",
      "Epoch 5000: 51.63347244262695: 0.8654\n",
      "Training finished at 1646154521.8222542; lasted 42.457027196884155 seconds.\n",
      "86.53999999999999 % success on test data\n",
      "--- (47) ---\n",
      "Training started at 1646154522.854197\n",
      "Epoch 0: 68.99478149414062: 0.1148\n",
      "Epoch 5000: 51.5516471862793: 0.7869\n",
      "Training finished at 1646154564.7944129; lasted 41.940215826034546 seconds.\n",
      "78.69 % success on test data\n",
      "--- (48) ---\n",
      "Training started at 1646154565.8241978\n",
      "Epoch 0: 69.05833435058594: 0.0939\n",
      "Epoch 5000: 51.684906005859375: 0.777\n",
      "Training finished at 1646154607.7475674; lasted 41.92336964607239 seconds.\n",
      "77.7 % success on test data\n",
      "--- (49) ---\n",
      "Training started at 1646154608.7753434\n",
      "Epoch 0: 69.14460754394531: 0.1176\n",
      "Epoch 5000: 48.818695068359375: 0.8668\n",
      "Training finished at 1646154650.9046319; lasted 42.1292884349823 seconds.\n",
      "86.68 % success on test data\n",
      "--- (50) ---\n",
      "Training started at 1646154651.9632223\n",
      "Epoch 0: 69.09782409667969: 0.136\n",
      "Epoch 5000: 49.75206756591797: 0.7787\n",
      "Training finished at 1646154694.0282762; lasted 42.065053939819336 seconds.\n",
      "77.86999999999999 % success on test data\n",
      "--- (51) ---\n",
      "Training started at 1646154695.0590322\n",
      "Epoch 0: 69.08978271484375: 0.0974\n",
      "Epoch 5000: 48.003353118896484: 0.8563\n",
      "Training finished at 1646154737.6471605; lasted 42.588128328323364 seconds.\n",
      "85.63 % success on test data\n",
      "--- (52) ---\n",
      "Training started at 1646154738.6838782\n",
      "Epoch 0: 69.20889282226562: 0.0958\n",
      "Epoch 5000: 48.73347854614258: 0.861\n",
      "Training finished at 1646154780.5901895; lasted 41.90631127357483 seconds.\n",
      "86.1 % success on test data\n",
      "--- (53) ---\n",
      "Training started at 1646154781.637971\n",
      "Epoch 0: 68.98847961425781: 0.1243\n",
      "Epoch 5000: 48.82128143310547: 0.874\n",
      "Training finished at 1646154823.4852865; lasted 41.847315549850464 seconds.\n",
      "87.4 % success on test data\n",
      "--- (54) ---\n",
      "Training started at 1646154824.5345123\n",
      "Epoch 0: 69.01915740966797: 0.0892\n",
      "Epoch 5000: 53.12300491333008: 0.7684\n",
      "Training finished at 1646154866.21431; lasted 41.679797649383545 seconds.\n",
      "76.84 % success on test data\n",
      "--- (55) ---\n",
      "Training started at 1646154867.2511902\n",
      "Epoch 0: 69.1847152709961: 0.0785\n",
      "Epoch 5000: 51.85682678222656: 0.7753\n",
      "Training finished at 1646154909.7822056; lasted 42.531015396118164 seconds.\n",
      "77.53 % success on test data\n",
      "--- (56) ---\n",
      "Training started at 1646154910.863808\n",
      "Epoch 0: 69.02027130126953: 0.1612\n",
      "Epoch 5000: 49.61178970336914: 0.8687\n",
      "Training finished at 1646154953.0156245; lasted 42.151816606521606 seconds.\n",
      "86.87 % success on test data\n",
      "--- (57) ---\n",
      "Training started at 1646154954.07252\n",
      "Epoch 0: 69.08785247802734: 0.1009\n",
      "Epoch 5000: 48.75389099121094: 0.7597\n",
      "Training finished at 1646154996.224347; lasted 42.15182709693909 seconds.\n",
      "75.97 % success on test data\n",
      "--- (58) ---\n",
      "Training started at 1646154997.2766232\n",
      "Epoch 0: 68.98387145996094: 0.1028\n",
      "Epoch 5000: 52.57987976074219: 0.7776\n",
      "Training finished at 1646155039.481291; lasted 42.204667806625366 seconds.\n",
      "77.75999999999999 % success on test data\n",
      "--- (59) ---\n",
      "Training started at 1646155040.5383744\n",
      "Epoch 0: 69.08488464355469: 0.1365\n",
      "Epoch 5000: 48.05309295654297: 0.8576\n",
      "Training finished at 1646155083.1004179; lasted 42.56204342842102 seconds.\n",
      "85.76 % success on test data\n",
      "--- (60) ---\n",
      "Training started at 1646155084.1796975\n",
      "Epoch 0: 69.04153442382812: 0.1513\n",
      "Epoch 5000: 54.90355682373047: 0.7749\n",
      "Training finished at 1646155126.2452343; lasted 42.06553673744202 seconds.\n",
      "77.49000000000001 % success on test data\n",
      "--- (61) ---\n",
      "Training started at 1646155127.299174\n",
      "Epoch 0: 69.04005432128906: 0.1009\n",
      "Epoch 5000: 46.25642395019531: 0.8798\n",
      "Training finished at 1646155169.6385224; lasted 42.33934831619263 seconds.\n",
      "87.98 % success on test data\n",
      "--- (62) ---\n",
      "Training started at 1646155170.6846898\n",
      "Epoch 0: 69.02467346191406: 0.0958\n",
      "Epoch 5000: 45.03288269042969: 0.9661\n",
      "Training finished at 1646155212.938163; lasted 42.25347328186035 seconds.\n",
      "96.61 % success on test data\n",
      "--- (63) ---\n",
      "Training started at 1646155214.014122\n",
      "Epoch 0: 69.07630157470703: 0.0958\n",
      "Epoch 5000: 48.88410186767578: 0.7576\n",
      "Training finished at 1646155256.3348846; lasted 42.320762634277344 seconds.\n",
      "75.76 % success on test data\n",
      "--- (64) ---\n",
      "Training started at 1646155257.4317029\n",
      "Epoch 0: 69.02027130126953: 0.1135\n",
      "Epoch 5000: 43.857913970947266: 0.957\n",
      "Training finished at 1646155298.9288852; lasted 41.49718236923218 seconds.\n",
      "95.7 % success on test data\n",
      "--- (65) ---\n",
      "Training started at 1646155300.0004582\n",
      "Epoch 0: 69.16057586669922: 0.1034\n",
      "Epoch 5000: 47.48455047607422: 0.8653\n",
      "Training finished at 1646155342.0964615; lasted 42.09600329399109 seconds.\n",
      "86.53 % success on test data\n",
      "--- (66) ---\n",
      "Training started at 1646155343.1821117\n",
      "Epoch 0: 69.0243911743164: 0.1135\n",
      "Epoch 5000: 45.40209197998047: 0.964\n",
      "Training finished at 1646155384.657897; lasted 41.47578525543213 seconds.\n",
      "96.39999999999999 % success on test data\n",
      "--- (67) ---\n",
      "Training started at 1646155385.7051325\n",
      "Epoch 0: 69.05091857910156: 0.0727\n",
      "Epoch 5000: 48.91785430908203: 0.8647\n",
      "Training finished at 1646155427.6535614; lasted 41.94842886924744 seconds.\n",
      "86.47 % success on test data\n",
      "--- (68) ---\n",
      "Training started at 1646155428.7284753\n",
      "Epoch 0: 69.0263900756836: 0.1008\n",
      "Epoch 5000: 50.5097541809082: 0.8751\n",
      "Training finished at 1646155470.5931783; lasted 41.86470293998718 seconds.\n",
      "87.51 % success on test data\n",
      "--- (69) ---\n",
      "Training started at 1646155471.6233685\n",
      "Epoch 0: 69.24575805664062: 0.108\n",
      "Epoch 5000: 49.179561614990234: 0.8839\n",
      "Training finished at 1646155513.9348445; lasted 42.31147599220276 seconds.\n",
      "88.39 % success on test data\n",
      "--- (70) ---\n",
      "Training started at 1646155515.0144665\n",
      "Epoch 0: 69.11465454101562: 0.1138\n",
      "Epoch 5000: 47.9365119934082: 0.8806\n",
      "Training finished at 1646155556.5371726; lasted 41.522706031799316 seconds.\n",
      "88.06 % success on test data\n",
      "--- (71) ---\n",
      "Training started at 1646155557.5671856\n",
      "Epoch 0: 69.1432876586914: 0.098\n",
      "Epoch 5000: 47.31653594970703: 0.9512\n",
      "Training finished at 1646155599.6444466; lasted 42.077260971069336 seconds.\n",
      "95.12 % success on test data\n",
      "--- (72) ---\n",
      "Training started at 1646155600.6803262\n",
      "Epoch 0: 69.07513427734375: 0.0996\n",
      "Epoch 5000: 44.881553649902344: 0.8712\n",
      "Training finished at 1646155642.9255092; lasted 42.24518299102783 seconds.\n",
      "87.12 % success on test data\n",
      "--- (73) ---\n",
      "Training started at 1646155643.9459765\n",
      "Epoch 0: 69.07508850097656: 0.155\n",
      "Epoch 5000: 48.693321228027344: 0.8719\n",
      "Training finished at 1646155686.1354315; lasted 42.18945503234863 seconds.\n",
      "87.19 % success on test data\n",
      "--- (74) ---\n",
      "Training started at 1646155687.183488\n",
      "Epoch 0: 69.08776092529297: 0.0982\n",
      "Epoch 5000: 46.82038879394531: 0.8685\n",
      "Training finished at 1646155729.47088; lasted 42.287392139434814 seconds.\n",
      "86.85000000000001 % success on test data\n",
      "--- (75) ---\n",
      "Training started at 1646155730.5818107\n",
      "Epoch 0: 69.05907440185547: 0.1269\n",
      "Epoch 5000: 43.85857009887695: 0.9544\n",
      "Training finished at 1646155772.7899327; lasted 42.20812201499939 seconds.\n",
      "95.44 % success on test data\n",
      "--- (76) ---\n",
      "Training started at 1646155773.8375008\n",
      "Epoch 0: 69.19882202148438: 0.0892\n",
      "Epoch 5000: 51.36536407470703: 0.7778\n",
      "Training finished at 1646155816.3029966; lasted 42.46549582481384 seconds.\n",
      "77.78 % success on test data\n",
      "--- (77) ---\n",
      "Training started at 1646155817.341452\n",
      "Epoch 0: 69.0406723022461: 0.1028\n",
      "Epoch 5000: 56.531951904296875: 0.6869\n",
      "Training finished at 1646155859.3150353; lasted 41.973583459854126 seconds.\n",
      "68.69 % success on test data\n",
      "--- (78) ---\n",
      "Training started at 1646155860.3611343\n",
      "Epoch 0: 68.97767639160156: 0.1141\n",
      "Epoch 5000: 52.73965072631836: 0.7651\n",
      "Training finished at 1646155902.863741; lasted 42.50260663032532 seconds.\n",
      "76.51 % success on test data\n",
      "--- (79) ---\n",
      "Training started at 1646155903.9034417\n",
      "Epoch 0: 69.18313598632812: 0.0974\n",
      "Epoch 5000: 44.85196304321289: 0.9614\n",
      "Training finished at 1646155945.901816; lasted 41.998374223709106 seconds.\n",
      "96.14 % success on test data\n",
      "--- (80) ---\n",
      "Training started at 1646155946.9617915\n",
      "Epoch 0: 69.0650405883789: 0.101\n",
      "Epoch 5000: 49.09062576293945: 0.8627\n",
      "Training finished at 1646155988.7408414; lasted 41.77904987335205 seconds.\n",
      "86.27 % success on test data\n",
      "--- (81) ---\n",
      "Training started at 1646155989.7774773\n",
      "Epoch 0: 68.90264129638672: 0.1329\n",
      "Epoch 5000: 46.53178024291992: 0.8646\n",
      "Training finished at 1646156031.6291735; lasted 41.851696252822876 seconds.\n",
      "86.46000000000001 % success on test data\n",
      "--- (82) ---\n",
      "Training started at 1646156032.676206\n",
      "Epoch 0: 69.26420593261719: 0.103\n",
      "Epoch 5000: 46.38905334472656: 0.9437\n",
      "Training finished at 1646156075.0503209; lasted 42.374114751815796 seconds.\n",
      "94.37 % success on test data\n",
      "--- (83) ---\n",
      "Training started at 1646156076.1227899\n",
      "Epoch 0: 69.0016860961914: 0.0982\n",
      "Epoch 5000: 45.148189544677734: 0.8781\n",
      "Training finished at 1646156118.720789; lasted 42.59799909591675 seconds.\n",
      "87.81 % success on test data\n",
      "--- (84) ---\n",
      "Training started at 1646156119.7707648\n",
      "Epoch 0: 69.0346450805664: 0.1893\n",
      "Epoch 5000: 46.78721237182617: 0.7758\n",
      "Training finished at 1646156162.252339; lasted 42.481574058532715 seconds.\n",
      "77.58 % success on test data\n",
      "--- (85) ---\n",
      "Training started at 1646156163.3340964\n",
      "Epoch 0: 69.17582702636719: 0.101\n",
      "Epoch 5000: 47.91455841064453: 0.8744\n",
      "Training finished at 1646156205.1347978; lasted 41.800701379776 seconds.\n",
      "87.44 % success on test data\n",
      "--- (86) ---\n",
      "Training started at 1646156206.1849883\n",
      "Epoch 0: 69.096435546875: 0.0958\n",
      "Epoch 5000: 49.58812713623047: 0.87\n",
      "Training finished at 1646156248.5933568; lasted 42.4083685874939 seconds.\n",
      "87.0 % success on test data\n",
      "--- (87) ---\n",
      "Training started at 1646156249.615158\n",
      "Epoch 0: 69.06173706054688: 0.101\n",
      "Epoch 5000: 57.959468841552734: 0.6898\n",
      "Training finished at 1646156293.412606; lasted 43.79744791984558 seconds.\n",
      "68.97999999999999 % success on test data\n",
      "--- (88) ---\n",
      "Training started at 1646156294.4425938\n",
      "Epoch 0: 69.06494140625: 0.0997\n",
      "Epoch 5000: 47.660701751708984: 0.8623\n",
      "Training finished at 1646156340.6782324; lasted 46.23563861846924 seconds.\n",
      "86.22999999999999 % success on test data\n",
      "--- (89) ---\n",
      "Training started at 1646156341.8280923\n",
      "Epoch 0: 69.21975708007812: 0.101\n",
      "Epoch 5000: 49.90793991088867: 0.8674\n",
      "Training finished at 1646156385.7254744; lasted 43.89738202095032 seconds.\n",
      "86.74 % success on test data\n"
     ]
    }
   ],
   "source": [
    "for i in range(90):\n",
    "    print(f'--- ({i}) ---')\n",
    "    model = ModelManager.get_untrained(ModelType.MnistCnnOto)\n",
    "    Coach.train(\n",
    "        model,\n",
    "        data,\n",
    "        nn.CrossEntropyLoss(reduction='sum'),\n",
    "        optim.Adagrad(model.parameters(), lr=1e-2),\n",
    "        30,\n",
    "        5001,\n",
    "        5000\n",
    "    )\n",
    "    adagrad.append(Coach.measure_performance(model, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- (0) ---\n",
      "Training started at 1646156465.7155545\n",
      "Epoch 0: 69.02589416503906: 0.1135\n",
      "Epoch 5000: 44.834983825683594: 0.9722\n",
      "Training finished at 1646156509.9149046; lasted 44.199350118637085 seconds.\n",
      "97.22 % success on test data\n",
      "--- (1) ---\n",
      "Training started at 1646156510.936437\n",
      "Epoch 0: 68.96785736083984: 0.1376\n",
      "Epoch 5000: 44.84233093261719: 0.9732\n",
      "Training finished at 1646156551.187158; lasted 40.25072121620178 seconds.\n",
      "97.32 % success on test data\n",
      "--- (2) ---\n",
      "Training started at 1646156552.2268622\n",
      "Epoch 0: 68.9742202758789: 0.0974\n",
      "Epoch 5000: 44.51525115966797: 0.9712\n",
      "Training finished at 1646156593.915777; lasted 41.68891477584839 seconds.\n",
      "97.11999999999999 % success on test data\n",
      "--- (3) ---\n",
      "Training started at 1646156594.980674\n",
      "Epoch 0: 69.13908386230469: 0.1028\n",
      "Epoch 5000: 43.842323303222656: 0.9703\n",
      "Training finished at 1646156636.433838; lasted 41.453163862228394 seconds.\n",
      "97.03 % success on test data\n",
      "--- (4) ---\n",
      "Training started at 1646156637.4745638\n",
      "Epoch 0: 69.01087188720703: 0.1032\n",
      "Epoch 5000: 48.551448822021484: 0.8788\n",
      "Training finished at 1646156677.4322758; lasted 39.957711935043335 seconds.\n",
      "87.88 % success on test data\n",
      "--- (5) ---\n",
      "Training started at 1646156678.4321866\n",
      "Epoch 0: 69.08355712890625: 0.1016\n",
      "Epoch 5000: 43.86992645263672: 0.9681\n",
      "Training finished at 1646156718.4444923; lasted 40.01230573654175 seconds.\n",
      "96.81 % success on test data\n",
      "--- (6) ---\n",
      "Training started at 1646156719.5212889\n",
      "Epoch 0: 69.07484436035156: 0.0974\n",
      "Epoch 5000: 43.882286071777344: 0.9703\n",
      "Training finished at 1646156759.8267155; lasted 40.305426597595215 seconds.\n",
      "97.03 % success on test data\n",
      "--- (7) ---\n",
      "Training started at 1646156760.850229\n",
      "Epoch 0: 69.07545471191406: 0.0326\n",
      "Epoch 5000: 46.90128707885742: 0.8748\n",
      "Training finished at 1646156801.0340378; lasted 40.18380880355835 seconds.\n",
      "87.48 % success on test data\n",
      "--- (8) ---\n",
      "Training started at 1646156802.1472285\n",
      "Epoch 0: 69.13861083984375: 0.1082\n",
      "Epoch 5000: 43.841888427734375: 0.97\n",
      "Training finished at 1646156842.3184025; lasted 40.17117404937744 seconds.\n",
      "97.0 % success on test data\n",
      "--- (9) ---\n",
      "Training started at 1646156843.3700907\n",
      "Epoch 0: 69.09252166748047: 0.1009\n",
      "Epoch 5000: 44.82160186767578: 0.9682\n",
      "Training finished at 1646156884.1835434; lasted 40.81345272064209 seconds.\n",
      "96.82 % success on test data\n",
      "--- (10) ---\n",
      "Training started at 1646156885.191265\n",
      "Epoch 0: 69.14415740966797: 0.1933\n",
      "Epoch 5000: 44.83209228515625: 0.9613\n",
      "Training finished at 1646156925.4528115; lasted 40.26154637336731 seconds.\n",
      "96.13000000000001 % success on test data\n",
      "--- (11) ---\n",
      "Training started at 1646156926.4698405\n",
      "Epoch 0: 68.93437957763672: 0.1009\n",
      "Epoch 5000: 43.926822662353516: 0.9748\n",
      "Training finished at 1646156966.9467213; lasted 40.4768807888031 seconds.\n",
      "97.48 % success on test data\n",
      "--- (12) ---\n",
      "Training started at 1646156967.9875195\n",
      "Epoch 0: 69.0176010131836: 0.0885\n",
      "Epoch 5000: 45.822593688964844: 0.9739\n",
      "Training finished at 1646157008.4056058; lasted 40.4180862903595 seconds.\n",
      "97.39 % success on test data\n",
      "--- (13) ---\n",
      "Training started at 1646157009.4192586\n",
      "Epoch 0: 69.12368774414062: 0.1084\n",
      "Epoch 5000: 45.576698303222656: 0.9604\n",
      "Training finished at 1646157049.415187; lasted 39.9959282875061 seconds.\n",
      "96.04 % success on test data\n",
      "--- (14) ---\n",
      "Training started at 1646157050.4680328\n",
      "Epoch 0: 69.08259582519531: 0.1044\n",
      "Epoch 5000: 46.38292694091797: 0.9732\n",
      "Training finished at 1646157090.6213973; lasted 40.153364419937134 seconds.\n",
      "97.32 % success on test data\n",
      "--- (15) ---\n",
      "Training started at 1646157091.652339\n",
      "Epoch 0: 69.10377502441406: 0.1028\n",
      "Epoch 5000: 43.84663391113281: 0.9697\n",
      "Training finished at 1646157132.081071; lasted 40.42873191833496 seconds.\n",
      "96.97 % success on test data\n",
      "--- (16) ---\n",
      "Training started at 1646157133.1698203\n",
      "Epoch 0: 69.05513000488281: 0.1028\n",
      "Epoch 5000: 46.444847106933594: 0.8727\n",
      "Training finished at 1646157173.6612635; lasted 40.491443157196045 seconds.\n",
      "87.27000000000001 % success on test data\n",
      "--- (17) ---\n",
      "Training started at 1646157174.7024572\n",
      "Epoch 0: 69.05892181396484: 0.1981\n",
      "Epoch 5000: 44.58766555786133: 0.9647\n",
      "Training finished at 1646157214.8132167; lasted 40.11075949668884 seconds.\n",
      "96.47 % success on test data\n",
      "--- (18) ---\n",
      "Training started at 1646157215.8377178\n",
      "Epoch 0: 69.08086395263672: 0.1499\n",
      "Epoch 5000: 43.834564208984375: 0.9747\n",
      "Training finished at 1646157256.3183646; lasted 40.48064684867859 seconds.\n",
      "97.47 % success on test data\n",
      "--- (19) ---\n",
      "Training started at 1646157257.339551\n",
      "Epoch 0: 68.92697143554688: 0.0974\n",
      "Epoch 5000: 46.31227493286133: 0.9663\n",
      "Training finished at 1646157297.787921; lasted 40.4483699798584 seconds.\n",
      "96.63000000000001 % success on test data\n",
      "--- (20) ---\n",
      "Training started at 1646157298.8171442\n",
      "Epoch 0: 69.10807037353516: 0.1032\n",
      "Epoch 5000: 46.311885833740234: 0.9723\n",
      "Training finished at 1646157338.8313587; lasted 40.014214515686035 seconds.\n",
      "97.23 % success on test data\n",
      "--- (21) ---\n",
      "Training started at 1646157339.879397\n",
      "Epoch 0: 69.11112976074219: 0.0892\n",
      "Epoch 5000: 43.85196304321289: 0.9761\n",
      "Training finished at 1646157380.1411505; lasted 40.26175355911255 seconds.\n",
      "97.61 % success on test data\n",
      "--- (22) ---\n",
      "Training started at 1646157381.1537106\n",
      "Epoch 0: 69.1312026977539: 0.0958\n",
      "Epoch 5000: 43.897987365722656: 0.9717\n",
      "Training finished at 1646157421.36313; lasted 40.20941948890686 seconds.\n",
      "97.17 % success on test data\n",
      "--- (23) ---\n",
      "Training started at 1646157422.4251688\n",
      "Epoch 0: 68.91654205322266: 0.1068\n",
      "Epoch 5000: 43.84878158569336: 0.9737\n",
      "Training finished at 1646157462.6878705; lasted 40.262701749801636 seconds.\n",
      "97.37 % success on test data\n",
      "--- (24) ---\n",
      "Training started at 1646157463.7764773\n",
      "Epoch 0: 68.9399642944336: 0.0958\n",
      "Epoch 5000: 43.883026123046875: 0.9712\n",
      "Training finished at 1646157504.2050583; lasted 40.42858099937439 seconds.\n",
      "97.11999999999999 % success on test data\n",
      "--- (25) ---\n",
      "Training started at 1646157505.2331626\n",
      "Epoch 0: 69.19371032714844: 0.2047\n",
      "Epoch 5000: 43.97344970703125: 0.976\n",
      "Training finished at 1646157545.249282; lasted 40.01611924171448 seconds.\n",
      "97.6 % success on test data\n",
      "--- (26) ---\n",
      "Training started at 1646157546.260294\n",
      "Epoch 0: 69.12957000732422: 0.1009\n",
      "Epoch 5000: 44.8261604309082: 0.9717\n",
      "Training finished at 1646157587.0392973; lasted 40.779003381729126 seconds.\n",
      "97.17 % success on test data\n",
      "--- (27) ---\n",
      "Training started at 1646157588.0742638\n",
      "Epoch 0: 69.24839782714844: 0.1234\n",
      "Epoch 5000: 44.95402145385742: 0.9714\n",
      "Training finished at 1646157628.0567236; lasted 39.98245978355408 seconds.\n",
      "97.14 % success on test data\n",
      "--- (28) ---\n",
      "Training started at 1646157629.092868\n",
      "Epoch 0: 69.1944580078125: 0.0961\n",
      "Epoch 5000: 44.318416595458984: 0.975\n",
      "Training finished at 1646157669.4401028; lasted 40.34723472595215 seconds.\n",
      "97.5 % success on test data\n",
      "--- (29) ---\n",
      "Training started at 1646157670.4591525\n",
      "Epoch 0: 69.04320526123047: 0.1125\n",
      "Epoch 5000: 43.931793212890625: 0.9762\n",
      "Training finished at 1646157710.5775435; lasted 40.118391036987305 seconds.\n",
      "97.61999999999999 % success on test data\n",
      "--- (30) ---\n",
      "Training started at 1646157711.6104276\n",
      "Epoch 0: 69.18881225585938: 0.1571\n",
      "Epoch 5000: 45.6806526184082: 0.9679\n",
      "Training finished at 1646157751.6708996; lasted 40.06047201156616 seconds.\n",
      "96.78999999999999 % success on test data\n",
      "--- (31) ---\n",
      "Training started at 1646157752.7017887\n",
      "Epoch 0: 69.09814453125: 0.1602\n",
      "Epoch 5000: 44.42587661743164: 0.9753\n",
      "Training finished at 1646157793.2892096; lasted 40.58742094039917 seconds.\n",
      "97.53 % success on test data\n",
      "--- (32) ---\n",
      "Training started at 1646157794.3486474\n",
      "Epoch 0: 69.10807037353516: 0.0982\n",
      "Epoch 5000: 44.9695930480957: 0.9698\n",
      "Training finished at 1646157836.8814986; lasted 42.532851219177246 seconds.\n",
      "96.98 % success on test data\n",
      "--- (33) ---\n",
      "Training started at 1646157837.9337142\n",
      "Epoch 0: 69.02086639404297: 0.1085\n",
      "Epoch 5000: 43.9761962890625: 0.9712\n",
      "Training finished at 1646157877.9965708; lasted 40.062856674194336 seconds.\n",
      "97.11999999999999 % success on test data\n",
      "--- (34) ---\n",
      "Training started at 1646157879.0689247\n",
      "Epoch 0: 69.21018981933594: 0.1146\n",
      "Epoch 5000: 45.28853225708008: 0.8762\n",
      "Training finished at 1646157919.3334558; lasted 40.26453113555908 seconds.\n",
      "87.62 % success on test data\n",
      "--- (35) ---\n",
      "Training started at 1646157920.3503082\n",
      "Epoch 0: 69.20136260986328: 0.0892\n",
      "Epoch 5000: 44.18207550048828: 0.9726\n",
      "Training finished at 1646157960.4987082; lasted 40.14840006828308 seconds.\n",
      "97.26 % success on test data\n",
      "--- (36) ---\n",
      "Training started at 1646157961.5703032\n",
      "Epoch 0: 68.99391174316406: 0.0958\n",
      "Epoch 5000: 44.62454605102539: 0.9729\n",
      "Training finished at 1646158001.6773198; lasted 40.10701656341553 seconds.\n",
      "97.28999999999999 % success on test data\n",
      "--- (37) ---\n",
      "Training started at 1646158002.6970468\n",
      "Epoch 0: 69.01284790039062: 0.1165\n",
      "Epoch 5000: 45.00591278076172: 0.9719\n",
      "Training finished at 1646158042.8607473; lasted 40.163700580596924 seconds.\n",
      "97.19 % success on test data\n",
      "--- (38) ---\n",
      "Training started at 1646158043.919119\n",
      "Epoch 0: 69.2656021118164: 0.107\n",
      "Epoch 5000: 45.026798248291016: 0.972\n",
      "Training finished at 1646158084.0785427; lasted 40.159423828125 seconds.\n",
      "97.2 % success on test data\n",
      "--- (39) ---\n",
      "Training started at 1646158085.2255077\n",
      "Epoch 0: 69.1912841796875: 0.1031\n",
      "Epoch 5000: 45.68553924560547: 0.9745\n",
      "Training finished at 1646158125.6338897; lasted 40.408381938934326 seconds.\n",
      "97.45 % success on test data\n",
      "--- (40) ---\n",
      "Training started at 1646158126.6628916\n",
      "Epoch 0: 68.99586486816406: 0.0979\n",
      "Epoch 5000: 45.072269439697266: 0.9629\n",
      "Training finished at 1646158167.302612; lasted 40.63972043991089 seconds.\n",
      "96.28999999999999 % success on test data\n",
      "--- (41) ---\n",
      "Training started at 1646158168.3448803\n",
      "Epoch 0: 69.04484558105469: 0.1265\n",
      "Epoch 5000: 49.15293884277344: 0.883\n",
      "Training finished at 1646158208.6986215; lasted 40.35374116897583 seconds.\n",
      "88.3 % success on test data\n",
      "--- (42) ---\n",
      "Training started at 1646158209.7470012\n",
      "Epoch 0: 69.1624526977539: 0.1048\n",
      "Epoch 5000: 45.80666732788086: 0.9607\n",
      "Training finished at 1646158249.9373727; lasted 40.1903715133667 seconds.\n",
      "96.07 % success on test data\n",
      "--- (43) ---\n",
      "Training started at 1646158251.0119972\n",
      "Epoch 0: 69.00128173828125: 0.1028\n",
      "Epoch 5000: 44.908477783203125: 0.9733\n",
      "Training finished at 1646158291.3032687; lasted 40.291271448135376 seconds.\n",
      "97.33000000000001 % success on test data\n",
      "--- (44) ---\n",
      "Training started at 1646158292.3262594\n",
      "Epoch 0: 69.02533721923828: 0.1996\n",
      "Epoch 5000: 44.84568786621094: 0.9701\n",
      "Training finished at 1646158332.4508936; lasted 40.12463426589966 seconds.\n",
      "97.00999999999999 % success on test data\n",
      "--- (45) ---\n",
      "Training started at 1646158333.4788673\n",
      "Epoch 0: 69.03067779541016: 0.17\n",
      "Epoch 5000: 50.66358184814453: 0.8736\n",
      "Training finished at 1646158373.655353; lasted 40.176485776901245 seconds.\n",
      "87.36 % success on test data\n",
      "--- (46) ---\n",
      "Training started at 1646158374.7463727\n",
      "Epoch 0: 69.12059020996094: 0.0578\n",
      "Epoch 5000: 44.95758819580078: 0.9767\n",
      "Training finished at 1646158414.896926; lasted 40.15055322647095 seconds.\n",
      "97.67 % success on test data\n",
      "--- (47) ---\n",
      "Training started at 1646158415.9705825\n",
      "Epoch 0: 69.28500366210938: 0.1674\n",
      "Epoch 5000: 43.866329193115234: 0.9678\n",
      "Training finished at 1646158456.1459184; lasted 40.17533588409424 seconds.\n",
      "96.78 % success on test data\n",
      "--- (48) ---\n",
      "Training started at 1646158457.2198756\n",
      "Epoch 0: 69.20600891113281: 0.098\n",
      "Epoch 5000: 44.715171813964844: 0.9743\n",
      "Training finished at 1646158497.2241578; lasted 40.00428223609924 seconds.\n",
      "97.43 % success on test data\n",
      "--- (49) ---\n",
      "Training started at 1646158498.2736685\n",
      "Epoch 0: 69.01802062988281: 0.103\n",
      "Epoch 5000: 44.14305877685547: 0.9762\n",
      "Training finished at 1646158538.3093445; lasted 40.03567600250244 seconds.\n",
      "97.61999999999999 % success on test data\n",
      "--- (50) ---\n",
      "Training started at 1646158539.33736\n",
      "Epoch 0: 69.0027084350586: 0.1362\n",
      "Epoch 5000: 43.962059020996094: 0.9726\n",
      "Training finished at 1646158579.739562; lasted 40.402202129364014 seconds.\n",
      "97.26 % success on test data\n",
      "--- (51) ---\n",
      "Training started at 1646158580.806617\n",
      "Epoch 0: 68.94591522216797: 0.098\n",
      "Epoch 5000: 43.872901916503906: 0.9684\n",
      "Training finished at 1646158620.7707465; lasted 39.96412944793701 seconds.\n",
      "96.84 % success on test data\n",
      "--- (52) ---\n",
      "Training started at 1646158621.7873304\n",
      "Epoch 0: 69.10787963867188: 0.098\n",
      "Epoch 5000: 45.94646453857422: 0.8785\n",
      "Training finished at 1646158661.6603565; lasted 39.87302613258362 seconds.\n",
      "87.85 % success on test data\n",
      "--- (53) ---\n",
      "Training started at 1646158662.7172124\n",
      "Epoch 0: 68.92697143554688: 0.1032\n",
      "Epoch 5000: 43.85414123535156: 0.9708\n",
      "Training finished at 1646158703.0641282; lasted 40.34691572189331 seconds.\n",
      "97.08 % success on test data\n",
      "--- (54) ---\n",
      "Training started at 1646158704.0802004\n",
      "Epoch 0: 69.22138977050781: 0.0984\n",
      "Epoch 5000: 44.83503723144531: 0.97\n",
      "Training finished at 1646158744.050671; lasted 39.970470666885376 seconds.\n",
      "97.0 % success on test data\n",
      "--- (55) ---\n",
      "Training started at 1646158745.0902472\n",
      "Epoch 0: 69.03301239013672: 0.1135\n",
      "Epoch 5000: 45.072906494140625: 0.971\n",
      "Training finished at 1646158785.12552; lasted 40.03527283668518 seconds.\n",
      "97.1 % success on test data\n",
      "--- (56) ---\n",
      "Training started at 1646158786.1467884\n",
      "Epoch 0: 69.07760620117188: 0.19\n",
      "Epoch 5000: 44.826629638671875: 0.9756\n",
      "Training finished at 1646158826.2747798; lasted 40.12799143791199 seconds.\n",
      "97.56 % success on test data\n",
      "--- (57) ---\n",
      "Training started at 1646158827.3092618\n",
      "Epoch 0: 69.063720703125: 0.1028\n",
      "Epoch 5000: 45.795780181884766: 0.8813\n",
      "Training finished at 1646158867.6440246; lasted 40.33476281166077 seconds.\n",
      "88.13 % success on test data\n",
      "--- (58) ---\n",
      "Training started at 1646158868.68315\n",
      "Epoch 0: 69.12283325195312: 0.098\n",
      "Epoch 5000: 45.32550048828125: 0.9708\n",
      "Training finished at 1646158908.769415; lasted 40.086264848709106 seconds.\n",
      "97.08 % success on test data\n",
      "--- (59) ---\n",
      "Training started at 1646158909.842546\n",
      "Epoch 0: 69.02243041992188: 0.1028\n",
      "Epoch 5000: 47.72760772705078: 0.8777\n",
      "Training finished at 1646158950.090755; lasted 40.24820899963379 seconds.\n",
      "87.77000000000001 % success on test data\n",
      "--- (60) ---\n",
      "Training started at 1646158951.1137407\n",
      "Epoch 0: 69.06243133544922: 0.1459\n",
      "Epoch 5000: 45.7710075378418: 0.9726\n",
      "Training finished at 1646158991.1646698; lasted 40.05092906951904 seconds.\n",
      "97.26 % success on test data\n",
      "--- (61) ---\n",
      "Training started at 1646158992.1920204\n",
      "Epoch 0: 69.17698669433594: 0.101\n",
      "Epoch 5000: 44.457889556884766: 0.9724\n",
      "Training finished at 1646159032.3373895; lasted 40.14536905288696 seconds.\n",
      "97.24000000000001 % success on test data\n",
      "--- (62) ---\n",
      "Training started at 1646159033.423896\n",
      "Epoch 0: 68.9905014038086: 0.1032\n",
      "Epoch 5000: 44.9415397644043: 0.967\n",
      "Training finished at 1646159073.5738578; lasted 40.149961709976196 seconds.\n",
      "96.7 % success on test data\n",
      "--- (63) ---\n",
      "Training started at 1646159074.6023164\n",
      "Epoch 0: 68.97040557861328: 0.095\n",
      "Epoch 5000: 44.85089874267578: 0.9702\n",
      "Training finished at 1646159115.0835514; lasted 40.48123502731323 seconds.\n",
      "97.02 % success on test data\n",
      "--- (64) ---\n",
      "Training started at 1646159116.1180432\n",
      "Epoch 0: 69.2771224975586: 0.1009\n",
      "Epoch 5000: 45.506561279296875: 0.9673\n",
      "Training finished at 1646159156.5913193; lasted 40.473276138305664 seconds.\n",
      "96.73 % success on test data\n",
      "--- (65) ---\n",
      "Training started at 1646159157.617264\n",
      "Epoch 0: 69.15170288085938: 0.1639\n",
      "Epoch 5000: 46.41694641113281: 0.966\n",
      "Training finished at 1646159197.5939348; lasted 39.97667074203491 seconds.\n",
      "96.6 % success on test data\n",
      "--- (66) ---\n",
      "Training started at 1646159198.6124544\n",
      "Epoch 0: 69.01673126220703: 0.1227\n",
      "Epoch 5000: 45.525123596191406: 0.9724\n",
      "Training finished at 1646159239.0450573; lasted 40.432602882385254 seconds.\n",
      "97.24000000000001 % success on test data\n",
      "--- (67) ---\n",
      "Training started at 1646159240.0951066\n",
      "Epoch 0: 69.06035614013672: 0.1018\n",
      "Epoch 5000: 44.383140563964844: 0.9648\n",
      "Training finished at 1646159280.250013; lasted 40.15490651130676 seconds.\n",
      "96.48 % success on test data\n",
      "--- (68) ---\n",
      "Training started at 1646159281.2933524\n",
      "Epoch 0: 69.03733825683594: 0.1118\n",
      "Epoch 5000: 43.85725021362305: 0.9735\n",
      "Training finished at 1646159321.3898196; lasted 40.09646725654602 seconds.\n",
      "97.35000000000001 % success on test data\n",
      "--- (69) ---\n",
      "Training started at 1646159322.402611\n",
      "Epoch 0: 69.0818099975586: 0.1032\n",
      "Epoch 5000: 43.86509323120117: 0.9708\n",
      "Training finished at 1646159362.502982; lasted 40.10037088394165 seconds.\n",
      "97.08 % success on test data\n",
      "--- (70) ---\n",
      "Training started at 1646159363.5264635\n",
      "Epoch 0: 69.03507995605469: 0.1009\n",
      "Epoch 5000: 45.27326965332031: 0.9742\n",
      "Training finished at 1646159404.0045087; lasted 40.47804522514343 seconds.\n",
      "97.42 % success on test data\n",
      "--- (71) ---\n",
      "Training started at 1646159405.038723\n",
      "Epoch 0: 69.17094421386719: 0.1016\n",
      "Epoch 5000: 44.70660400390625: 0.974\n",
      "Training finished at 1646159445.1836102; lasted 40.1448872089386 seconds.\n",
      "97.39999999999999 % success on test data\n",
      "--- (72) ---\n",
      "Training started at 1646159446.2171483\n",
      "Epoch 0: 69.09654998779297: 0.115\n",
      "Epoch 5000: 46.065189361572266: 0.8813\n",
      "Training finished at 1646159486.2298625; lasted 40.01271414756775 seconds.\n",
      "88.13 % success on test data\n",
      "--- (73) ---\n",
      "Training started at 1646159487.2568412\n",
      "Epoch 0: 69.18834686279297: 0.0959\n",
      "Epoch 5000: 52.71651840209961: 0.8787\n",
      "Training finished at 1646159527.6494186; lasted 40.39257740974426 seconds.\n",
      "87.87 % success on test data\n",
      "--- (74) ---\n",
      "Training started at 1646159528.6950896\n",
      "Epoch 0: 69.15890502929688: 0.1825\n",
      "Epoch 5000: 44.82992172241211: 0.9737\n",
      "Training finished at 1646159569.467379; lasted 40.772289514541626 seconds.\n",
      "97.37 % success on test data\n",
      "--- (75) ---\n",
      "Training started at 1646159570.4983208\n",
      "Epoch 0: 69.09429931640625: 0.1135\n",
      "Epoch 5000: 44.04678726196289: 0.9722\n",
      "Training finished at 1646159611.7958038; lasted 41.29748296737671 seconds.\n",
      "97.22 % success on test data\n",
      "--- (76) ---\n",
      "Training started at 1646159612.846568\n",
      "Epoch 0: 68.98609161376953: 0.0958\n",
      "Epoch 5000: 46.671016693115234: 0.9741\n",
      "Training finished at 1646159653.2796884; lasted 40.433120250701904 seconds.\n",
      "97.41 % success on test data\n",
      "--- (77) ---\n",
      "Training started at 1646159654.310645\n",
      "Epoch 0: 69.09365844726562: 0.0958\n",
      "Epoch 5000: 44.12821960449219: 0.9715\n",
      "Training finished at 1646159694.6859055; lasted 40.37526035308838 seconds.\n",
      "97.15 % success on test data\n",
      "--- (78) ---\n",
      "Training started at 1646159695.7339144\n",
      "Epoch 0: 69.0497817993164: 0.1635\n",
      "Epoch 5000: 44.77488708496094: 0.9701\n",
      "Training finished at 1646159736.1091568; lasted 40.375242471694946 seconds.\n",
      "97.00999999999999 % success on test data\n",
      "--- (79) ---\n",
      "Training started at 1646159737.1361468\n",
      "Epoch 0: 69.1270980834961: 0.1036\n",
      "Epoch 5000: 43.83629608154297: 0.9764\n",
      "Training finished at 1646159777.2321851; lasted 40.09603834152222 seconds.\n",
      "97.64 % success on test data\n",
      "--- (80) ---\n",
      "Training started at 1646159778.3055227\n",
      "Epoch 0: 69.05201721191406: 0.1113\n",
      "Epoch 5000: 43.84072494506836: 0.9738\n",
      "Training finished at 1646159818.4215498; lasted 40.11602711677551 seconds.\n",
      "97.38 % success on test data\n",
      "--- (81) ---\n",
      "Training started at 1646159819.44827\n",
      "Epoch 0: 69.07308197021484: 0.1518\n",
      "Epoch 5000: 47.84464645385742: 0.8805\n",
      "Training finished at 1646159859.8265605; lasted 40.37829041481018 seconds.\n",
      "88.05 % success on test data\n",
      "--- (82) ---\n",
      "Training started at 1646159860.8855608\n",
      "Epoch 0: 69.15923309326172: 0.1212\n",
      "Epoch 5000: 43.84363555908203: 0.9705\n",
      "Training finished at 1646159901.5328782; lasted 40.64731740951538 seconds.\n",
      "97.05 % success on test data\n",
      "--- (83) ---\n",
      "Training started at 1646159902.5500593\n",
      "Epoch 0: 69.02825164794922: 0.098\n",
      "Epoch 5000: 44.21397399902344: 0.9714\n",
      "Training finished at 1646159942.6562757; lasted 40.10621643066406 seconds.\n",
      "97.14 % success on test data\n",
      "--- (84) ---\n",
      "Training started at 1646159943.709429\n",
      "Epoch 0: 69.08934020996094: 0.1003\n",
      "Epoch 5000: 44.01935958862305: 0.9712\n",
      "Training finished at 1646159984.0465822; lasted 40.33715319633484 seconds.\n",
      "97.11999999999999 % success on test data\n",
      "--- (85) ---\n",
      "Training started at 1646159985.0965242\n",
      "Epoch 0: 69.0672836303711: 0.098\n",
      "Epoch 5000: 43.84210205078125: 0.9702\n",
      "Training finished at 1646160025.1996078; lasted 40.10308361053467 seconds.\n",
      "97.02 % success on test data\n",
      "--- (86) ---\n",
      "Training started at 1646160026.2197616\n",
      "Epoch 0: 69.11631774902344: 0.1103\n",
      "Epoch 5000: 43.91773223876953: 0.9693\n",
      "Training finished at 1646160067.5424612; lasted 41.322699546813965 seconds.\n",
      "96.93 % success on test data\n",
      "--- (87) ---\n",
      "Training started at 1646160068.573694\n",
      "Epoch 0: 68.95471954345703: 0.1828\n",
      "Epoch 5000: 43.836368560791016: 0.9755\n",
      "Training finished at 1646160108.9434643; lasted 40.36977028846741 seconds.\n",
      "97.55 % success on test data\n",
      "--- (88) ---\n",
      "Training started at 1646160109.973926\n",
      "Epoch 0: 68.96305084228516: 0.0974\n",
      "Epoch 5000: 47.836421966552734: 0.8842\n",
      "Training finished at 1646160150.4562743; lasted 40.48234820365906 seconds.\n",
      "88.42 % success on test data\n",
      "--- (89) ---\n",
      "Training started at 1646160151.5072193\n",
      "Epoch 0: 69.08622741699219: 0.1135\n",
      "Epoch 5000: 43.88420104980469: 0.9701\n",
      "Training finished at 1646160191.563415; lasted 40.0561957359314 seconds.\n",
      "97.00999999999999 % success on test data\n"
     ]
    }
   ],
   "source": [
    "for i in range(90):\n",
    "    print(f'--- ({i}) ---')\n",
    "    model = ModelManager.get_untrained(ModelType.MnistCnnOto)\n",
    "    Coach.train(\n",
    "        model,\n",
    "        data,\n",
    "        nn.CrossEntropyLoss(reduction='sum'),\n",
    "        optim.RMSprop(model.parameters(), lr=1e-3),\n",
    "        30,\n",
    "        5001,\n",
    "        5000\n",
    "    )\n",
    "    rmsprop.append(Coach.measure_performance(model, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- (0) ---\n",
      "Training started at 1646160243.123983\n",
      "Epoch 0: 69.09040069580078: 0.1135\n",
      "Epoch 5000: 45.26240539550781: 0.9691\n",
      "Training finished at 1646160288.1013148; lasted 44.97733187675476 seconds.\n",
      "96.91 % success on test data\n",
      "--- (1) ---\n",
      "Training started at 1646160289.2130427\n",
      "Epoch 0: 69.150146484375: 0.1014\n",
      "Epoch 5000: 43.87419128417969: 0.9704\n",
      "Training finished at 1646160335.40789; lasted 46.19484734535217 seconds.\n",
      "97.04 % success on test data\n",
      "--- (2) ---\n",
      "Training started at 1646160336.4625561\n",
      "Epoch 0: 69.25453186035156: 0.1362\n",
      "Epoch 5000: 44.985679626464844: 0.9754\n",
      "Training finished at 1646160380.7386332; lasted 44.27607703208923 seconds.\n",
      "97.54 % success on test data\n",
      "--- (3) ---\n",
      "Training started at 1646160381.7607477\n",
      "Epoch 0: 69.09335327148438: 0.1035\n",
      "Epoch 5000: 45.992740631103516: 0.8831\n",
      "Training finished at 1646160425.8366742; lasted 44.075926542282104 seconds.\n",
      "88.31 % success on test data\n",
      "--- (4) ---\n",
      "Training started at 1646160426.896686\n",
      "Epoch 0: 69.24952697753906: 0.1098\n",
      "Epoch 5000: 50.17534255981445: 0.8678\n",
      "Training finished at 1646160471.882783; lasted 44.98609685897827 seconds.\n",
      "86.78 % success on test data\n",
      "--- (5) ---\n",
      "Training started at 1646160472.9240448\n",
      "Epoch 0: 68.95787811279297: 0.1032\n",
      "Epoch 5000: 47.81782913208008: 0.8738\n",
      "Training finished at 1646160518.7917416; lasted 45.86769676208496 seconds.\n",
      "87.38 % success on test data\n",
      "--- (6) ---\n",
      "Training started at 1646160519.828666\n",
      "Epoch 0: 68.86300659179688: 0.1009\n",
      "Epoch 5000: 44.807472229003906: 0.9726\n",
      "Training finished at 1646160564.256776; lasted 44.428110122680664 seconds.\n",
      "97.26 % success on test data\n",
      "--- (7) ---\n",
      "Training started at 1646160565.3102682\n",
      "Epoch 0: 69.03579711914062: 0.1548\n",
      "Epoch 5000: 44.19029235839844: 0.9712\n",
      "Training finished at 1646160609.2084424; lasted 43.89817428588867 seconds.\n",
      "97.11999999999999 % success on test data\n",
      "--- (8) ---\n",
      "Training started at 1646160610.244053\n",
      "Epoch 0: 68.93736267089844: 0.1009\n",
      "Epoch 5000: 46.648170471191406: 0.9717\n",
      "Training finished at 1646160656.47449; lasted 46.23043704032898 seconds.\n",
      "97.17 % success on test data\n",
      "--- (9) ---\n",
      "Training started at 1646160657.5444837\n",
      "Epoch 0: 69.17153930664062: 0.0958\n",
      "Epoch 5000: 44.10957336425781: 0.9723\n",
      "Training finished at 1646160702.31051; lasted 44.76602625846863 seconds.\n",
      "97.23 % success on test data\n",
      "--- (10) ---\n",
      "Training started at 1646160703.3935301\n",
      "Epoch 0: 69.07017517089844: 0.1006\n",
      "Epoch 5000: 44.86233901977539: 0.9705\n",
      "Training finished at 1646160747.7146764; lasted 44.32114624977112 seconds.\n",
      "97.05 % success on test data\n",
      "--- (11) ---\n",
      "Training started at 1646160748.7495263\n",
      "Epoch 0: 68.83781433105469: 0.1009\n",
      "Epoch 5000: 46.912445068359375: 0.8756\n",
      "Training finished at 1646160794.216627; lasted 45.467100620269775 seconds.\n",
      "87.56 % success on test data\n",
      "--- (12) ---\n",
      "Training started at 1646160795.2825413\n",
      "Epoch 0: 69.076416015625: 0.1009\n",
      "Epoch 5000: 45.835357666015625: 0.8723\n",
      "Training finished at 1646160839.605657; lasted 44.323115825653076 seconds.\n",
      "87.22999999999999 % success on test data\n",
      "--- (13) ---\n",
      "Training started at 1646160840.720623\n",
      "Epoch 0: 69.02472686767578: 0.1306\n",
      "Epoch 5000: 45.782135009765625: 0.9711\n",
      "Training finished at 1646160885.020881; lasted 44.30025792121887 seconds.\n",
      "97.11 % success on test data\n",
      "--- (14) ---\n",
      "Training started at 1646160886.0727081\n",
      "Epoch 0: 69.14749908447266: 0.0983\n",
      "Epoch 5000: 49.31419372558594: 0.8848\n",
      "Training finished at 1646160930.497797; lasted 44.42508888244629 seconds.\n",
      "88.48 % success on test data\n",
      "--- (15) ---\n",
      "Training started at 1646160931.5027866\n",
      "Epoch 0: 69.21636962890625: 0.0956\n",
      "Epoch 5000: 44.21011734008789: 0.9664\n",
      "Training finished at 1646160975.836912; lasted 44.33412528038025 seconds.\n",
      "96.64 % success on test data\n",
      "--- (16) ---\n",
      "Training started at 1646160976.882812\n",
      "Epoch 0: 69.08243560791016: 0.1726\n",
      "Epoch 5000: 43.84414291381836: 0.972\n",
      "Training finished at 1646161021.1895769; lasted 44.30676484107971 seconds.\n",
      "97.2 % success on test data\n",
      "--- (17) ---\n",
      "Training started at 1646161022.2435148\n",
      "Epoch 0: 69.0603256225586: 0.1128\n",
      "Epoch 5000: 44.82902145385742: 0.9641\n",
      "Training finished at 1646161066.281171; lasted 44.03765630722046 seconds.\n",
      "96.41 % success on test data\n",
      "--- (18) ---\n",
      "Training started at 1646161067.31259\n",
      "Epoch 0: 69.04045104980469: 0.1032\n",
      "Epoch 5000: 48.76247024536133: 0.8719\n",
      "Training finished at 1646161111.6628132; lasted 44.35022330284119 seconds.\n",
      "87.19 % success on test data\n",
      "--- (19) ---\n",
      "Training started at 1646161112.7149866\n",
      "Epoch 0: 68.88724517822266: 0.096\n",
      "Epoch 5000: 45.4395751953125: 0.9704\n",
      "Training finished at 1646161157.2187483; lasted 44.503761768341064 seconds.\n",
      "97.04 % success on test data\n",
      "--- (20) ---\n",
      "Training started at 1646161158.276715\n",
      "Epoch 0: 69.26011657714844: 0.1183\n",
      "Epoch 5000: 49.151947021484375: 0.8654\n",
      "Training finished at 1646161202.5611668; lasted 44.284451723098755 seconds.\n",
      "86.53999999999999 % success on test data\n",
      "--- (21) ---\n",
      "Training started at 1646161203.615398\n",
      "Epoch 0: 69.1911849975586: 0.2148\n",
      "Epoch 5000: 47.58537673950195: 0.8805\n",
      "Training finished at 1646161247.6104598; lasted 43.99506187438965 seconds.\n",
      "88.05 % success on test data\n",
      "--- (22) ---\n",
      "Training started at 1646161248.6368554\n",
      "Epoch 0: 69.0081558227539: 0.1028\n",
      "Epoch 5000: 46.80500030517578: 0.8801\n",
      "Training finished at 1646161293.1880505; lasted 44.55119514465332 seconds.\n",
      "88.01 % success on test data\n",
      "--- (23) ---\n",
      "Training started at 1646161294.2748892\n",
      "Epoch 0: 69.12577819824219: 0.098\n",
      "Epoch 5000: 46.1065788269043: 0.8695\n",
      "Training finished at 1646161338.3489554; lasted 44.074066162109375 seconds.\n",
      "86.95 % success on test data\n",
      "--- (24) ---\n",
      "Training started at 1646161339.455977\n",
      "Epoch 0: 69.03072357177734: 0.1035\n",
      "Epoch 5000: 44.83504104614258: 0.9732\n",
      "Training finished at 1646161383.5701027; lasted 44.11412572860718 seconds.\n",
      "97.32 % success on test data\n",
      "--- (25) ---\n",
      "Training started at 1646161384.652755\n",
      "Epoch 0: 69.10576629638672: 0.1009\n",
      "Epoch 5000: 43.86819839477539: 0.9757\n",
      "Training finished at 1646161430.8680558; lasted 46.21530079841614 seconds.\n",
      "97.57000000000001 % success on test data\n",
      "--- (26) ---\n",
      "Training started at 1646161431.966697\n",
      "Epoch 0: 68.97945404052734: 0.1355\n",
      "Epoch 5000: 50.13494873046875: 0.8008\n",
      "Training finished at 1646161476.0020077; lasted 44.03531074523926 seconds.\n",
      "80.08 % success on test data\n",
      "--- (27) ---\n",
      "Training started at 1646161477.104926\n",
      "Epoch 0: 69.14252471923828: 0.1028\n",
      "Epoch 5000: 46.566017150878906: 0.9616\n",
      "Training finished at 1646161521.6029243; lasted 44.49799823760986 seconds.\n",
      "96.16 % success on test data\n",
      "--- (28) ---\n",
      "Training started at 1646161522.6578875\n",
      "Epoch 0: 69.06090545654297: 0.0892\n",
      "Epoch 5000: 43.9122314453125: 0.968\n",
      "Training finished at 1646161567.0879226; lasted 44.43003511428833 seconds.\n",
      "96.8 % success on test data\n",
      "--- (29) ---\n",
      "Training started at 1646161568.1621995\n",
      "Epoch 0: 69.05423736572266: 0.1808\n",
      "Epoch 5000: 52.75861358642578: 0.8827\n",
      "Training finished at 1646161612.2769494; lasted 44.114749908447266 seconds.\n",
      "88.27000000000001 % success on test data\n",
      "--- (30) ---\n",
      "Training started at 1646161613.2992787\n",
      "Epoch 0: 69.2134017944336: 0.1009\n",
      "Epoch 5000: 44.69273376464844: 0.9688\n",
      "Training finished at 1646161657.902255; lasted 44.60297632217407 seconds.\n",
      "96.88 % success on test data\n",
      "--- (31) ---\n",
      "Training started at 1646161658.9482505\n",
      "Epoch 0: 69.0: 0.101\n",
      "Epoch 5000: 45.64799880981445: 0.9606\n",
      "Training finished at 1646161703.3564217; lasted 44.4081711769104 seconds.\n",
      "96.06 % success on test data\n",
      "--- (32) ---\n",
      "Training started at 1646161704.3770592\n",
      "Epoch 0: 69.20092010498047: 0.101\n",
      "Epoch 5000: 43.8985595703125: 0.9729\n",
      "Training finished at 1646161748.5748935; lasted 44.19783425331116 seconds.\n",
      "97.28999999999999 % success on test data\n",
      "--- (33) ---\n",
      "Training started at 1646161749.6028554\n",
      "Epoch 0: 69.0291519165039: 0.1217\n",
      "Epoch 5000: 47.295921325683594: 0.8806\n",
      "Training finished at 1646161793.5192478; lasted 43.91639232635498 seconds.\n",
      "88.06 % success on test data\n",
      "--- (34) ---\n",
      "Training started at 1646161794.5192113\n",
      "Epoch 0: 69.10059356689453: 0.1032\n",
      "Epoch 5000: 45.55939483642578: 0.971\n",
      "Training finished at 1646161839.2253857; lasted 44.70617437362671 seconds.\n",
      "97.1 % success on test data\n",
      "--- (35) ---\n",
      "Training started at 1646161840.28814\n",
      "Epoch 0: 69.10514831542969: 0.1296\n",
      "Epoch 5000: 44.90101623535156: 0.9704\n",
      "Training finished at 1646161884.5089061; lasted 44.22076606750488 seconds.\n",
      "97.04 % success on test data\n",
      "--- (36) ---\n",
      "Training started at 1646161885.5159457\n",
      "Epoch 0: 69.05738067626953: 0.0892\n",
      "Epoch 5000: 51.73136901855469: 0.876\n",
      "Training finished at 1646161930.0143213; lasted 44.49837565422058 seconds.\n",
      "87.6 % success on test data\n",
      "--- (37) ---\n",
      "Training started at 1646161931.013158\n",
      "Epoch 0: 69.1739273071289: 0.1223\n",
      "Epoch 5000: 44.16758346557617: 0.9748\n",
      "Training finished at 1646161975.1878324; lasted 44.17467427253723 seconds.\n",
      "97.48 % success on test data\n",
      "--- (38) ---\n",
      "Training started at 1646161976.2287729\n",
      "Epoch 0: 69.0853500366211: 0.1303\n",
      "Epoch 5000: 45.27640151977539: 0.9655\n",
      "Training finished at 1646162020.6796696; lasted 44.45089673995972 seconds.\n",
      "96.55 % success on test data\n",
      "--- (39) ---\n",
      "Training started at 1646162021.6978235\n",
      "Epoch 0: 68.97746276855469: 0.1788\n",
      "Epoch 5000: 44.94794464111328: 0.876\n",
      "Training finished at 1646162065.623698; lasted 43.92587447166443 seconds.\n",
      "87.6 % success on test data\n",
      "--- (40) ---\n",
      "Training started at 1646162066.661641\n",
      "Epoch 0: 69.11166381835938: 0.0892\n",
      "Epoch 5000: 45.768123626708984: 0.9705\n",
      "Training finished at 1646162111.0063572; lasted 44.3447163105011 seconds.\n",
      "97.05 % success on test data\n",
      "--- (41) ---\n",
      "Training started at 1646162112.0715864\n",
      "Epoch 0: 69.04621887207031: 0.1973\n",
      "Epoch 5000: 43.96040344238281: 0.9715\n",
      "Training finished at 1646162155.4696357; lasted 43.39804935455322 seconds.\n",
      "97.15 % success on test data\n",
      "--- (42) ---\n",
      "Training started at 1646162156.5422497\n",
      "Epoch 0: 68.99681091308594: 0.1135\n",
      "Epoch 5000: 47.00047302246094: 0.8778\n",
      "Training finished at 1646162200.569669; lasted 44.027419328689575 seconds.\n",
      "87.78 % success on test data\n",
      "--- (43) ---\n",
      "Training started at 1646162201.6256034\n",
      "Epoch 0: 69.26988220214844: 0.1807\n",
      "Epoch 5000: 46.281063079833984: 0.9721\n",
      "Training finished at 1646162245.7382095; lasted 44.112606048583984 seconds.\n",
      "97.21 % success on test data\n",
      "--- (44) ---\n",
      "Training started at 1646162246.8307571\n",
      "Epoch 0: 69.08245849609375: 0.0984\n",
      "Epoch 5000: 44.46849822998047: 0.968\n",
      "Training finished at 1646162291.4331179; lasted 44.60236072540283 seconds.\n",
      "96.8 % success on test data\n",
      "--- (45) ---\n",
      "Training started at 1646162292.49807\n",
      "Epoch 0: 69.11766815185547: 0.101\n",
      "Epoch 5000: 47.48784637451172: 0.967\n",
      "Training finished at 1646162336.6290407; lasted 44.13097071647644 seconds.\n",
      "96.7 % success on test data\n",
      "--- (46) ---\n",
      "Training started at 1646162337.685377\n",
      "Epoch 0: 69.09571075439453: 0.1338\n",
      "Epoch 5000: 51.73411560058594: 0.8802\n",
      "Training finished at 1646162381.9860241; lasted 44.300647258758545 seconds.\n",
      "88.02 % success on test data\n",
      "--- (47) ---\n",
      "Training started at 1646162383.0095232\n",
      "Epoch 0: 69.20561981201172: 0.0974\n",
      "Epoch 5000: 43.845462799072266: 0.9731\n",
      "Training finished at 1646162427.455101; lasted 44.44557785987854 seconds.\n",
      "97.31 % success on test data\n",
      "--- (48) ---\n",
      "Training started at 1646162428.607186\n",
      "Epoch 0: 69.08008575439453: 0.1197\n",
      "Epoch 5000: 44.2912712097168: 0.9723\n",
      "Training finished at 1646162472.8535323; lasted 44.24634623527527 seconds.\n",
      "97.23 % success on test data\n",
      "--- (49) ---\n",
      "Training started at 1646162473.870466\n",
      "Epoch 0: 69.03775024414062: 0.137\n",
      "Epoch 5000: 49.76572799682617: 0.8835\n",
      "Training finished at 1646162517.5198123; lasted 43.649346351623535 seconds.\n",
      "88.35 % success on test data\n",
      "--- (50) ---\n",
      "Training started at 1646162518.636453\n",
      "Epoch 0: 69.14562225341797: 0.116\n",
      "Epoch 5000: 44.30850601196289: 0.9689\n",
      "Training finished at 1646162562.8624086; lasted 44.22595572471619 seconds.\n",
      "96.89 % success on test data\n",
      "--- (51) ---\n",
      "Training started at 1646162563.9663625\n",
      "Epoch 0: 69.17684936523438: 0.1028\n",
      "Epoch 5000: 44.837615966796875: 0.9732\n",
      "Training finished at 1646162607.3394098; lasted 43.37304735183716 seconds.\n",
      "97.32 % success on test data\n",
      "--- (52) ---\n",
      "Training started at 1646162608.3737729\n",
      "Epoch 0: 69.0234603881836: 0.1028\n",
      "Epoch 5000: 46.096405029296875: 0.8856\n",
      "Training finished at 1646162652.6524112; lasted 44.27863836288452 seconds.\n",
      "88.56 % success on test data\n",
      "--- (53) ---\n",
      "Training started at 1646162653.6833575\n",
      "Epoch 0: 69.1021728515625: 0.1038\n",
      "Epoch 5000: 45.11688995361328: 0.9669\n",
      "Training finished at 1646162698.1322324; lasted 44.448874950408936 seconds.\n",
      "96.69 % success on test data\n",
      "--- (54) ---\n",
      "Training started at 1646162699.1885607\n",
      "Epoch 0: 69.08430480957031: 0.1001\n",
      "Epoch 5000: 45.934627532958984: 0.8867\n",
      "Training finished at 1646162743.4282553; lasted 44.239694595336914 seconds.\n",
      "88.67 % success on test data\n",
      "--- (55) ---\n",
      "Training started at 1646162744.4641614\n",
      "Epoch 0: 69.20577239990234: 0.098\n",
      "Epoch 5000: 44.66712951660156: 0.9728\n",
      "Training finished at 1646162788.305177; lasted 43.841015577316284 seconds.\n",
      "97.28 % success on test data\n",
      "--- (56) ---\n",
      "Training started at 1646162789.3651257\n",
      "Epoch 0: 69.05647277832031: 0.153\n",
      "Epoch 5000: 44.85752868652344: 0.9704\n",
      "Training finished at 1646162833.652819; lasted 44.28769326210022 seconds.\n",
      "97.04 % success on test data\n",
      "--- (57) ---\n",
      "Training started at 1646162834.7498004\n",
      "Epoch 0: 69.15975952148438: 0.0899\n",
      "Epoch 5000: 45.810630798339844: 0.9732\n",
      "Training finished at 1646162879.003726; lasted 44.25392556190491 seconds.\n",
      "97.32 % success on test data\n",
      "--- (58) ---\n",
      "Training started at 1646162880.0722108\n",
      "Epoch 0: 69.0812759399414: 0.1149\n",
      "Epoch 5000: 43.894466400146484: 0.9691\n",
      "Training finished at 1646162924.6328082; lasted 44.56059741973877 seconds.\n",
      "96.91 % success on test data\n",
      "--- (59) ---\n",
      "Training started at 1646162925.6815617\n",
      "Epoch 0: 69.14175415039062: 0.098\n",
      "Epoch 5000: 44.593711853027344: 0.9747\n",
      "Training finished at 1646162969.588594; lasted 43.90703225135803 seconds.\n",
      "97.47 % success on test data\n",
      "--- (60) ---\n",
      "Training started at 1646162970.6761105\n",
      "Epoch 0: 69.08448791503906: 0.1032\n",
      "Epoch 5000: 47.791648864746094: 0.876\n",
      "Training finished at 1646163014.7638185; lasted 44.08770799636841 seconds.\n",
      "87.6 % success on test data\n",
      "--- (61) ---\n",
      "Training started at 1646163015.8740492\n",
      "Epoch 0: 69.00769805908203: 0.1138\n",
      "Epoch 5000: 45.50811767578125: 0.9708\n",
      "Training finished at 1646163059.6015406; lasted 43.72749137878418 seconds.\n",
      "97.08 % success on test data\n",
      "--- (62) ---\n",
      "Training started at 1646163060.6274426\n",
      "Epoch 0: 69.06593322753906: 0.1028\n",
      "Epoch 5000: 46.25315475463867: 0.8782\n",
      "Training finished at 1646163104.7306573; lasted 44.103214740753174 seconds.\n",
      "87.82 % success on test data\n",
      "--- (63) ---\n",
      "Training started at 1646163105.759784\n",
      "Epoch 0: 68.93302917480469: 0.1028\n",
      "Epoch 5000: 45.72745895385742: 0.9583\n",
      "Training finished at 1646163149.8274927; lasted 44.06770873069763 seconds.\n",
      "95.83 % success on test data\n",
      "--- (64) ---\n",
      "Training started at 1646163150.9077244\n",
      "Epoch 0: 69.14212799072266: 0.098\n",
      "Epoch 5000: 48.71411895751953: 0.8813\n",
      "Training finished at 1646163194.9974284; lasted 44.08970403671265 seconds.\n",
      "88.13 % success on test data\n",
      "--- (65) ---\n",
      "Training started at 1646163196.0325043\n",
      "Epoch 0: 68.96976470947266: 0.0974\n",
      "Epoch 5000: 43.83638000488281: 0.9705\n",
      "Training finished at 1646163240.521146; lasted 44.4886417388916 seconds.\n",
      "97.05 % success on test data\n",
      "--- (66) ---\n",
      "Training started at 1646163241.536286\n",
      "Epoch 0: 69.08010864257812: 0.1032\n",
      "Epoch 5000: 44.890174865722656: 0.872\n",
      "Training finished at 1646163285.5800347; lasted 44.04374861717224 seconds.\n",
      "87.2 % success on test data\n",
      "--- (67) ---\n",
      "Training started at 1646163286.6589184\n",
      "Epoch 0: 69.1746597290039: 0.0974\n",
      "Epoch 5000: 43.85795211791992: 0.9723\n",
      "Training finished at 1646163330.8618467; lasted 44.20292830467224 seconds.\n",
      "97.23 % success on test data\n",
      "--- (68) ---\n",
      "Training started at 1646163331.8988526\n",
      "Epoch 0: 69.07147216796875: 0.101\n",
      "Epoch 5000: 44.45850372314453: 0.8764\n",
      "Training finished at 1646163376.2117987; lasted 44.3129460811615 seconds.\n",
      "87.64 % success on test data\n",
      "--- (69) ---\n",
      "Training started at 1646163377.3057547\n",
      "Epoch 0: 69.0041732788086: 0.1102\n",
      "Epoch 5000: 44.83649444580078: 0.8746\n",
      "Training finished at 1646163420.9999816; lasted 43.69422698020935 seconds.\n",
      "87.46000000000001 % success on test data\n",
      "--- (70) ---\n",
      "Training started at 1646163422.0822442\n",
      "Epoch 0: 69.04790496826172: 0.0889\n",
      "Epoch 5000: 44.91808319091797: 0.9589\n",
      "Training finished at 1646163465.6959445; lasted 43.61370038986206 seconds.\n",
      "95.89 % success on test data\n",
      "--- (71) ---\n",
      "Training started at 1646163466.7549005\n",
      "Epoch 0: 69.03430938720703: 0.098\n",
      "Epoch 5000: 45.876609802246094: 0.8691\n",
      "Training finished at 1646163510.4747229; lasted 43.7198224067688 seconds.\n",
      "86.91 % success on test data\n",
      "--- (72) ---\n",
      "Training started at 1646163511.5079935\n",
      "Epoch 0: 69.11438751220703: 0.0982\n",
      "Epoch 5000: 43.862911224365234: 0.9683\n",
      "Training finished at 1646163555.4726536; lasted 43.96466016769409 seconds.\n",
      "96.83 % success on test data\n",
      "--- (73) ---\n",
      "Training started at 1646163556.4966335\n",
      "Epoch 0: 69.15542602539062: 0.098\n",
      "Epoch 5000: 46.424964904785156: 0.9563\n",
      "Training finished at 1646163600.2549803; lasted 43.75834679603577 seconds.\n",
      "95.63000000000001 % success on test data\n",
      "--- (74) ---\n",
      "Training started at 1646163601.29335\n",
      "Epoch 0: 69.13072204589844: 0.098\n",
      "Epoch 5000: 47.89069366455078: 0.8761\n",
      "Training finished at 1646163645.444152; lasted 44.15080213546753 seconds.\n",
      "87.61 % success on test data\n",
      "--- (75) ---\n",
      "Training started at 1646163646.4524422\n",
      "Epoch 0: 69.13114929199219: 0.1363\n",
      "Epoch 5000: 44.107696533203125: 0.9724\n",
      "Training finished at 1646163689.8712614; lasted 43.418819189071655 seconds.\n",
      "97.24000000000001 % success on test data\n",
      "--- (76) ---\n",
      "Training started at 1646163690.9158711\n",
      "Epoch 0: 69.1550521850586: 0.0958\n",
      "Epoch 5000: 45.811161041259766: 0.8844\n",
      "Training finished at 1646163734.8942392; lasted 43.978368043899536 seconds.\n",
      "88.44 % success on test data\n",
      "--- (77) ---\n",
      "Training started at 1646163735.9571848\n",
      "Epoch 0: 69.15296173095703: 0.0976\n",
      "Epoch 5000: 44.002471923828125: 0.9714\n",
      "Training finished at 1646163780.4094777; lasted 44.452292919158936 seconds.\n",
      "97.14 % success on test data\n",
      "--- (78) ---\n",
      "Training started at 1646163781.4623888\n",
      "Epoch 0: 69.00360870361328: 0.0998\n",
      "Epoch 5000: 44.900306701660156: 0.9729\n",
      "Training finished at 1646163825.7104197; lasted 44.2480309009552 seconds.\n",
      "97.28999999999999 % success on test data\n",
      "--- (79) ---\n",
      "Training started at 1646163826.7450473\n",
      "Epoch 0: 69.12345886230469: 0.098\n",
      "Epoch 5000: 43.98480987548828: 0.9747\n",
      "Training finished at 1646163870.8782196; lasted 44.133172273635864 seconds.\n",
      "97.47 % success on test data\n",
      "--- (80) ---\n",
      "Training started at 1646163871.903168\n",
      "Epoch 0: 69.16923522949219: 0.0892\n",
      "Epoch 5000: 47.68373107910156: 0.8785\n",
      "Training finished at 1646163915.2010167; lasted 43.29784870147705 seconds.\n",
      "87.85 % success on test data\n",
      "--- (81) ---\n",
      "Training started at 1646163916.222867\n",
      "Epoch 0: 69.00692749023438: 0.0892\n",
      "Epoch 5000: 43.83790588378906: 0.972\n",
      "Training finished at 1646163960.3196573; lasted 44.0967903137207 seconds.\n",
      "97.2 % success on test data\n",
      "--- (82) ---\n",
      "Training started at 1646163961.3885732\n",
      "Epoch 0: 69.12989807128906: 0.1396\n",
      "Epoch 5000: 45.81291961669922: 0.9691\n",
      "Training finished at 1646164005.668962; lasted 44.280388832092285 seconds.\n",
      "96.91 % success on test data\n",
      "--- (83) ---\n",
      "Training started at 1646164006.7170951\n",
      "Epoch 0: 69.10810089111328: 0.1414\n",
      "Epoch 5000: 44.246917724609375: 0.8819\n",
      "Training finished at 1646164050.4694057; lasted 43.75231051445007 seconds.\n",
      "88.19 % success on test data\n",
      "--- (84) ---\n",
      "Training started at 1646164051.5085046\n",
      "Epoch 0: 69.1110610961914: 0.0982\n",
      "Epoch 5000: 45.761051177978516: 0.9714\n",
      "Training finished at 1646164095.864437; lasted 44.35593247413635 seconds.\n",
      "97.14 % success on test data\n",
      "--- (85) ---\n",
      "Training started at 1646164096.979769\n",
      "Epoch 0: 69.15220642089844: 0.098\n",
      "Epoch 5000: 44.41727828979492: 0.9681\n",
      "Training finished at 1646164141.4862273; lasted 44.5064582824707 seconds.\n",
      "96.81 % success on test data\n",
      "--- (86) ---\n",
      "Training started at 1646164142.51625\n",
      "Epoch 0: 68.87765502929688: 0.1009\n",
      "Epoch 5000: 44.872215270996094: 0.9679\n",
      "Training finished at 1646164188.241301; lasted 45.725051164627075 seconds.\n",
      "96.78999999999999 % success on test data\n",
      "--- (87) ---\n",
      "Training started at 1646164189.2875881\n",
      "Epoch 0: 69.11669921875: 0.0759\n",
      "Epoch 5000: 45.01018524169922: 0.9734\n",
      "Training finished at 1646164234.4791121; lasted 45.191524028778076 seconds.\n",
      "97.34 % success on test data\n",
      "--- (88) ---\n",
      "Training started at 1646164235.5315928\n",
      "Epoch 0: 69.06011962890625: 0.101\n",
      "Epoch 5000: 46.89394760131836: 0.9617\n",
      "Training finished at 1646164279.959849; lasted 44.42825627326965 seconds.\n",
      "96.17 % success on test data\n",
      "--- (89) ---\n",
      "Training started at 1646164280.992718\n",
      "Epoch 0: 68.97225189208984: 0.1175\n",
      "Epoch 5000: 44.84764862060547: 0.968\n",
      "Training finished at 1646164328.4414551; lasted 47.448737144470215 seconds.\n",
      "96.8 % success on test data\n"
     ]
    }
   ],
   "source": [
    "for i in range(90):\n",
    "    print(f'--- ({i}) ---')\n",
    "    model = ModelManager.get_untrained(ModelType.MnistCnnOto)\n",
    "    Coach.train(\n",
    "        model,\n",
    "        data,\n",
    "        nn.CrossEntropyLoss(reduction='sum'),\n",
    "        optim.Adam(model.parameters(), lr=1e-3),\n",
    "        30,\n",
    "        5001,\n",
    "        5000\n",
    "    )\n",
    "    adam.append(Coach.measure_performance(model, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x16b8c2fffa0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for performences in [sorted(simple), sorted(momentum), sorted(nesterov)]:\n",
    "performences = sorted(simple)\n",
    "distribution = {}\n",
    "for i in range(1, len(performences) + 1):\n",
    "    distribution[performences[i - 1]] = i / len(performences)\n",
    "my_list = distribution.items()\n",
    "sorted(my_list)\n",
    "x, y = zip(*my_list)\n",
    "lab.plot(x, y, color = \"blue\", label=\"SGD\")\n",
    "\n",
    "performences = sorted(adagrad)\n",
    "distribution = {}\n",
    "for i in range(1, len(performences) + 1):\n",
    "    distribution[performences[i - 1]] = i / len(performences)\n",
    "my_list = distribution.items()\n",
    "sorted(my_list)\n",
    "x, y = zip(*my_list)\n",
    "lab.plot(x, y, color = \"red\", label=\"Adagrad\")\n",
    "\n",
    "performences = sorted(rmsprop)\n",
    "distribution = {}\n",
    "for i in range(1, len(performences) + 1):\n",
    "    distribution[performences[i - 1]] = i / len(performences)\n",
    "my_list = distribution.items()\n",
    "sorted(my_list)\n",
    "x, y = zip(*my_list)\n",
    "lab.plot(x, y, color=\"green\", label=\"RMSProp\")\n",
    "\n",
    "performences = sorted(adam)\n",
    "distribution = {}\n",
    "for i in range(1, len(performences) + 1):\n",
    "    distribution[performences[i - 1]] = i / len(performences)\n",
    "my_list = distribution.items()\n",
    "sorted(my_list)\n",
    "x, y = zip(*my_list)\n",
    "lab.plot(x, y, color=\"yellow\", label=\"Adam\")\n",
    "\n",
    "lab.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple, adagrad, rmsprop, adam = [], [], [], []\n",
    "\n",
    "with open('oto_sgd.txt') as f:\n",
    "    line_counter = 0\n",
    "    for line in f:\n",
    "        if line_counter % 6 == 5:\n",
    "            parts = line.split(' ')\n",
    "            simple.append(float(parts[0]) / 100)\n",
    "        line_counter += 1\n",
    "\n",
    "with open('oto_adagrad.txt') as f:\n",
    "    line_counter = 0\n",
    "    for line in f:\n",
    "        if line_counter % 6 == 5:\n",
    "            parts = line.split(' ')\n",
    "            adagrad.append(float(parts[0]) / 100)\n",
    "        line_counter += 1\n",
    "\n",
    "with open('oto_rmsprop.txt') as f:\n",
    "    line_counter = 0\n",
    "    for line in f:\n",
    "        if line_counter % 6 == 5:\n",
    "            parts = line.split(' ')\n",
    "            rmsprop.append(float(parts[0]) / 100)\n",
    "        line_counter += 1\n",
    "\n",
    "with open('oto_adam.txt') as f:\n",
    "    line_counter = 0\n",
    "    for line in f:\n",
    "        if line_counter % 6 == 5:\n",
    "            parts = line.split(' ')\n",
    "            adam.append(float(parts[0]) / 100)\n",
    "        line_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('oto.json', 'w') as file:\n",
    "    json.dump([simple, adagrad, rmsprop, adam], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for performences in [sorted(simple), sorted(momentum), sorted(nesterov)]:\n",
    "performences = sorted(simple)\n",
    "distribution = {}\n",
    "for i in range(1, len(performences) + 1):\n",
    "    distribution[performences[i - 1]] = i / len(performences)\n",
    "my_list = distribution.items()\n",
    "sorted(my_list)\n",
    "x, y = zip(*my_list)\n",
    "lab.plot(x, y, color = \"blue\", label=\"SGD\")\n",
    "\n",
    "performences = sorted(adagrad)\n",
    "distribution = {}\n",
    "for i in range(1, len(performences) + 1):\n",
    "    distribution[performences[i - 1]] = i / len(performences)\n",
    "my_list = distribution.items()\n",
    "sorted(my_list)\n",
    "x, y = zip(*my_list)\n",
    "lab.plot(x, y, color = \"red\", label=\"Adagrad\")\n",
    "\n",
    "performences = sorted(rmsprop)\n",
    "distribution = {}\n",
    "for i in range(1, len(performences) + 1):\n",
    "    distribution[performences[i - 1]] = i / len(performences)\n",
    "my_list = distribution.items()\n",
    "sorted(my_list)\n",
    "x, y = zip(*my_list)\n",
    "lab.plot(x, y, color=\"green\", label=\"RMSProp\")\n",
    "\n",
    "performences = sorted(adam)\n",
    "distribution = {}\n",
    "for i in range(1, len(performences) + 1):\n",
    "    distribution[performences[i - 1]] = i / len(performences)\n",
    "my_list = distribution.items()\n",
    "sorted(my_list)\n",
    "x, y = zip(*my_list)\n",
    "lab.plot(x, y, color=\"yellow\", label=\"Adam\")\n",
    "\n",
    "lab.legend()\n",
    "\n",
    "lab.savefig('oto.pdf')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9d03d93fdbeeb361cc45deae4888a34596acc9ca3c93366af240ad46910d2d2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
