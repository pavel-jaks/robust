{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import MnistData, Clipper\n",
    "from models import ModelManager, ModelType\n",
    "from adversarials import ClassificationAdversarials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "linfty_norm_radius = 50 / 255\n",
    "lone_norm_radius = 28 * 28 * 50 / 255\n",
    "ltwo_norm_radius = 28 * 50 / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelManager.get_trained(ModelType.MnistCnnB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MnistData(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_lambda = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CW functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_batch_linfty_norm(input:torch.Tensor) -> torch.Tensor:\n",
    "    return torch.max(torch.max(torch.max(torch.abs(input), dim=3)[0], dim=2)[0], dim=1)[0]\n",
    "\n",
    "def mnist_batch_lone_norm(input:torch.Tensor) -> torch.Tensor:\n",
    "    return (torch.abs(input)).sum(3).sum(2).sum(1)\n",
    "\n",
    "def mnist_batch_ltwo_norm(input:torch.Tensor) -> torch.Tensor:\n",
    "    return ((input ** 2).sum(3).sum(2).sum(1)) ** (1 / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cw_batch_clip_always(model: nn.Module, benign_examples: torch.Tensor, labels: torch.Tensor, norm_radius, c_lambda: float, batch_norm) -> torch.Tensor:\n",
    "    adversarial_examples = torch.zeros(benign_examples.shape)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "    step_size = 1e-2\n",
    "    for _ in range(100):\n",
    "        adversarial_examples.requires_grad = True\n",
    "        if adversarial_examples.grad is not None:\n",
    "            adversarial_examples.grad.zero_()\n",
    "        benign_examples.requires_grad = True\n",
    "        if benign_examples.grad is not None:\n",
    "            benign_examples.grad.zero_()\n",
    "        loss = batch_norm(adversarial_examples - benign_examples).sum() - c_lambda * loss_fn(model(adversarial_examples), labels)\n",
    "        loss.backward()\n",
    "        adversarial_examples = (adversarial_examples - step_size * adversarial_examples.grad.apply_(lambda x: 1 if x >= 0 else -1)).detach()\n",
    "        adversarial_examples = Clipper.clip_batch(benign_examples, adversarial_examples, batch_norm, norm_radius)\n",
    "    # return Clipper.clip_batch(benign_examples, adversarial_examples, batch_norm, norm_radius)\n",
    "    return adversarial_examples\n",
    "\n",
    "def cw_batch_clip_once(model: nn.Module, benign_examples: torch.Tensor, labels: torch.Tensor, norm_radius, c_lambda: float, batch_norm) -> torch.Tensor:\n",
    "    adversarial_examples = torch.zeros(benign_examples.shape)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "    step_size = 1e-2\n",
    "    for _ in range(100):\n",
    "        adversarial_examples.requires_grad = True\n",
    "        if adversarial_examples.grad is not None:\n",
    "            adversarial_examples.grad.zero_()\n",
    "        benign_examples.requires_grad = True\n",
    "        if benign_examples.grad is not None:\n",
    "            benign_examples.grad.zero_()\n",
    "        loss = batch_norm(adversarial_examples - benign_examples).sum() - c_lambda * loss_fn(model(adversarial_examples), labels)\n",
    "        loss.backward()\n",
    "        adversarial_examples = (adversarial_examples - step_size * adversarial_examples.grad.apply_(lambda x: 1 if x >= 0 else -1)).detach()\n",
    "        # adversarial_examples = Clipper.clip_batch(benign_examples, adversarial_examples, batch_norm, norm_radius)\n",
    "    return Clipper.clip_batch(benign_examples, adversarial_examples, batch_norm, norm_radius)\n",
    "    # return adversarial_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_examples, labels = data.choose_first_well_classified(batch_size, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_linfty_examples_clip_always = cw_batch_clip_always(model, benign_examples, labels, linfty_norm_radius, c_lambda, mnist_batch_linfty_norm)\n",
    "cw_lone_examples_clip_always = cw_batch_clip_always(model, benign_examples, labels, lone_norm_radius, c_lambda, mnist_batch_lone_norm)\n",
    "cw_ltwo_examples_clip_always = cw_batch_clip_always(model, benign_examples, labels, ltwo_norm_radius, c_lambda, mnist_batch_ltwo_norm)\n",
    "\n",
    "cw_linfty_examples_clip_once = cw_batch_clip_once(model, benign_examples, labels, linfty_norm_radius, c_lambda, mnist_batch_linfty_norm)\n",
    "cw_lone_examples_clip_once = cw_batch_clip_once(model, benign_examples, labels, lone_norm_radius, c_lambda, mnist_batch_lone_norm)\n",
    "cw_ltwo_examples_clip_once = cw_batch_clip_once(model, benign_examples, labels, ltwo_norm_radius, c_lambda, mnist_batch_ltwo_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save'em all\n",
    "for i in range(batch_size):\n",
    "    example = np.array(benign_examples[i].detach()).reshape(28, 28)\n",
    "    plt.imshow(example, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f\"AEXAMPLES\\\\CW_BATCH\\\\c_lambda_{c_lambda}_benign_{i}.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "    \n",
    "    example = np.array(cw_linfty_examples_clip_always[i].detach()).reshape(28, 28)\n",
    "    plt.imshow(example, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f\"AEXAMPLES\\\\CW_BATCH\\\\clip_always_c_lambda_{c_lambda}_cw_linfty_{i}.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "    \n",
    "    example = np.array(cw_lone_examples_clip_always[i].detach()).reshape(28, 28)\n",
    "    plt.imshow(example, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f\"AEXAMPLES\\\\CW_BATCH\\\\clip_always_c_lambda_{c_lambda}_cw_lone_{i}.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "    \n",
    "    example = np.array(cw_ltwo_examples_clip_always[i].detach()).reshape(28, 28)\n",
    "    plt.imshow(example, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f\"AEXAMPLES\\\\CW_BATCH\\\\clip_always_c_lambda_{c_lambda}_cw_ltwo_{i}.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "\n",
    "    example = np.array(cw_linfty_examples_clip_once[i].detach()).reshape(28, 28)\n",
    "    plt.imshow(example, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f\"AEXAMPLES\\\\CW_BATCH\\\\clip_once_c_lambda_{c_lambda}_cw_linfty_{i}.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "    \n",
    "    example = np.array(cw_lone_examples_clip_once[i].detach()).reshape(28, 28)\n",
    "    plt.imshow(example, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f\"AEXAMPLES\\\\CW_BATCH\\\\clip_once_c_lambda_{c_lambda}_cw_lone_{i}.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "    \n",
    "    example = np.array(cw_ltwo_examples_clip_once[i].detach()).reshape(28, 28)\n",
    "    plt.imshow(example, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f\"AEXAMPLES\\\\CW_BATCH\\\\clip_once_c_lambda_{c_lambda}_cw_ltwo_{i}.png\", bbox_inches=\"tight\", pad_inches=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cw_linfty_always: 8\n",
      "cw_lone_always: 5\n",
      "cw_ltwo_always: 0\n",
      "cw_linfty_once: 0\n",
      "cw_lone_once: 2\n",
      "cw_ltwo_once: 7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cw_linfty_adversarials = ClassificationAdversarials.get_adversarials(model, benign_examples, labels, cw_linfty_examples_clip_always)\n",
    "print(f'cw_linfty_always: {len(cw_linfty_adversarials)}')\n",
    "\n",
    "cw_lone_adversarials = ClassificationAdversarials.get_adversarials(model, benign_examples, labels, cw_lone_examples_clip_always)\n",
    "print(f'cw_lone_always: {len(cw_lone_adversarials)}')\n",
    "\n",
    "cw_ltwo_adversarials = ClassificationAdversarials.get_adversarials(model, benign_examples, labels, cw_ltwo_examples_clip_always)\n",
    "print(f'cw_ltwo_always: {len(cw_ltwo_adversarials)}')\n",
    "\n",
    "cw_linfty_adversarials = ClassificationAdversarials.get_adversarials(model, benign_examples, labels, cw_linfty_examples_clip_once)\n",
    "print(f'cw_linfty_once: {len(cw_linfty_adversarials)}')\n",
    "\n",
    "cw_lone_adversarials = ClassificationAdversarials.get_adversarials(model, benign_examples, labels, cw_lone_examples_clip_once)\n",
    "print(f'cw_lone_once: {len(cw_lone_adversarials)}')\n",
    "\n",
    "cw_ltwo_adversarials = ClassificationAdversarials.get_adversarials(model, benign_examples, labels, cw_ltwo_examples_clip_once)\n",
    "print(f'cw_ltwo_once: {len(cw_ltwo_adversarials)}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9d03d93fdbeeb361cc45deae4888a34596acc9ca3c93366af240ad46910d2d2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
