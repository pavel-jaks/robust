{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import MnistData, Clipper\n",
    "from models import ModelManager, ModelType\n",
    "from adversarials import ClassificationAdversarials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "linfty_norm_radius = 50 / 255\n",
    "lone_norm_radius = 28 * 28 * 50 / 255\n",
    "ltwo_norm_radius = 28 * 50 / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelManager.get_trained(ModelType.MnistCnnB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MnistData(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_lambda = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CW functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_batch_linfty_norm(input:torch.Tensor) -> torch.Tensor:\n",
    "    return torch.max(torch.max(torch.max(torch.abs(input), dim=3)[0], dim=2)[0], dim=1)[0]\n",
    "\n",
    "def mnist_batch_lone_norm(input:torch.Tensor) -> torch.Tensor:\n",
    "    return (torch.abs(input)).sum(3).sum(2).sum(1)\n",
    "\n",
    "def mnist_batch_ltwo_norm(input:torch.Tensor) -> torch.Tensor:\n",
    "    return ((input ** 2).sum(3).sum(2).sum(1)) ** (1 / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cw_batch_clip_always(model: nn.Module, benign_examples: torch.Tensor, labels: torch.Tensor, norm_radius, c_lambda: float, batch_norm) -> torch.Tensor:\n",
    "    adversarial_examples = torch.zeros(benign_examples.shape)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "    step_size = 1e-2\n",
    "    for _ in range(100):\n",
    "        adversarial_examples.requires_grad = True\n",
    "        if adversarial_examples.grad is not None:\n",
    "            adversarial_examples.grad.zero_()\n",
    "        benign_examples.requires_grad = True\n",
    "        if benign_examples.grad is not None:\n",
    "            benign_examples.grad.zero_()\n",
    "        loss = batch_norm(adversarial_examples - benign_examples).sum() - c_lambda * loss_fn(model(adversarial_examples), labels)\n",
    "        loss.backward()\n",
    "        adversarial_examples = (adversarial_examples - step_size * adversarial_examples.grad.apply_(lambda x: 1 if x >= 0 else -1)).detach()\n",
    "        adversarial_examples = Clipper.clip_batch(benign_examples, adversarial_examples, batch_norm, norm_radius)\n",
    "    # return Clipper.clip_batch(benign_examples, adversarial_examples, batch_norm, norm_radius)\n",
    "    return adversarial_examples\n",
    "\n",
    "def cw_batch_clip_once(model: nn.Module, benign_examples: torch.Tensor, labels: torch.Tensor, norm_radius, c_lambda: float, batch_norm) -> torch.Tensor:\n",
    "    adversarial_examples = torch.zeros(benign_examples.shape)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "    step_size = 1e-2\n",
    "    for _ in range(100):\n",
    "        adversarial_examples.requires_grad = True\n",
    "        if adversarial_examples.grad is not None:\n",
    "            adversarial_examples.grad.zero_()\n",
    "        benign_examples.requires_grad = True\n",
    "        if benign_examples.grad is not None:\n",
    "            benign_examples.grad.zero_()\n",
    "        loss = batch_norm(adversarial_examples - benign_examples).sum() - c_lambda * loss_fn(model(adversarial_examples), labels)\n",
    "        loss.backward()\n",
    "        adversarial_examples = (adversarial_examples - step_size * adversarial_examples.grad.apply_(lambda x: 1 if x >= 0 else -1)).detach()\n",
    "        # adversarial_examples = Clipper.clip_batch(benign_examples, adversarial_examples, batch_norm, norm_radius)\n",
    "    return Clipper.clip_batch(benign_examples, adversarial_examples, batch_norm, norm_radius)\n",
    "    # return adversarial_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_examples, labels = data.choose_first_well_classified(batch_size, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_linfty_examples_clip_always = cw_batch_clip_always(model, benign_examples, labels, linfty_norm_radius, c_lambda, mnist_batch_linfty_norm)\n",
    "cw_lone_examples_clip_always = cw_batch_clip_always(model, benign_examples, labels, lone_norm_radius, c_lambda, mnist_batch_lone_norm)\n",
    "cw_ltwo_examples_clip_always = cw_batch_clip_always(model, benign_examples, labels, ltwo_norm_radius, c_lambda, mnist_batch_ltwo_norm)\n",
    "\n",
    "cw_linfty_examples_clip_once = cw_batch_clip_once(model, benign_examples, labels, linfty_norm_radius, c_lambda, mnist_batch_linfty_norm)\n",
    "cw_lone_examples_clip_once = cw_batch_clip_once(model, benign_examples, labels, lone_norm_radius, c_lambda, mnist_batch_lone_norm)\n",
    "cw_ltwo_examples_clip_once = cw_batch_clip_once(model, benign_examples, labels, ltwo_norm_radius, c_lambda, mnist_batch_ltwo_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save some\n",
    "for i in [1, 3, 5, 7, 2, 34, 18, 37, 17, 4]:\n",
    "    example = np.array(benign_examples[i].detach()).reshape(28, 28)\n",
    "    plt.imshow(example, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f\"AEXAMPLES\\\\CW_BATCH\\\\benign_{i}.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "    \n",
    "    example = np.array(cw_linfty_examples_clip_always[i].detach()).reshape(28, 28)\n",
    "    plt.imshow(example, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f\"AEXAMPLES\\\\CW_BATCH\\\\clip_always_c_lambda_{c_lambda}_cw_linfty_{i}.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "    \n",
    "    example = np.array(cw_lone_examples_clip_always[i].detach()).reshape(28, 28)\n",
    "    plt.imshow(example, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f\"AEXAMPLES\\\\CW_BATCH\\\\clip_always_c_lambda_{c_lambda}_cw_lone_{i}.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "    \n",
    "    example = np.array(cw_ltwo_examples_clip_always[i].detach()).reshape(28, 28)\n",
    "    plt.imshow(example, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f\"AEXAMPLES\\\\CW_BATCH\\\\clip_always_c_lambda_{c_lambda}_cw_ltwo_{i}.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "\n",
    "    example = np.array(cw_linfty_examples_clip_once[i].detach()).reshape(28, 28)\n",
    "    plt.imshow(example, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f\"AEXAMPLES\\\\CW_BATCH\\\\clip_once_c_lambda_{c_lambda}_cw_linfty_{i}.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "    \n",
    "    example = np.array(cw_lone_examples_clip_once[i].detach()).reshape(28, 28)\n",
    "    plt.imshow(example, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f\"AEXAMPLES\\\\CW_BATCH\\\\clip_once_c_lambda_{c_lambda}_cw_lone_{i}.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "    \n",
    "    example = np.array(cw_ltwo_examples_clip_once[i].detach()).reshape(28, 28)\n",
    "    plt.imshow(example, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f\"AEXAMPLES\\\\CW_BATCH\\\\clip_once_c_lambda_{c_lambda}_cw_ltwo_{i}.png\", bbox_inches=\"tight\", pad_inches=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cw_linfty_always: 74\n",
      "cw_lone_always: 90\n",
      "cw_ltwo_always: 16\n",
      "cw_linfty_once: 0\n",
      "cw_lone_once: 93\n",
      "cw_ltwo_once: 94\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cw_linfty_adversarials = ClassificationAdversarials.get_adversarials(model, benign_examples, labels, cw_linfty_examples_clip_always)\n",
    "print(f'cw_linfty_always: {len(cw_linfty_adversarials)}')\n",
    "\n",
    "cw_lone_adversarials = ClassificationAdversarials.get_adversarials(model, benign_examples, labels, cw_lone_examples_clip_always)\n",
    "print(f'cw_lone_always: {len(cw_lone_adversarials)}')\n",
    "\n",
    "cw_ltwo_adversarials = ClassificationAdversarials.get_adversarials(model, benign_examples, labels, cw_ltwo_examples_clip_always)\n",
    "print(f'cw_ltwo_always: {len(cw_ltwo_adversarials)}')\n",
    "\n",
    "cw_linfty_adversarials = ClassificationAdversarials.get_adversarials(model, benign_examples, labels, cw_linfty_examples_clip_once)\n",
    "print(f'cw_linfty_once: {len(cw_linfty_adversarials)}')\n",
    "\n",
    "cw_lone_adversarials = ClassificationAdversarials.get_adversarials(model, benign_examples, labels, cw_lone_examples_clip_once)\n",
    "print(f'cw_lone_once: {len(cw_lone_adversarials)}')\n",
    "\n",
    "cw_ltwo_adversarials = ClassificationAdversarials.get_adversarials(model, benign_examples, labels, cw_ltwo_examples_clip_once)\n",
    "print(f'cw_ltwo_once: {len(cw_ltwo_adversarials)}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9d03d93fdbeeb361cc45deae4888a34596acc9ca3c93366af240ad46910d2d2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
